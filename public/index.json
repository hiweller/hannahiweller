[{"authors":null,"categories":null,"content":"I want to know how to interpret organisms in the light of their developmental and evolutionary history. We often assume every trait on an organism has been \u0026ldquo;optimized\u0026rdquo; under natural selection, but this is almost never true: evolution wanders.\nI find it helpful to think about every living thing as being like a very, very old house. Some of the parts are there because they\u0026rsquo;ve always been useful (walls), and some of the parts are new additions for new functions (solar panels); but a lot of parts are there because it was easier to modify what existed than it was to build something from scratch, especially if the house needs to be habitable the whole time (and the available changes are random). And, like houses, I think we can learn a lot about organisms by understanding them as the outcomes of their history.\n  View my CV.\n  View my most recent posts\n  Get in touch!\n","date":1620345600,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1620396986,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/author/hannah-weller/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/hannah-weller/","section":"authors","summary":"I want to know how to interpret organisms in the light of their developmental and evolutionary history. We often assume every trait on an organism has been \u0026ldquo;optimized\u0026rdquo; under natural selection, but this is almost never true: evolution wanders.","tags":null,"title":"Hannah Weller","type":"authors"},{"authors":null,"categories":null,"content":"   Table of Contents  What you will learn Program overview Courses in this program Meet your instructor FAQs    What you will learn  Fundamental Python programming skills Statistical concepts and how to apply them in practice Gain experience with the Scikit, including data visualization with Plotly and data wrangling with Pandas  Program overview The demand for skilled data science practitioners is rapidly growing. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi.\nCourses in this program  Python basics Build a foundation in Python.   Visualization Learn how to visualize data with Plotly.   Statistics Introduction to statistics for data science.   Meet your instructor Hannah Weller FAQs Are there prerequisites? There are no prerequisites for the first course.\n How often do the courses run? Continuously, at your own pace.\n  Begin the course   ","date":1611446400,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1611446400,"objectID":"59c3ce8e202293146a8a934d37a4070b","permalink":"/courses/example/","publishdate":"2021-01-24T00:00:00Z","relpermalink":"/courses/example/","section":"courses","summary":"An example of using Wowchemy's Book layout for publishing online courses.","tags":null,"title":"üìä Learn Data Science","type":"book"},{"authors":null,"categories":null,"content":"Build a foundation in Python.\n  1-2 hours per week, for 8 weeks\nLearn   Quiz What is the difference between lists and tuples? Lists\n Lists are mutable - they can be changed Slower than tuples Syntax: a_list = [1, 2.0, 'Hello world']  Tuples\n Tuples are immutable - they can\u0026rsquo;t be changed Tuples are faster than lists Syntax: a_tuple = (1, 2.0, 'Hello world')   Is Python case-sensitive? Yes\n","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"17a31b92253d299002593b7491eedeea","permalink":"/courses/example/python/","publishdate":"2021-01-01T00:00:00Z","relpermalink":"/courses/example/python/","section":"courses","summary":"Build a foundation in Python.\n","tags":null,"title":"Python basics","type":"book"},{"authors":null,"categories":null,"content":"Learn how to visualize data with Plotly.\n  1-2 hours per week, for 8 weeks\nLearn   Quiz When is a heatmap useful? Lorem ipsum dolor sit amet, consectetur adipiscing elit.\n Write Plotly code to render a bar chart import plotly.express as px data_canada = px.data.gapminder().query(\u0026quot;country == 'Canada'\u0026quot;) fig = px.bar(data_canada, x='year', y='pop') fig.show()  ","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"1b341b3479c8c6b1f807553b77e21b7c","permalink":"/courses/example/visualization/","publishdate":"2021-01-01T00:00:00Z","relpermalink":"/courses/example/visualization/","section":"courses","summary":"Learn how to visualize data with Plotly.\n","tags":null,"title":"Visualization","type":"book"},{"authors":null,"categories":null,"content":"Introduction to statistics for data science.\n  1-2 hours per week, for 8 weeks\nLearn The general form of the normal probability density function is:\n$$ f(x) = \\frac{1}{\\sigma \\sqrt{2\\pi} } e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2} $$\n The parameter $\\mu$ is the mean or expectation of the distribution. $\\sigma$ is its standard deviation. The variance of the distribution is $\\sigma^{2}$.   Quiz What is the parameter $\\mu$? The parameter $\\mu$ is the mean or expectation of the distribution.\n","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"6f4078728d71b1b791d39f218bf2bdb1","permalink":"/courses/example/stats/","publishdate":"2021-01-01T00:00:00Z","relpermalink":"/courses/example/stats/","section":"courses","summary":"Introduction to statistics for data science.\n","tags":null,"title":"Statistics","type":"book"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature.   Slides can be added in a few ways:\n Create slides using Wowchemy\u0026rsquo;s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further event details, including page elements such as image galleries, can be added to the body of this page.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"a8edef490afe42206247b6ac05657af0","permalink":"/talk/example-talk/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example-talk/","section":"event","summary":"An example talk using Wowchemy's Markdown slides feature.","tags":[],"title":"Example Talk","type":"event"},{"authors":["Christopher J. Mayerl","John G. Capano","Noraly Van Meer Mme","Hannah I. Weller","Elska B. Kaczmarek","Maria Chadam","Richard W. Blob","Elizabeth L. Brainerd","Jeanette Wyneken"],"categories":null,"content":"","date":1733011200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1733011200,"objectID":"d85ad51a0e5ffd5ce97f29d4b76d1690","permalink":"/publication/mayerl-turtle-2024/","publishdate":"2025-03-04T15:40:54.160994Z","relpermalink":"/publication/mayerl-turtle-2024/","section":"publication","summary":"ABSTRACT  Locomotion in water and on land impose dramatically different demands, yet many animals successfully move in both environments. Most turtle species perform both aquatic and terrestrial locomotion but vary in how they use their limbs. Freshwater turtles use anteroposterior movements of the limbs during walking and swimming with contralateral fore‚Äê and hindlimbs moving in synchrony. In contrast, sea turtles swim primarily with ‚Äúpowerstroke‚Äù movements, characterized by synchronous forelimb motions while the hindlimbs act as rudders. High‚Äêspeed video has been used to study powerstroking, but pectoral girdle movements and long‚Äêaxis rotation (LAR) of the humerus are likely both key components to turtle locomotor function and cannot be quantified from external video. Here, we used XROMM to measure pectoral girdle and humeral movements in a sea turtle (loggerhead, Caretta caretta ) compared to the freshwater river cooter ( Pseudemys concinna ) during terrestrial and aquatic locomotion. The largest difference among species was in yaw of the pectoral girdle during swimming, with loggerheads showing almost no yaw during powerstroking whereas pectoral girdle yaw in the cooter during rowing was over 30¬∞. The magnitude of humeral LAR was greatest during loggerhead powerstroking and the temporal pattern of supination and pronation was opposite from that of cooters. We hypothesize that these kinematic differences are driven by differences in how the limbs are used to power propulsion. Rotations at the glenoid drive the overall patterns of movement in freshwater turtles, whereas glenohumeral LAR in loggerheads is used to direct the position and orientation of the elbow, which is the joint that determines the orientation of the thrust‚Äêgenerating structure (the flipper) in loggerheads.","tags":null,"title":"Turtle Girdles: Comparing the Relationships Between Environment and Behavior on Forelimb Function in Loggerhead Sea Turtles (*Caretta caretta*) and River Cooters (*Pseudemys concinna*)","type":"publication"},{"authors":["Hayley L. Crowell","John David Curlis","Hannah I. Weller","Alison R. Davis Rabosky"],"categories":null,"content":"","date":1717200000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1717200000,"objectID":"ca82b474981f3933f516c37325c11f28","permalink":"/publication/crowell-ecological-2024/","publishdate":"2025-03-04T15:40:54.070107Z","relpermalink":"/publication/crowell-ecological-2024/","section":"publication","summary":"Abstract Ultraviolet (UV) colour patterns invisible to humans are widespread in nature. However, research bias favouring species with conspicuous colours under sexual selection can limit our assessment of other ecological drivers of UV colour, like interactions between predators and prey. Here we demonstrate widespread UV colouration across Western Hemisphere snakes and find stronger support for a predator defence function than for reproduction. We find that UV colouration has evolved repeatedly in species with ecologies most sensitive to bird predation, with no sexual dichromatism at any life stage. By modelling visual systems of potential predators, we find that snake conspicuousness correlates with UV colouration and predator¬†cone number, providing a plausible mechanism for selection. Our results suggest that UV reflectance should not be assumed absent in ‚Äúcryptically coloured‚Äù animals, as signalling beyond human visual capacities may be a key outcome of species interactions in many taxa for which UV colour is likely underreported.","tags":null,"title":"Ecological drivers of ultraviolet colour evolution in snakes","type":"publication"},{"authors":["Hannah Weller"],"categories":["misc"],"content":"The anecdote A couple of years ago, I got an email from my friend Anna Rose, at the time a textiles curator at the Rhode Island School of Design (RISD), asking me if I knew anything about infrared photography. She wanted to know if she could use it to uncover some lost history, which was such an exciting idea my mind immediately rejected it a fantasy that I was going to have to shoot down.\nThe context here was that a team of researchers from Fa\u0026rsquo;asamoa Arts (Su\u0026rsquo;a Uilisone Fitiao, Reggie Meredith Fitiao, and Mary Anne Bordonaro) were cataloging the painted barkcloths (siapo) from American Samoa in RISD\u0026rsquo;s textile collections. I could not be less qualified to describe the history or cultural significance of the siapo tradition in American Samoa, or to speak on the appropriateness of RISD having these particular examples (although the Fa\u0026rsquo;asamoa Arts site and this book by Mary J. Pritchard seem like excellent places to start learning more, and I\u0026rsquo;ve put the full explanation for the project that I was sent at the end of this post). Here\u0026rsquo;s an example they had out when I went to visit:\nThe reason she had contacted me, though, was because o\u0026rsquo;a\u0026ndash;a brown dye used in creating siapo, made from a tree of the same name\u0026ndash;oxidizes to near-black over time. So rather than intricate geometric patterns, some of the siapo the research team were cataloging looked like this:\nBut RISD also had, although apparently nobody knew how to use it, a professionally converted full-spectrum camera capable of imaging in ultraviolet and infrared. For imaging nerds: it was specifically a MaxMax-modified Canon UV-VIS-IR camera with an X-Nite 1000 infrared lens. IR photography has been plenty often in art conservation, so they thought it was worth trying to see if we could uncover the hidden siapo designs using this camera. So one afternoon I walked the ten or so minutes from Brown\u0026rsquo;s campus to RISD\u0026rsquo;s collections to help out with this.\nI will be honest: I thought I would show them how to use the camera, and then be the wet blanket explaining why it didn\u0026rsquo;t reveal the design. I think when we photograph things in UV or IR, we\u0026rsquo;re always hoping to reveal a hidden world, the hyperspectral equivalent of the numbers on the Ishihara colorblindness test cards. And as a scientist I think I\u0026rsquo;m inclined to be skeptical of anything I want badly to be true. In this case, for the design to be visible, there would need to be a difference in infrared absorption between the two dyes, which seemed unlikely since lama (the black dye) is a combination of o\u0026rsquo;a (the brown dye) and burned candlenut kernels. Differences in IR absorption would probably be minor and hard to see without an intense source of infrared light.\nAnd guess what! Yeah, we saw nothing. I didn\u0026rsquo;t take a picture, but fortunately, I can recreate for you almost exactly what it looked like when we popped the IR lens on the camera and pointed it at the siapo, because it was literally just black:\nAt this point I said something like well, haha, that\u0026rsquo;s usually how it goes, sorry, although if you want to try something else, I guess you could get a light source with a lot of infrared light instead of photographing it just under your presumably fluorescent ceiling lights, but I wouldn\u0026rsquo;t get your hopes up.\nAnd Anna Rose asked me \u0026ldquo;Well, what\u0026rsquo;s the cheapest source of infrared light?\u0026rdquo; and I said, \u0026ldquo;THE SUN, I guess, but it\u0026rsquo;s not like you can take this 150-year-old barkcloth outside,\u0026rdquo; and she said oh we totally can, as long as it stays on museum grounds, and she and her intern promptly wheeled it through the museum cafe and onto the patio where people were having their lunch.\nIt was fine, but this move felt so illegal to me at the time that I honestly feel like writing about it now is incriminating. I don\u0026rsquo;t know the statute of limitations on taking a piece of preserved cultural heritage onto a museum cafe patio. On the other hand it worked.\nI distinctly remember that her intern pulled over a chair from an empty table, stood on top of it, flipped open the camera with the IR lens still on, and actually gasped out loud. And then she showed me, and I also gasped out loud.\nI\u0026rsquo;m not even sure how to describe how satisfying this moment was. It was like a scenario I would make up in a daydream about all my esoteric color knowledge suddenly feeling useful. A lot of science is working for months or years on something you find interesting only to show it to other people and have most of them say at most, \u0026ldquo;oh, nice,\u0026rdquo; and this took about fifteen minutes to provide something tangible that everyone found exciting. This is how science works in like, forensic TV shows.\nThe irradiance The first image we took with the IR lens was indoors with a standard fluorescent ceiling light. Fluorescent lights are meant to more or less emulate the way that daylight appears to our eyes by providing peaks of emitted light around the peak wavelength sensitivities of our cone cells (around 430, 540, and 570 nm; image from Waveform Lighting):\nThose peaks do a great job of illuminating objects for human vision so that they appear more or less as they would under natural sunlight. But they provide very little light outside of those peaks, especially for red and infrared wavelengths (\u0026gt;650nm). Compare that to the amount of light above 650nm in sunlight:\nPeople can\u0026rsquo;t see infrared light, so this doesn\u0026rsquo;t make a big practical difference in how things look indoors vs. outdoors. But the IR lens on the camera filters out any non-infrared light, which under a fluorescent lightsource means it\u0026rsquo;s just filtering out ALL of the available environmental light. Even if a surface differs in how much it absorbs and reflects IR wavelengths\u0026ndash;leading to a difference in infrared \u0026ldquo;color\u0026rdquo;\u0026ndash;we weren\u0026rsquo;t able to see that when there was no available infrared light to absorb or reflect.\nI like this example because it\u0026rsquo;s just enough outside the normal human sensory experience to feel non-obvious, but it\u0026rsquo;s a phenomenon that happens all the time with imaging without us ever clocking it. We think of color as an inherent property of an object (e.g. \u0026ldquo;my dress is green, it just looks black with the bad lighting in the hallway\u0026rdquo;), but it\u0026rsquo;s not purely academic to point out that color is an emergent property of how a surface interacts with available environmental light. It can have practical consequences!\nA much more succinct, intuitive example is Olfaur Eliasson\u0026rsquo;s \u0026ldquo;Room for One Colour\u0026rdquo;, which he very neatly demonstrated in about ten seconds of a Netflix trailer:\nInformation about the siapo project Here\u0026rsquo;s the info that RISD sent me when I asked. Any questions about the project should certainly go to the research team or possibly the museum, not me.\n On June 7th, 2022, researchers froman indigenous arts nonprofit in American Samoa came to the RISD Museum to view Samoan barkcloth, called siapo. Their organization, known as Folauga o le Tatau Malaga Aganu‚Äôu Fa‚Äôasamoa (a.k.a. Fa‚Äôasamoa Arts), seeks to educate others about the indigenous arts of American Samoa and promote the integration of these ancient art forms into modern lives.Their travel was funded by the Amerika Samoa Humanities Council, and will culminate in a new creative nonfiction book: U‚ÄôA: Ancient Art through Contemporary Eyes. The research team consists of three people. Su‚Äôa Uilisone Fitiao is a Tufuga Ta Tatau (a traditional tattoo master), a siapo maker, and woodcarver. Reggie Meredith Fitiao (MFA) is a siapo maker and art educator. Mary Anne Bordonaro is a grant writer, short story writer, and will be the author of the book being funded by this grant.\n ","date":1717027200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1714743592,"objectID":"87b905ad42d72217cda8192fee4632bd","permalink":"/post/an-anecdote-about-irradiance/","publishdate":"2024-05-30T00:00:00Z","relpermalink":"/post/an-anecdote-about-irradiance/","section":"post","summary":"A good reminder about why color scientists care so much about light sources.","tags":["color","light","risd","art"],"title":"An anecdote about irradiance","type":"post"},{"authors":["Hannah Weller"],"categories":["recolorize"],"content":"As much as I would like recolorize to provide a single perfect solution to color segmentation that requires no user input, I made it modular for a reason. Supposed \u0026ldquo;one-click\u0026rdquo; methods can be really frustrating when they just barely don\u0026rsquo;t work for your use case and offer no simple way to fix their output.\nI know that recolorize will often produce results that are pretty good but not quite right: maybe a part of your image is in deep shadows or a specimen was damaged in preservation. I have certainly had cases where I wanted to open up a recolorize object in ImageJ to just outline a particularly troublesome region of an image, and others using the package have voiced similar suggestions.\nWith that in mind, I wrote an (experimental) function to allow the user to select a region of the image to alter. This is obviously not as sophisticated as image analysis software with a dedicated user interface, but I think it works really well when you have small fixes to implement, because it means you don\u0026rsquo;t have to clumsily go between a bunch of different softwares for different things.\nI tried to keep this function as lightweight as possible, so it\u0026rsquo;s not fancy, but maybe it will be useful. Let\u0026rsquo;s define the function first and then I\u0026rsquo;ll demonstrate how it works:\nlibrary(recolorize) recolor_selection \u0026lt;- function(rc_obj, color_to = 1, selection = \u0026quot;rect\u0026quot;, locator_color = \u0026quot;red\u0026quot;, n_polygons = 1, plotting = TRUE, recolor_background = FALSE) { # store old map rc_original_pix \u0026lt;- rc_obj$pixel_assignments # choose region selection method: selection \u0026lt;- match.arg(arg = selection, choices = c(\u0026quot;rectangle\u0026quot;, \u0026quot;polygon\u0026quot;)) # make image: img \u0026lt;- recoloredImage(rc_obj, type = \u0026quot;raster\u0026quot;) layout(1); par(mar = rep(0, 4)) plot(img) # plot horizontal and vertical bounds to keep user from selecting # outside the image boundaries: abline(h = c(0, dim(img)[1]), v = c(0, dim(img)[2]), col = \u0026quot;darkgrey\u0026quot;) # user-select area and change color of pixels inside region of interest if (selection == \u0026quot;rectangle\u0026quot;) { # select a rectangle u \u0026lt;- spatstat.geom::clickbox(add = TRUE, col = locator_color) # store the input u$xrange \u0026lt;- round(u$xrange) u$yrange \u0026lt;- round(u$yrange) # flip the yrange (in the image, y is numbered bottom to top, but in an array # it's indexed top to bottom) u$yrange \u0026lt;- dim(img)[1] - u$yrange # and change pixel assignments to new color rc_obj$pixel_assignments[u$yrange[1]:u$yrange[2], u$xrange[1]:u$xrange[2]] \u0026lt;- color_to } else if (selection == \u0026quot;polygon\u0026quot;) { # select polygon(s) u \u0026lt;- spatstat.geom::clickpoly(add = TRUE, col = locator_color, np = n_polygons) u$xrange \u0026lt;- round(u$xrange) u$yrange \u0026lt;- round(u$yrange) u$bdry \u0026lt;- lapply(u$bdry, \\(x) lapply(x, round)) # find all pixels inside of the bounding box for the polygon(s) xy \u0026lt;- expand.grid(u$xrange[1]:u$xrange[2], u$yrange[1]:u$yrange[2]) # for every polygon... for (i in 1:length(u$bdry)) { pol \u0026lt;- u$bdry[[i]] # find which points in the bounding box fall inside the polygon if (i == 1) { col_idx \u0026lt;- xy[which(secr::pointsInPolygon(xy, do.call(cbind, pol), logical = T)), ] } else { new_idx \u0026lt;- xy[which(secr::pointsInPolygon(xy, do.call(cbind, pol), logical = T)), ] col_idx \u0026lt;- rbind(col_idx, new_idx) } } # reverse x/y order, y col_idx \u0026lt;- as.matrix(col_idx[ , 2:1]) col_idx[,1] \u0026lt;- dim(img)[1] - col_idx[,1] # and change color rc_obj$pixel_assignments[col_idx] \u0026lt;- color_to } # make sure the background is still transparent if (!recolor_background) { rc_obj$pixel_assignments[rc_original_pix == 0] \u0026lt;- 0 } # if plotting, plot if (plotting) { # draw box or polygon around the region that was changed plot_region \u0026lt;- function() { xdim \u0026lt;- dim(rc_obj$pixel_assignments)[2] ydim \u0026lt;- dim(rc_obj$pixel_assignments)[1] if (selection == \u0026quot;rectangle\u0026quot;) { rect(xleft = u$xrange[1] / xdim, xright = u$xrange[2] / xdim, ybottom = 1 - u$yrange[1] / ydim, ytop = 1 - u$yrange[2] / ydim, border = locator_color) } else { for (i in 1:length(u$bdry)) { polygon(u$bdry[[i]]$x / xdim, u$bdry[[i]]$y / ydim, border = locator_color) } } } # reset graphical parameters when function exits: current_par \u0026lt;- graphics::par(no.readonly = TRUE) on.exit(graphics::par(current_par)) # set layout graphics::layout(matrix(1:3, nrow = 1), widths = rep(1, 3)) # plot original image graphics::par(mar = c(0, 0, 2, 0)) recolorize::plotImageArray(rc_obj$original_img, main = \u0026quot;Recolored original\u0026quot;) plot_region() # plot old zone map with polygon/rectangle graphics::par(mar = c(0, 0, 2, 0)) recolorize::plotImageArray(constructImage(rc_original_pix, rc_obj$centers), main = \u0026quot;Input\u0026quot;) plot_region() # plot new color map \u0026amp; palette graphics::par(mar = c(0, 0, 2, 0)) recolorize::plotImageArray(recoloredImage(rc_obj), main = \u0026quot;Output\u0026quot;) plot_region() } return(rc_obj) }  To demonstrate, let\u0026rsquo;s take one of the beetle images that come with the package:\nimg \u0026lt;- system.file(\u0026quot;extdata/fulgidissima.png\u0026quot;, package = \u0026quot;recolorize\u0026quot;) rc \u0026lt;- recolorize2(img, bins = 3, cutoff = 60, plotting = FALSE)  ## ## Using 3^3 = 27 total bins  rc2 \u0026lt;- mergeLayers(rc, list(c(3, 6)), plotting = FALSE) plot(rc2)  A couple of things bother me in the resulting zone map. These images are of pinned specimens, so when initially processing them, I removed the region of the pin as \u0026ldquo;background\u0026rdquo;, leaving this somewhat awkward-looking hole on the right side of the beetle. Second, in real life, apparently that pin created a shadow when the photograph was taken, because we can see it apparently created enough of a shadow that some pixels on that right side got clustered with the dark red/brown color of the bands on the elytra.\nWe can use the polygon selection option of the recolor_selection function to select that area and force all pixels inside that region to change to color class 2 (green).\nrc3 \u0026lt;- recolor_selection(rc2, selection = \u0026quot;polygon\u0026quot;, locator_color = \u0026quot;yellow\u0026quot;, n_polygons = 1, plotting = TRUE, color_to = 2)  This will pop up a plot of the recolorize object and prompt you to draw a region (in this case, a single polygon). A blog post can\u0026rsquo;t recreate the user interface aspect where I actually selected a polygon, but here\u0026rsquo;s a screenshot just before I clicked \u0026lsquo;Finish\u0026rsquo;:\nHere, I selected the area around the pin that I want to change to green. (I\u0026rsquo;m ignoring the groove between the elytra, which is also clustered with class 5; if I were actually trying to analyse this image with respect to a specific biological question I\u0026rsquo;d have to decide how to handle it, since obviously it doesn\u0026rsquo;t belong to the same color class as the red bands. But this is an example! I do what I want!)\nThen I get a diagnostic plot of the region I recolored: You can see that the pinhole is still present though. That\u0026rsquo;s the default behavior of the function as I wrote it, since you might be trying to fix something on the border of the image and you don\u0026rsquo;t want to accidentally color in the background. But you can turn that off by setting recolor_background = TRUE.\nrc4 \u0026lt;- recolor_selection(rc2, selection = \u0026quot;polygon\u0026quot;, locator_color = \u0026quot;yellow\u0026quot;, n_polygons = 1, plotting = TRUE, color_to = 2, recolor_background = TRUE)  Now the pinhole is filled in.\nAnd that\u0026rsquo;s it! You can use rectangles instead by setting selection = \u0026quot;rectangle\u0026quot;, and you can select more than one polygon at a time. This is not currently a formal part of the package because I think it would need way more fine-tuning before that\u0026rsquo;s feasible (and at that point, I might as well make an R Shiny app). For example, right now this function doesn\u0026rsquo;t record the changes as part of the call element of a recolorize object, which otherwise records everything that produced it:\n# these are different: print(rc$call)  ## recolorize2(img = img, bins = 3, cutoff = 60, plotting = FALSE)  print(rc2$call)  ## [[1]] ## recolorize2(img = img, bins = 3, cutoff = 60, plotting = FALSE) ## ## [[2]] ## mergeLayers(recolorize_obj = rc, merge_list = list(c(3, 6)), ## plotting = FALSE)  # but these are exactly like rc2: print(rc3$call)  ## [[1]] ## recolorize2(img = img, bins = 3, cutoff = 60, plotting = FALSE) ## ## [[2]] ## mergeLayers(recolorize_obj = rc, merge_list = list(c(3, 6)), ## plotting = FALSE)  print(rc4$call)  ## [[1]] ## recolorize2(img = img, bins = 3, cutoff = 60, plotting = FALSE) ## ## [[2]] ## mergeLayers(recolorize_obj = rc, merge_list = list(c(3, 6)), ## plotting = FALSE)  I don\u0026rsquo;t necessarily want to build out a whole suite of tools for taking manual input like this. I think that would defeat the purpose of the broader package (to automate what can be automated), and I think it could lead down the unproductive road of me trying to recreate a worse version of ImageJ. But I can definitely appreciate the need for something like this, and hopefully it is useful to multiple people.\nIf you do use this tool and have some feedback for what else you\u0026rsquo;d need from it, or if you implement your own solution, please do get in touch. I think this could eventually be a useful addition to the package and feedback is the best way for me to know what\u0026rsquo;s working and what\u0026rsquo;s missing.\n","date":1711065600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1711028983,"objectID":"217493dd2d9979ee5b19be5655889a87","permalink":"/post/editing-zone-maps-with-user-selected-regions/","publishdate":"2024-03-22T00:00:00Z","relpermalink":"/post/editing-zone-maps-with-user-selected-regions/","section":"post","summary":"Sometimes you just need to tweak things by hand.","tags":["color","image processing","R","r packages"],"title":"Editing zone maps with user-selected regions","type":"post"},{"authors":[],"categories":["personal"],"content":"I wanted to write an update for three general reasons:\n Make sure I still know how to use my own website. Indicate to anyone visiting my website that it\u0026rsquo;s still an active repository of my academic activities. I think if I were looking at someone\u0026rsquo;s website and it hadn\u0026rsquo;t been updated for a year, I\u0026rsquo;d want the reassurance. Writing gets harder the less of it you do.  I defended my dissertation more or less a year ago (April 4th, 2023), finished up my work at Brown University, and took the summer off to recover. This was not as restful as I was hoping it would be. A couple of weeks after I defended, I got back some paper reviews that made me doubt myself so hard they triggered like four panic attacks in the space of a week. Then I had a family health situation that drained the color out of the whole world for a while.\nGood things happened, too. I got to meet my postdoc lab in Helsinki for the first time, and after that I got to go to Cairns for a conference (ICVM), and Sydney for two days after that. The conference was good\u0026ndash;I presented recolorize and it was received much better than I\u0026rsquo;d hoped it would be, and I saw a lot of people I really like. Australia, as people told me ahead of time, was like a mashup of visiting an alien planet and meeting a bunch of the A-list celebrities of charismatic megafauna\u0026ndash;mudskippers, fruit bats, ibises, echidnas, you get it.\n(A personal highlight was seeing some archerfish in an awful ditch next to a highway. I had never seen these iconic freshwater fish outside of an aquarium, and there were like three of them there, spitting insects off the riverbank foliage, hiding underneath a rusty shopping cart that had been overturned in the water. It reminded me of a Lewis Thomas essay called Ponds where he describes city-wide disgust for goldfish in a pond that had formed on an abandoned lot:\n Goldfish in a glass bowl are harmless to the human mind, maybe even helpful to minds casting about for something, anything, to think about. But goldfish let loose, propagating themselves, worst of all surviving in what has to be a sessile eddy of the East River, somehow threaten us all. We do not like to think that life is possible under some conditions, especially the conditions of a Manhattan pond. There are four abandoned tires, any number of broken beer bottles, fourteen shoes and a single sneaker, and a visible layer, all over the surface, of that grayish-green film that settles on all New York surfaces.\nThe mud at the banks of the pond is not proper country mud but reconstituted Manhattan landfill, ancient garbage, fossilized coffee grounds and grapefruit rind, the defecation of a city. For goldfish to be swimming in such water, streaking back and forth mysteriously in small schools, feeding, obviously feeding, looking as healthy and well-off as goldfish in the costliest kind of window-box aquarium, means something is wrong with our standards. It is, in some deep sense beyond words, insulting.\n But I have maybe too much to say about that.)\nSummer was anyways a transitional period, and maybe it\u0026rsquo;s good that I had to deal with multiple kinds of transitions. I did respond successfully to the reviews in the end, and learned how to deal with the intensity of the family health thing. I also moved to Helsinki in September.\nI have so much to say about living in Helsinki and starting my postdoc that I don\u0026rsquo;t really know how to say it. I know doing a PhD is stressful for most people, but in different ways. I spent most of mine feeling useless and irrelevant. I was worried, really worried, when I started my postdoc, that I would feel like that again. I don\u0026rsquo;t. The specifics aren\u0026rsquo;t really worth elaborating\u0026ndash;but I think any moment of transition is a time to figure out how much of your stress you carry with you, and how much of it comes from around you. What I can say is that I really like it here. I feel like my skills are valued, and like I\u0026rsquo;m learning a lot from the people I work with. I don\u0026rsquo;t feel like a crab in the crabs-in-a-bucket metaphor anymore. I feel like, you know, one of those cool decorator crabs that takes good pieces of their environment and sticks them on their carapace and walks around with it. I also feel more resilient, which is not something I expected to be able to say.\nHelsinki is restorative for someone like me. It\u0026rsquo;s very open and quiet, and there are trees everywhere. Obviously the social welfare is good, once you get into the bureaucracy, and obviously it\u0026rsquo;s cold and dark in the winter. I like how quickly the days change length. Right now we\u0026rsquo;re getting like six more minutes of sunlight every day.\nI\u0026rsquo;m gaining the skills I know I\u0026rsquo;ll need if I want a tenure-track job: I\u0026rsquo;m mentoring master\u0026rsquo;s students, gaining skills with genomic data, learning how to do genotype-phenotype mapping, and getting deeper into various machine learning models for image analysis. I think those better belong on a less personal update. Hopefully in papers!\nSo I\u0026rsquo;ll end by just saying how surprised I am that I\u0026rsquo;ve recovered so fully from how I felt during my PhD. I don\u0026rsquo;t feel like I\u0026rsquo;m wasting my time on things that don\u0026rsquo;t matter. I don\u0026rsquo;t feel like nobody cares what I do. I still get joy from science\u0026ndash;I still can\u0026rsquo;t shut up about projects I find interesting. I\u0026rsquo;m starting to think with more tools and at bigger scales. I think I was worried I\u0026rsquo;d let that part of myself\u0026mdash;I don\u0026rsquo;t know. Atrophy. I so long gritting my teeth and just trying not to freak out. It\u0026rsquo;s hard for creativity to really drive your actions when your primary workplace emotion is \u0026ldquo;biting on a stick\u0026rdquo; and you think of yourself as fragile. I made so many decisions out of fear and thinking about it makes me sad.\nI was talking with someone important to me about what constitutes an unacceptably bad PhD experience. He pointed out that some people, even years after finishing their PhDs, can\u0026rsquo;t talk about that time in their lives with clarity: the stress is so intense that it seems to warp their ability to describe what happened. I don\u0026rsquo;t think that\u0026rsquo;s me, but I was worried it would be. I think I came out okay. It doesn\u0026rsquo;t hurt to think about that time, even though I have plenty of things I would like to change. I made it. I want to keep going. That\u0026rsquo;s the update.\n","date":1710979200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1711029123,"objectID":"b2352f103e176bc3fc4876616623739b","permalink":"/post/march-2024-update/","publishdate":"2024-03-21T00:00:00Z","relpermalink":"/post/march-2024-update/","section":"post","summary":"I wanted to write an update for three general reasons:\n Make sure I still know how to use my own website. Indicate to anyone visiting my website that it\u0026rsquo;s still an active repository of my academic activities.","tags":[],"title":"March 2024 update","type":"post"},{"authors":["Hannah I. Weller","Anna E. Hiller","Nathan P. Lord","Steven M. Van¬†Belleghem"],"categories":null,"content":"","date":1706745600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1706745600,"objectID":"b1328823d2646267e9aab3c2578bb5fe","permalink":"/publication/weller-recolorize-2024/","publishdate":"2025-03-04T15:40:54.088624Z","relpermalink":"/publication/weller-recolorize-2024/","section":"publication","summary":"Colour pattern variation provides biological information in fields ranging from disease ecology to speciation dynamics. Comparing colour pattern geometries across images requires colour segmentation, where pixels in an image are assigned to one of a set of colour classes shared by all images. Manual methods for colour segmentation are slow and subjective, while automated methods can struggle with high technical variation in aggregate image sets. We present recolorize, an R package toolbox for human-¬≠subjective colour segmentation with functions for batch-¬≠processing low-¬≠variation image sets and additional tools for handling images from diverse (high-¬≠variation) sources. The package also includes export options for a variety of formats and colour analysis packages. This paper illustrates recolorize for three example datasets, including high variation, batch processing and combining with reflectance spectra, and demonstrates the downstream use of methods that rely on this output.","tags":null,"title":"recolorize: An R package for flexible colour segmentation of biological images","type":"publication"},{"authors":["James P. Tumulty","Sara E. Miller","Steven M. Van Belleghem","Hannah I. Weller","Christopher M. Jernigan","Sierra Vincent","Regan J. Staudenraus","Andrew W. Legan","Timothy J. Polnaszek","Floria M.K. Uy","Alexander Walton","Michael J. Sheehan"],"categories":null,"content":"","date":1701388800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1701388800,"objectID":"55da208b2dd48bc20185c5e2359f2dce","permalink":"/publication/tumulty-evidence-2023/","publishdate":"2025-03-04T15:40:54.156717Z","relpermalink":"/publication/tumulty-evidence-2023/","section":"publication","summary":"","tags":null,"title":"Evidence for a selective link between cooperation and individual recognition","type":"publication"},{"authors":["Shannon E. Paquette","Nathan R. Martin","April Rodd","Katherine E. Manz","Eden Allen","Manuel Camarillo","Hannah I. Weller","Kurt Pennell","Jessica S. Plavicki"],"categories":null,"content":"","date":1698796800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1698796800,"objectID":"a9c84b94e6e6ebb5254ede5804e3db1f","permalink":"/publication/paquette-evaluation-2023/","publishdate":"2025-03-04T15:40:54.166255Z","relpermalink":"/publication/paquette-evaluation-2023/","section":"publication","summary":"","tags":null,"title":"Evaluation of Neural Regulation and Microglial Responses to Brain Injury in Larval Zebrafish Exposed to Perfluorooctane Sulfonate","type":"publication"},{"authors":["Karly E Cohen","Hannah I Weller","Mark W Westneat","Adam P Summers"],"categories":null,"content":"","date":1688169600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1688169600,"objectID":"71b0bc6b905efb2f7c37290de1daebb8","permalink":"/publication/cohen-evolutionary-2023/","publishdate":"2025-03-04T15:40:54.116755Z","relpermalink":"/publication/cohen-evolutionary-2023/","section":"publication","summary":"Vertebrate dentitions are often collapsed into a few discrete categories, obscuring both potentially important functional differences between them and insight into their evolution. The terms homodonty and heterodonty typically conflate tooth morphology with tooth function, and require context-dependent subcategories to take on any specific meaning. Qualifiers like incipient, transient, or phylogenetic homodonty attempt to provide a more rigorous definition but instead highlight the difficulties in categorizing dentitions. To address these issues, we recently proposed a method for quantifying the function of dental batteries based on the estimated stress of each tooth (inferred using surface area) standardized for jaw out-lever (inferred using tooth position). This method reveals a homodonty-heterodonty functional continuum where small and large teeth work together to transmit forces to a prey item. Morphological homodonty or heterodonty refers to morphology, whereas functional homodonty or heterodonty refers to transmission of stress. In this study, we use Halichoeres wrasses to explore how functional continuum can be used in phylogenetic analyses by generating two continuous metrics from the functional homodontyheterodonty continuum. Here we show that functionally heterodont teeth have evolved at least three times in Halichoeres wrasses. There are more functionally heterodont teeth on upper jaws than on lower jaws, but functionally heterodont teeth on the lower jaws bear significantly more stress. These nuances, which have functional consequences, would be missed by binning entire dentitions into discrete categories. This analysis points out areas worth taking a closer look at from a mechanical and developmental point of view with respect to the distribution and type of heterodonty seen in different jaws and different areas of jaws. These data, on a small group of wrasses, suggest continuous dental variables can be a rich source of insight into the evolution of fish feeding mechanisms across a wider variety of species.","tags":null,"title":"The Evolutionary Continuum of Functional Homodonty to Heterodonty in the Dentition of textitHalichoeres Wrasses","type":"publication"},{"authors":["Hannah Weller"],"categories":[],"content":"The goal of this post is to demonstrate how I use the dichromat R package to check if my custom color palettes are colorblind-friendly.\nA lot has been written about choosing colorblind-friendly color palettes when visualizing data, and I won\u0026rsquo;t rehash here why it\u0026rsquo;s important. Honestly, the colorblind scientists I know tend to be pretty pragmatic about this problem (most of them will tell me a version of \u0026ldquo;I can usually figure it out from context\u0026rdquo;), and I do think that folks can get a little intense about this particular aspect of visual accessibility, sometimes at the expense of other aspects. But since choosing colorblind-friendly palettes is easy, I don\u0026rsquo;t see why we shouldn\u0026rsquo;t do it any time color is being used to indicate something on a graph.\nA lot of the advice that I run into when I\u0026rsquo;m trying to choose a color palette reduces down to three general categories: 1) use a pre-generated palette that has been carefully checked for being distinguishable for all the most common types of colorblindness, 2) rules of thumb like \u0026ldquo;stay away from green and orange,\u0026rdquo; or 3) print it out in black and white and/or run your graph through an online color blindness simulator to see if it looks okay. Personally, I find the second one too vague, the third one too cumbersome for tweaking, and the first one too inflexible when I want the colors to represent something specific, like habitat type. If you are also stubborn and detail-oriented, you know what I mean.\nAs for the dichromat package: nothing I write here is prescriptive! But this is an option I happen to use a lot, and the dichromat package has been incredibly useful for me in this context, so I wanted to show how I use it.\nThe dichromat package The main function of the dichromat package is dichromat (I love when packages do this), and it approximates the effects of three of the most common forms of color blindness.\nlibrary(dichromat) red_green_colors \u0026lt;- RColorBrewer::brewer.pal(11, \u0026quot;RdYlGn\u0026quot;) # convert to the three dichromacy approximations protan \u0026lt;- dichromat(red_green_colors, type = \u0026quot;protan\u0026quot;) deutan \u0026lt;- dichromat(red_green_colors, type = \u0026quot;deutan\u0026quot;) tritan \u0026lt;- dichromat(red_green_colors, type = \u0026quot;tritan\u0026quot;) # plot for comparison layout(matrix(1:4, nrow = 4)); par(mar = rep(1, 4)) recolorize::plotColorPalette(red_green_colors, main = \u0026quot;Trichromacy\u0026quot;) recolorize::plotColorPalette(protan, main = \u0026quot;Protanopia\u0026quot;) recolorize::plotColorPalette(deutan, main = \u0026quot;Deutanopia\u0026quot;) recolorize::plotColorPalette(tritan, main = \u0026quot;Tritanopia\u0026quot;)  Here I\u0026rsquo;m demonstrating the functions on a red-yellow-green color palette, which unsurprisingly is pretty terrible for red-green colorblindness (protanopia, or red-down, and deutanopia, or green-down).\nFor convenience if I\u0026rsquo;m tweaking colors, I\u0026rsquo;ll usually write a function to do this automatically:\ncheck_cb \u0026lt;- function(palette, return_cb_palettes = FALSE, ...) { # make an empty list cb_palettes \u0026lt;- setNames(vector(\u0026quot;list\u0026quot;, length = 3), nm = c(\u0026quot;protan\u0026quot;, \u0026quot;deutan\u0026quot;, \u0026quot;tritan\u0026quot;)) # generate colorblindness approximations for (i in 1:length(cb_palettes)) { cb_palettes[[i]] \u0026lt;- dichromat::dichromat(palette, names(cb_palettes)[i]) } # reset graphical parameters when function exits: current_par \u0026lt;- graphics::par(no.readonly = TRUE) on.exit(graphics::par(current_par)) # plot for comparison layout(matrix(1:4, nrow = 4)); par(mar = rep(1, 4)) recolorize::plotColorPalette(palette, main = \u0026quot;Trichromacy\u0026quot;, ...) pnames \u0026lt;- c(\u0026quot;Protanopia\u0026quot;, \u0026quot;Deutanopia\u0026quot;, \u0026quot;Tritanopia\u0026quot;) for (i in 1:3) { recolorize::plotColorPalette(cb_palettes[[i]], main = pnames[i], ...) } if (return_cb_palettes) { return(cb_palettes) } } check_cb(RColorBrewer::brewer.pal(9, \u0026quot;RdBu\u0026quot;))  These are only simulations, and individuals vary in the type and severity of their colorblindness. In particular, these approximations are for types of color blindness where one of the cone types is completely absent, but it\u0026rsquo;s much more common for people to just have reduced sensitivity in one of these cone types (protanomaly, deuteranomaly, and tritanomaly). But as far as I\u0026rsquo;m aware, you can generally assume that if two colors are distinguishable to a protanopic viewer they will also be distinguishable to a protanomalous viewer, and so on.\nStill, although most of us would probably know to stay away from a red-green color palette, plenty of other common examples are just as egregious. For example, the default ggplot2 color scale is about the worst thing you could possibly use, because all of the colors vary only in hue and not at all in luminance:\nggplot_colors \u0026lt;- scales::hue_pal()(10) check_cb(ggplot_colors)  To be fair, a lot of these are hard to distinguish for trichromatic viewers. (I don\u0026rsquo;t actually know why the developers chose this palette, but I actually wonder if it\u0026rsquo;s because it\u0026rsquo;s so obviously bad that they were hoping users would always change it to suit their needs?)\nThe terrain palette, mostly used for terrestrial maps, is also particularly bad:\nterrain \u0026lt;- terrain.colors(10) check_cb(terrain)  The same is true of red-green-blue palettes, which are often the default when we need three colors:\n# ggplot strikes again, but this time the colors look deceptively different for trichromats! check_cb(scales::hue_pal()(3))  Let\u0026rsquo;s take this last example, because it\u0026rsquo;s one that I\u0026rsquo;ve run into a version of many times. Let\u0026rsquo;s say it\u0026rsquo;s the standard in your field to represent something with red, green, and blue (e.g. rotational axes). You don\u0026rsquo;t have to just stay away from red and green as a rule‚Äîyou just need to tweak the particular colors you use so that something other than color distinguishes them. In practice the easiest thing is usually the brightness of the color.\n# convert to an RGB matrix: original_rgb \u0026lt;- t(col2rgb(scales::hue_pal()(3)) / 255) # decrease brightness of green: tweaked_rgb \u0026lt;- recolorize::adjust_color(original_rgb, which_colors = 2, brightness = 0.7) tweaked_rgb \u0026lt;- rgb(tweaked_rgb) # check again - looks much better check_cb(tweaked_rgb)  Sometimes even color palettes that intuitively seem like they should be really robust to color blindness turn out not to be! For example, check out red vs. a medium (60%) grey:\nred_grey \u0026lt;- c(\u0026quot;#FF0000\u0026quot;, \u0026quot;#636363\u0026quot;) check_cb(red_grey)  They\u0026rsquo;re not identical, but for protanopic viewers, they are very similar. I remember being taught a rule of thumb that any bright color + a greyscale value were pretty much always a safe combination, but if you are not sensitive to long-wavelength light, the difference between a deep red and a dark grey is not much. I didn\u0026rsquo;t realize this until I was navigating a digital interface where reserved spots where marked with grey and open ones with red, and the protanopic person I was with made a comment about wishing the interface had different colors for reserved seats!\nTrying the color palettes out in your actual graphs It\u0026rsquo;s perfectly reasonable to look at the last example and point out that although the red and grey colors for the protanopic viewer are very similar, they are not actually identical, and therefore you could get away with them. That\u0026rsquo;s true! This is where things start to get subjective. Personally, I find that I have to actually try the color palettes out in the graphs where I intend to use them. This is where the dichromat() function really comes in handy, since you can just swap out the different color palettes.\nSometimes the color is redundant with some other way that information is arranged‚Äîin that case, it doesn\u0026rsquo;t really matter if your color palette is colorblind-friendly:\n# create a dataset # feat. creatures from Animal Land Where There Are No People (1897) species \u0026lt;- c(rep(\u0026quot;womp\u0026quot;, 2), rep(\u0026quot;boddle\u0026quot;, 2), rep(\u0026quot;temmalunk\u0026quot;, 2)) anatomy \u0026lt;- rep(c(\u0026quot;legs\u0026quot; , \u0026quot;eyes\u0026quot;) , 3) number \u0026lt;- abs(rnorm(6, 2, 2)) size \u0026lt;- abs(rnorm(6, 5, 2)) data \u0026lt;- data.frame(species, anatomy, number, size) # graph library(ggplot2) p \u0026lt;- ggplot(data, aes(fill = anatomy, y = number, x = anatomy)) + geom_bar(position = \u0026quot;dodge\u0026quot;, stat = \u0026quot;identity\u0026quot;) + facet_wrap(~species) + theme_bw() + xlab(\u0026quot;\u0026quot;) # trichomatic: p + scale_fill_manual(values = red_grey) + ggtitle(\u0026quot;Trichromacy\u0026quot;)  # and protanopic: p + scale_fill_manual(values = dichromat(red_grey, type = \u0026quot;protan\u0026quot;)) + ggtitle(\u0026quot;Protanopia\u0026quot;)  But in some cases, color is the only thing that identifies groups, so having near-identical colors actually makes it really hard to understand the graph:\nx \u0026lt;- rnorm(40, 5) df \u0026lt;- data.frame(x = x, group = c(rep(\u0026quot;y1\u0026quot;, 20), rep(\u0026quot;y2\u0026quot;, 20)), value = c(x[1:20] + rnorm(10, sd = 2), x[21:40]*2 + rnorm(10, sd = 2))) p \u0026lt;- ggplot(df, aes(x = x, y = value, color = group)) + geom_point(size = 3) + theme_bw() p + scale_color_manual(values = red_grey) + ggtitle(\u0026quot;Trichromacy\u0026quot;)  p + scale_color_manual(values = dichromat(red_grey, type = \u0026quot;protan\u0026quot;)) + ggtitle(\u0026quot;Protanopia\u0026quot;)  Like I said‚Äîat this point, this is a subjective matter of how much you care about aesthetics and this particular form of accessibility. If I really wanted to use a red and grey color palette, I would choose a darker grey and a more orange-y red. If I just wanted two colors that are easy to distinguish, I would choose a light grey and a dark grey, or blue and red:\nred_grey_2 \u0026lt;- c(\u0026quot;tomato\u0026quot;, \u0026quot;grey30\u0026quot;) grey2 \u0026lt;- c(\u0026quot;grey70\u0026quot;, \u0026quot;grey20\u0026quot;) red_blue \u0026lt;- c(\u0026quot;tomato\u0026quot;, \u0026quot;dodgerblue\u0026quot;) p + scale_color_manual(values = dichromat(red_grey_2, type = \u0026quot;protan\u0026quot;)) + ggtitle(\u0026quot;Altered red/grey palette\u0026quot;)  p + scale_color_manual(values = dichromat(grey2, type = \u0026quot;protan\u0026quot;)) + ggtitle(\u0026quot;\u0026quot;) + ggtitle(\u0026quot;Greyscale palette\u0026quot;)  p + scale_color_manual(values = dichromat(red_blue, type = \u0026quot;protan\u0026quot;)) + ggtitle(\u0026quot;Blue and red palette\u0026quot;)  Feel free to try out the above code using the other two types of color blindness‚ÄîI found they still made the two groups easy to distinguish.\nBut with all that said, I don\u0026rsquo;t think that this in particular is worth agonizing about too much. You\u0026rsquo;ll always be able to find some advice that this color combination or that one is totally unacceptable, whether it\u0026rsquo;s because they are not colorblind-friendly, or not perceptually uniform, or won\u0026rsquo;t print out well for a greyscale printer. At a certain point you have to declare something is good enough for a reasonable viewer, move on, and brace for whatever reviewer pet peeve you happen to encounter.\nSummary   Use the dichromat() function in the dichromat R package to convert a color palette to approximations of how it would be seen by three types of color blindness.\n  Try the resulting colorblind approximation color palettes out in the graphs you intend to make.\n  If you discover that your chosen colors don\u0026rsquo;t work well for your intended purpose, try increasing the contrast in brightness between the colors by making one much darker or lighter.\n  When in doubt, just use the viridis palette!\n  check_cb(viridisLite::viridis(10), cex_text = 0)  ","date":1684195200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1684271060,"objectID":"e3a457fa78a0e0e6519c75b0cafa7de0","permalink":"/post/using-the-dichromat-package-to-check-if-your-plot-is-colorblind-friendly/","publishdate":"2023-05-16T00:00:00Z","relpermalink":"/post/using-the-dichromat-package-to-check-if-your-plot-is-colorblind-friendly/","section":"post","summary":"How to use the dichromat package to quickly simulate how a color palette would look to colorblind viewers.","tags":["color","R","plotting"],"title":"Using the dichromat package to check if your plot is colorblind-friendly","type":"post"},{"authors":["Hannah I. Weller","Hern√°n L√≥pez-Fern√°ndez","Caleb D. McMahan","Elizabeth L. Brainerd"],"categories":null,"content":"","date":1654041600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1654041600,"objectID":"efefc8273fb0d8e466cff8f025bedba8","permalink":"/publication/weller-relaxed-2022/","publishdate":"2025-03-04T15:40:54.136101Z","relpermalink":"/publication/weller-relaxed-2022/","section":"publication","summary":"Multifunctionality is often framed as a core constraint of evolution, yet many evolutionary transitions involve traits taking on additional functions. Mouthbrooding, a form of parental care where offspring develop inside a parent‚Äôs mouth, increases multifunctionality by adding a major function (reproduction) to a structure already serving other vital functions (feeding and respiration). Despite increasing multifunctionality, mouthbrooding has evolved repeatedly from other forms of parental care in at least seven Ô¨Åsh families. We hypothesized that mouthbrooding is more likely to evolve in lineages with feeding adaptations that are already advantageous for mouthbrooding. We tested this hypothesis in Neotropical cichlids, where mouthbrooding has evolved four or Ô¨Åve times, largely within winnowing clades, providing several pairwise comparisons between substrate-brooding and mouthbrooding sister taxa. We found that the mouthbrooding transition rate was 15 times higher in winnowing than in nonwinnowing clades and that mouthbrooders and winnowers overlapped substantially in their buccal cavity morphologies, which is where offspring are incubated. Species that exhibit one or both of these behaviors had larger, more curved buccal cavities, while species that exhibit neither behavior had narrow, cylindrical buccal cavities. Given the results we present here, we propose a new conceptual model for the evolution of mouthbrooding, integrating the roles of multifunctional morphology and the environment. We suggest that functional transitions like mouthbrooding offer a different perspective on multifunctionality: increasing constraints in one trait may release them for another, generating new evolutionary opportunities.","tags":null,"title":"Relaxed Feeding Constraints Facilitate the Evolution of Mouthbrooding in Neotropical Cichlids","type":"publication"},{"authors":[],"categories":["recolorize"],"content":"Kei-Lin Ooi at the University of Melbourne has a very good question about batch resizing (excerpted from an email):\n I am using images from online databases and thus the images are not standardized. I am using the anisotropic blur to smooth out the image and eliminate image \u0026ldquo;noise\u0026rdquo;, but the anisotropic blur function is dependent on the image resolution. I was wondering if there was a way of batch processing images to scale and resize them to be, for example, 10 pixels / millimeter, so that the blur function smooths each image proportionally?\n This is an important step of image processing, so I wanted to use Kei-Lin\u0026rsquo;s email as motivation for writing out slightly more general instructions for this problem.\nIf you\u0026rsquo;re reading this post, you\u0026rsquo;re most likely already familiar with image resolution, usually defined as pixels per some real-world unit, like millimeters or inches (not the number of pixels in the image). When we\u0026rsquo;re taking photographs for analysis, we often try to take the highest resolution pictures we can of each subject, which might mean zooming in on smaller specimens and zooming out for large ones. The result is that we\u0026rsquo;re sampling those smaller specimens at a much higher resolution than the larger ones, which if nothing else introduces a confounding variable to our measurements.\nLuckily, this is pretty easy to fix a long as you know the resolution of each of your images: you just have to calculate the rescaling factor for the image from its current resolution and its target resolution, and provide that factor as an argument to an image resizing tool. We\u0026rsquo;ll do it in R (for pretty obvious inertia reasons), but I\u0026rsquo;ll list some alternatives at the end of the post.\nGenerating example images First, I\u0026rsquo;ll generate some simple exmple images by resizing an original image. We\u0026rsquo;ll do this with one of the images that comes with recolorize, reading it in at 1x, 2x, and 3x the original size:\n# get image path image_path \u0026lt;- system.file(\u0026quot;extdata/chongi.png\u0026quot;, package = \u0026quot;recolorize\u0026quot;) # load recolorize library(recolorize) # read in image at 1x, 2x, and 3x sample_images \u0026lt;- lapply(1:3, function(i) readImage(image_path, resize = i))  We can verify that the dimensions of the 2x and 3x images are indeed 2x and 3x the dimensions of the original:\nlapply(sample_images, dim)  ## [[1]] ## [1] 226 90 4 ## ## [[2]] ## [1] 452 180 4 ## ## [[3]] ## [1] 678 270 4  I\u0026rsquo;m using this as sample data because the final solution is obvious: we want to downsample the 2x and 3x images to have the same dimensions as the 1x image again. For now, let\u0026rsquo;s save them as their own set of images:\n# save as images for (i in 1:3) { png::writePNG(sample_images[[i]], paste0(\u0026quot;beetle_\u0026quot;, i, \u0026quot;.png\u0026quot;)) } # and make a list of image paths image_paths \u0026lt;- dir(\u0026quot;.\u0026quot;, \u0026quot;beetle_\u0026quot;)  Skip this step if you have your own images that you want to resize!\nCalculating current resolution and rescaling factors Let\u0026rsquo;s say the beetle in this image is 35mm long, and I measure it in ImageJ on each image to find that the beetle is 200px long in image 1, 400px long in image 2, and 600px long in image 3. That means my image resolutions are 200/35 = 5.71 px/mm for image 1, 400/35 = 11.43 px/mm for image 2, and 600/35 = 17.14 px/mm in image 3. For a real dataset, I would probably make a metadata spreadsheet of image name in one column and resolution in another so that you can keep track of everything.\nIn our example, we\u0026rsquo;ll try to resize everything so that it\u0026rsquo;s the same as the original resolution of 5.71 px/mm. For an actual dataset, you could pick any target resolution so long as it is equal to or lower than the lowest resolution of any image in your dataset.\nIn our case, we know our rescaling factors because we picked them, but let\u0026rsquo;s pretend we need to find them. The simplest calculation is to just divide the target resolution by the current resolution:\ncurrent_resolutions \u0026lt;- c(200/35, 400/35, 600/35) target_resolution \u0026lt;- 200/35 rescaling_factors \u0026lt;- target_resolution / current_resolutions print(rescaling_factors)  ## [1] 1.0000000 0.5000000 0.3333333  As expected, we end up with rescaling factors of 1 for the original image (no resizing), 0.5 for the 2x image (1/2), and 0.3333 for the 3x image (1/3).\nRescaling images The easiest thing to do is to rescale the images using the resize argument in recolorize, since you have the option any time you read in an image for the package:\n# make an empty list for images image_list \u0026lt;- vector(\u0026quot;list\u0026quot;, length = length(image_paths)) image_list \u0026lt;- setNames(image_list, basename(image_paths)) # read in images and rescale appropriately for (i in 1:length(image_paths)) { image_list[[i]] \u0026lt;- recolorize::readImage(image_paths[i], resize = rescaling_factors[i]) }  However, you\u0026rsquo;ll notice something if you look at the actual image dimensions:\nlapply(image_list, dim)  ## $beetle_1.png ## [1] 226 90 4 ## ## $beetle_2.png ## [1] 226 90 4 ## ## $beetle_3.png ## [1] 223 89 4  The first two images now have identical dimensions, but the last one‚Äîwhich we scaled up 3x‚Äîisn\u0026rsquo;t quite right: it\u0026rsquo;s 223x89 pixels, instead of 226x90 pixels, so we have a resolution of about 5.63 px/mm instead of 5.71 px/mm. As far as I can tell, this is an artifact of our particular rescaling factor (1/3) being a repeating decimal (0.333\u0026hellip;) that gets rounded by the computer. One way around that would be to explicitly calculate the image dimensions you want, and use the imager library directly to specify those dimensions:\nlibrary(imager) # load the image as a cimg object img \u0026lt;- load.image(image_paths[3]) # this doesn't work (this is the function recolorize calls): img_imresize \u0026lt;- imresize(img, scale = 1/3) dim(img_imresize)  ## [1] 89 223 1 4  # cimg objects have x and y flipped, but this is still 223 pixels long x 89 # pixels wide # we can explicitly calculate image dimensions: new_dimensions \u0026lt;- dim(img)[1:2] * 1/3 # and specify those dimensions directly: img_resize \u0026lt;- resize(img, size_x = new_dimensions[1], size_y = new_dimensions[2]) dim(img_resize)  ## [1] 90 226 1 4  In practice, you\u0026rsquo;ll most likely encounter these fractional rescaling artifacts a lot, and even doing it the more precise way in imager doesn\u0026rsquo;t totally eliminate it, since images have to be represented as an integer number of pixels. In this case, our artifact translated to a 0.08 px/mm discrepancy, partly because the images were so small to begin with.\nSaving images Once you rescale an image, you\u0026rsquo;ll want to save it. I would use writePNG() if your images are arrays, but the other option is imager\u0026rsquo;s save.image() function:\n# we can save it with save.image: save.image(img_resize, file = \u0026quot;beetle_3_resized.png\u0026quot;, quality = 1) # or we can use a non-exported recolorize function to turn it back into an array: img_array \u0026lt;- recolorize:::cimg_to_array(img_resize) # and then export it: png::writePNG(img_array, \u0026quot;beetle_3_resized.png\u0026quot;)  That\u0026rsquo;s basically it! Calculate the rescaling factor for each image, apply it to that image, and save the downsampled result.\nA slightly more realistic example Here\u0026rsquo;s some code for running through a folder of images (the beetles that come with the package). The easier way:\n# get vector of images: images \u0026lt;- dir(system.file(\u0026quot;extdata\u0026quot;, package = \u0026quot;recolorize\u0026quot;), \u0026quot;png\u0026quot;, full.names = TRUE) # best practice would be to have these stored in a CSV in one column with image # name in the other column, but for this example we'll just define image # resolution as a vector: image_resolutions \u0026lt;- c(5.71, 6.63, 5.32, 5.91, 6.63) # px/mm # target resolution is 5 px/mm, since our lowest resolution is 5.3 px/mm target_resolution \u0026lt;- 5 # for every image... for (i in 1:length(images)) { # read in new image, rescaling it appropriately new_image \u0026lt;- readImage(images[i], resize = target_resolution / image_resolutions[i]) # get filename for renaming filename \u0026lt;- basename(tools::file_path_sans_ext(images[i])) # and store output in current working directory png::writePNG(new_image, target = paste0(filename, \u0026quot;_resized.png\u0026quot;)) }  And the slightly more complicated way:\n# or, you could use the more precise option: for (i in 1:length(images)) { # load image img \u0026lt;- load.image(images[i]) # define new dimensions new_dimensions \u0026lt;- dim(new_image)[1:2] * (target_resolution / image_resolutions[i]) # resize new_image \u0026lt;- resize(img, size_x = new_dimensions[1], size_y = new_dimensions[2]) # and save filename \u0026lt;- basename(tools::file_path_sans_ext(images[i])) save.image(new_image, file = paste0(filename, \u0026quot;_resized.png\u0026quot;)) }  Alternatives to rescaling in R Almost any image processing tool will let you resize/rescale an image. The most tedious (and familiar) way is probably to use Photoshop or Gimp, manually opening each image and resizing it to desired dimensions.\nPersonally, I\u0026rsquo;m a big fan of ffmpeg, a popular command line tool for video, audio, and image processing. For example, to resize the 3x beetle, we would just call the following line in the terminal:\n\u0026gt; ffmpeg -i beetle_3.png -vf \u0026quot;scale=iw/3:ih/3\u0026quot; beetle_3_resized.png  It\u0026rsquo;s pretty easy to automate the renaming and rescaling. The only thing that would be difficult here is figuring out how to supply a different rescaling argument for each image (notice here I just divided the image width and height by 3). I\u0026rsquo;m sure you could do this by writing a short bash script, but I didn\u0026rsquo;t bother to do that for this post.\nHopefully this is helpful‚ÄîI guess the takeaway is that you have to calculate a simple ratio and then use any of half a dozen tools to resize your image. Pretty straightforward!\n","date":1647216000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1647282483,"objectID":"8c3117337a611ddb712a7ef8b9009656","permalink":"/post/scaling-images-to-a-uniform-resolution/","publishdate":"2022-03-14T00:00:00Z","relpermalink":"/post/scaling-images-to-a-uniform-resolution/","section":"post","summary":"Kei-Lin Ooi at the University of Melbourne has a very good question about batch resizing (excerpted from an email):\n I am using images from online databases and thus the images are not standardized.","tags":["tutorial","images","image processing","resolution","resizing"],"title":"Scaling images to a uniform resolution","type":"post"},{"authors":["John G. Capano","Scott M. Boback","Hannah I. Weller","Robert L. Cieri","Charles F. Zwemer","Elizabeth L. Brainerd"],"categories":null,"content":"","date":1646092800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1646092800,"objectID":"0a7a12489e45bda70fcbec2680ceacd1","permalink":"/publication/capano-modular-2022/","publishdate":"2025-03-04T15:40:54.110129Z","relpermalink":"/publication/capano-modular-2022/","section":"publication","summary":"The evolution of constriction and of large prey ingestion within snakes are key innovations that may explain the remarkable diversity, distribution and ecological scope of this clade, relative to other elongate vertebrates. However, these behaviors may have simultaneously hindered lung ventilation such that early snakes may have had to circumvent these mechanical constraints before those behaviors could evolve. Here, we demonstrate that Boa constrictor can modulate which specific segments of ribs are used to ventilate the lung in response to physically hindered body wall motions. We show that the modular actuation of specific segments of ribs likely results from active recruitment or quiescence of derived accessory musculature. We hypothesize that constriction and large prey ingestion were unlikely to have evolved without modular lung ventilation because of their interference with lung ventilation, high metabolic demands and reliance on sustained lung convection. This study provides a new perspective on snake evolution and suggests that modular lung ventilation evolved during or prior to constriction and large prey ingestion, facilitating snakes‚Äô remarkable radiation relative to other elongate vertebrates.","tags":null,"title":"Modular lung ventilation in *Boa constrictor*","type":"publication"},{"authors":["Hannah Weller","Steven Van Belleghem"],"categories":["recolorize"],"content":"This tutorial will go through the process of combining patternize and recolorize tools to produce a PCA quantifying color pattern variation in wasp faces (Polistes fuscatus). This subset of 20 images (and workflow) is excerpted from \u0026lsquo;Evidence for a selective link between cooperation and individual recognition\u0026rsquo; (Tumulty et al. 2023), with permission from the lead author. (Thanks James!)\nThe patternize package is an excellent tool for quantifying variation in color patterns. I use it frequently, because it\u0026rsquo;s a consistent, scaleable method for quantification that works for almost any organism, and it allows you to control for variation in shape, orientation, and size by first aligning all of your color patterns to a RasterStack‚Äîanalogous to the Procrustes fit step of geometric morphometrics. It also helps that the package author, Steven Van Belleghem, is extremely friendly!\nIn practice, the most difficult step of using patternize is often the color segmentation: like several other color analysis packages and tools, patternize frequently relies on k-means clustering to extract color patches from images (especially for batch processing). That\u0026rsquo;s where recolorize comes in. In order to use recolorize to do the segmentation step for patternize, we have to follow three steps:\n Align original images as RasterStacks using alignLan() in patternize Segment the list of aligned images using recolorize Convert list of segmented images back into the patternize format  Let\u0026rsquo;s do it!\nExample files All the data and code used in this tutorial can be found here in the \u0026lsquo;wasps\u0026rsquo; subfolder: https://github.com/hiweller/recolorize_examples\nStep 1: Image alignment in patternize Note: Make sure you have installed the development version of patternize (e.g. by running devtools::install_github(\u0026quot;StevenVB12/patternize\u0026quot;)).\nWe\u0026rsquo;re starting with a folder of unaltered images of wasp faces. These images have been color-corrected and cropped to the wasp\u0026rsquo;s face, but the background hasn\u0026rsquo;t been masked out, since patternize will do that when we do the alignment.\nNote that even on this dataset‚Äîwhich is highly standardized‚Äîthe images have slight variations in the size, shape, and angle of the head, making it difficult to differentiate variation due to color pattern differences from that due to other factors.\nTo use the alignLan() function, we need to provide XY coordinates of landmarks (one set per image). I did these in ImageJ using the multi-point tool, but really you just need a two-column, tab-delimited text file with X coordinates on the left and Y coordinates on the right, and no header. Our landmarking scheme for the wasp faces only had 8 points: So, I opened up each image in ImageJ, selected those landmarks using the multi-point tool, then saved those pixel coordinates as a plain text file with the same suffix. For example, the pixel coordinates for polistes_01.jpg is called polites_01_landmarks.txt (you can see it here).\nI also made a mask for the images using the polygon selection tool in ImageJ, which will allow us to do the batch background masking. In this case, we only want to retain the frons and clypeus of the head (masking out the eyes and antennae openings), so I outlined them in a representative image (polistes_05) and saved those as XY coordinates as well (here). Because we\u0026rsquo;re aligning using landmarks, we only need to make one outline which will be applied to all images in the dataset‚Äîmuch faster than masking each image manually!\nOnce you have all those files (original images, XY landmark coordinates, and masking outline coordinates), we can combine them using patternize. I organized my files into separate folders (images in the original_images/ folder, landmark text files in the landmarks folder, etc), but you don\u0026rsquo;t have to do that so long as your files are organized and named in a way that works with the makeList() function.\n# load library library(patternize) ### Align set of 20 images ### # set of specimen IDs IDlist \u0026lt;- tools::file_path_sans_ext(dir(\u0026quot;original_images/\u0026quot;, \u0026quot;.jpg\u0026quot;)) # make list with images imageList \u0026lt;- makeList(IDlist, type = \u0026quot;image\u0026quot;, prepath = \u0026quot;original_images/\u0026quot;, extension = \u0026quot;.jpg\u0026quot;) # make list with landmarks landmarkList \u0026lt;- makeList(IDlist, type = \u0026quot;landmark\u0026quot;, prepath = \u0026quot;landmarks/\u0026quot;, extension = \u0026quot;_landmarks.txt\u0026quot;) # Set target as polistes 05 target \u0026lt;- landmarkList[['polistes_05']] # Set up mask, which excludes eyes/mandibles and antenna holes mask1 \u0026lt;- read.table(\u0026quot;masks/polistes_05_mask.txt\u0026quot;, header = FALSE) mask2 \u0026lt;- read.table(\u0026quot;masks/polistes_05_Lantenna.txt\u0026quot;, header = FALSE) mask3 \u0026lt;- read.table(\u0026quot;masks/polistes_05_Rantenna.txt\u0026quot;, header = FALSE) ### Alignment ### # this takes ~1 minute on a 16Gb RAM laptop running Ubuntu imageList_aligned \u0026lt;- alignLan(imageList, landmarkList, transformRef = target, adjustCoords = TRUE, plotTransformed = T, resampleFactor = 5, cartoonID = 'polistes_05', maskOutline = list(mask1, mask2, mask3), inverse = list(FALSE, TRUE, TRUE))  The resulting imageList_aligned object is a list of aligned RasterBrick objects, one per image, with everything but the region of interest (frons and clypeus) removed.\nTip: I usually save this list as an .RDS file so I can load it back in at my convenience without having to do the alignment step again:\n# save RDS file saveRDS(imageList_aligned, \u0026quot;rds_files/imageList_aligned.rds\u0026quot;) # read it in: imageList_aligned \u0026lt;- readRDS(\u0026quot;rds_files/imageList_aligned.rds\u0026quot;)  Step 2: Segment images in recolorize The wonky step in this workflow is that patternize works with raster images, and recolorize works with arrays, so we have to convert from raster objects to arrays before using recolorize. The brick_to_array() function makes that fairly straightforward:\n# load library library(recolorize) # convert from RasterBricks to image arrays using the brick_to_array function: imgs \u0026lt;- lapply(imageList_aligned, brick_to_array) names(imgs) \u0026lt;- names(imageList_aligned) # save raster extents for later conversion: extent_list \u0026lt;- lapply(imageList_aligned, raster::extent)  Now we have a list of image arrays. Plotting them using plotImageArray() helps us to see what the alignLan() step did for our original images:\n(Note that it also flipped the images upside because the y coordinate systems are reversed‚Äìthis is annoying, but doesn\u0026rsquo;t affect our results, so we\u0026rsquo;ll leave it for now.)\nWe want to map each of these wasp faces to the same set of three colors: dark brown, reddish brown, and yellow. K-means clustering could work in theory for this problem (we would fit n = 3 colors for each image), but in practice, we get the colors back in a random order‚Äîand they\u0026rsquo;re not actually the same color. Here\u0026rsquo;s what it looks like if we try to do that:\nfor (i in 1:length(imgs)) { rc \u0026lt;- recolorize(imgs[i], method = \u0026quot;k\u0026quot;, n = 3, plotting = FALSE) plotColorPalette(rc$centers) }  These colors are definitely similar across images, but they\u0026rsquo;re not consistent, especially since not all wasps have all three colors present. Maybe most intractably, they\u0026rsquo;re not in the same order: yellow is color 1, 2, or 3 depending on the image, and sometimes it\u0026rsquo;s not there at all.\nInstead, we\u0026rsquo;ll come up with our list of 3 colors by combining color palettes across images, and then use the imposeColors() function in recolorize to map each of our images to the same color palette. First, generate a color palette for each image:\n# make an empty list for storing the recolorize objects rc_list \u0026lt;- vector(\u0026quot;list\u0026quot;, length(imgs)) names(rc_list) \u0026lt;- names(imgs) # for every image, run the same recolorize2 function to fit a recolorize object: for (i in 1:length(imgs)) { rc_list[[i]] \u0026lt;- recolorize2(imgs[[i]], bins = 3, cutoff = 35, plotting = FALSE) }  I kept it pretty simple for this example (we\u0026rsquo;re just calling recolorize2() with the same parameters for each image), but you could get more complicated with what you put in the for loop. (You could even choose to use k-means clustering as the method here by setting method = \u0026quot;k\u0026quot; and specifying the number of colors, since we\u0026rsquo;re just using it as a starting point, but since k-means is not deterministic that poses problems for repeatability.)\nNext you can combine the color palettes from all of the recolorize objects in rc_list and use hclust_color to plot them and return a list of which colors to group together:\n# get a dataframe of all colors: all_palettes \u0026lt;- do.call(rbind, lapply(rc_list, function(i) i$centers)) # and for cluster sizes (as a proportion of their original image): all_sizes \u0026lt;- do.call(c, lapply(rc_list, function(i) i$sizes)) # plot colors using hclust and return grouping list: par(mar = rep(2, 4)) cluster_list \u0026lt;- hclust_color(all_palettes, n_final = 3)  The cluster_list object is a list, each element of which is a vector of which of the original colors should be clustered together. See the rest of the hclust_color() options to various ways to combine colors by similarity‚Äîby default, it calculates the Euclidean distance matrix between all provided color centers in CIE Lab color space. We can use that list to combine all the colors and come up with our universal palette:\n# make an empty matrix for storing the new palette wasp_palette \u0026lt;- matrix(NA, ncol = 3, nrow = length(cluster_list)) # for every color in cluster_list... for (i in 1:length(cluster_list)) { # get the center indices idx \u0026lt;- cluster_list[[i]] # get the average value for each channel, using cluster size to get a weighted average ctr \u0026lt;- apply(all_palettes, 2, function(j) weighted.mean(j[idx], w = all_sizes[idx])) # store in the palette matrix wasp_palette[i, ] \u0026lt;- ctr } # check that our colors seem reasonable par(mar = rep(0, 4)) plotColorPalette(wasp_palette)  And now, we can use imposeColors() to map every image to the same set of three colors:\nimpose_list \u0026lt;- lapply(imgs, function(i) imposeColors(i, wasp_palette, adjust_centers = FALSE, plotting = FALSE)) # let's look at our palettes! layout(matrix(1:20, nrow = 4)) par(mar = rep(0, 4)) for (i in impose_list) { plotColorPalette(i$centers, i$sizes) }  Although the proportions of each color vary by image, the order/value of the colors does not (unlike with k-means). This is the key step. As long as you can provide a color palette to which all of your images should be mapped, you can use imposeColors() to map every image to those colors. The earlier portion where we did an initial fit and used hclust_color is a good option when you want to come up with a color palette intrinsic to your original images, but it may still take some toying around before you find a palette that works.\nThe last step is to convert each recolorize fit in back to a patternize format, which we can do with the recolorize_to_patternize() function:\n# convert to patternize: patternize_list \u0026lt;- lapply(impose_list, recolorize_to_patternize) # and set extents again: for (i in 1:length(patternize_list)) { for (j in 1:length(patternize_list[[1]])) { raster::extent(patternize_list[[i]][[j]]) \u0026lt;- extent_list[[i]] } }  This is a list of lists: there is one element per sample ID in patternize_list (20 total), and each of those elements is a list of RasterLayer objects, one per color class (3 per sample ID). You may need to reshuffle these depending on what you want to do.\nNow, back to patternize!\nStep 3: Color pattern analyses in patternize Since we now have the images segmented in the way that patternize needs, we can run any of the regular patternize functions on it (see the methods paper and examples repository). Here, we\u0026rsquo;ll use a custom function based on code that Steven sent me for running a PCA on the entire color pattern (all three colors simultaneously, rather than one color class at a time). You can see the full function here. If you\u0026rsquo;ve downloaded the wasp example dataset, the easiest thing to do is to just source it and run the function on the list we made earlier:\nsource(\u0026quot;patPCA_total.R\u0026quot;) wasp_pca \u0026lt;- patPCA_total(patternize_list, quietly = FALSE)  ## Summing raster lists...  ## Making dataframe from rasters...  ## Running PCA on 3 colors and 20 images...  ## done  That\u0026rsquo;s it! The wasp_pca object is a prcomp object (the standard class for principal components analysis in R).\nBonus: visualization It can be hard to tell whether the PCA is capturing relevant axes of color pattern variation from a scatterplot; I find it more intuitive to plot some version the actual images. The add_image() function is an easy way to do that:\n# first, make a blank plot PCx \u0026lt;- 1; PCy \u0026lt;- 2 pca_summary \u0026lt;- summary(wasp_pca) limits \u0026lt;- apply(wasp_pca$x[ , c(PCx, PCy)], 2, range) par(mar = c(4, 4, 2, 1)) plot(wasp_pca$x[ , c(PCx, PCy)], type = \u0026quot;n\u0026quot;, asp = 1, xlim = limits[ , 1] + c(-5, 5), ylim = limits[ , 2] + c(-10, 10), xlab=paste0('PC1 (', round(pca_summary$importance[2, PCx]*100, 1), ' %)'), ylab=paste0('PC2 (', round(pca_summary$importance[2, PCy]*100, 1), ' %)')) # then add images: for (i in 1:length(impose_list)) { add_image(impose_list[[i]]$original_img, x = wasp_pca$x[i, PCx], y = wasp_pca$x[i, PCy], width = 20) }  You could also plot images from a folder on your computer:\nplot(wasp_pca$x[ , c(PCx, PCy)], type = \u0026quot;n\u0026quot;, asp = 1, xlim = limits[ , 1] + c(-5, 5), ylim = limits[ , 2] + c(-10, 10), xlab=paste0('PC1 (', round(pca_summary$importance[2, PCx]*100, 1), ' %)'), ylab=paste0('PC2 (', round(pca_summary$importance[2, PCy]*100, 1), ' %)')) # read in images from the original folder: images \u0026lt;- lapply(dir(\u0026quot;original_images/\u0026quot;, full.names = TRUE), readImage) # and plot: for (i in 1:length(images)) { add_image(images[[i]], x = wasp_pca$x[i, PCx], y = wasp_pca$x[i, PCy], width = 20) }  (If I were to use these images for a paper figure, though, I would go through and mask out the background using transparencies‚Äîthese are a little hard to see on a white background.)\nThat\u0026rsquo;s it for the tutorial. I would recommend downloading the example code and files from the linked GitHub repository if you want to try it out: there are a few steps involved, but ultimately it\u0026rsquo;s a reasonably simple procedure. I\u0026rsquo;d love to be able to write a one-and-done version of this process, but if you\u0026rsquo;ve been reading the other recolorize documentation, you\u0026rsquo;ll be familiar with my perspective on this. Basically, if I try to impose a general structure for how to do this every time, that\u0026rsquo;s not going to be flexible enough to encompass many use cases, and I prefer to keep things modular. Still, if you have any ideas for how to make this a more friendly process, I\u0026rsquo;m all ears!\n","date":1643241600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1643310262,"objectID":"dd275337967fce679313bfd67eec6b63","permalink":"/post/recolorize-patternize-workflow/","publishdate":"2022-01-27T00:00:00Z","relpermalink":"/post/recolorize-patternize-workflow/","section":"post","summary":"How to combine the recolorize and patternize packages to perform quantitative color pattern analyses.","tags":["color","r","recolorize","r packages"],"title":"Recolorize \u0026 patternize workflow","type":"post"},{"authors":["Hannah Weller"],"categories":[],"content":"One of the most direct ways to tell whether or not your image analysis is working is to plot your images themselves as points in your result plots, which is usually easier said than done. Lots of packages in R will allow you to do some form of this, but I usually run into two problems: 1) I have to download a (sometimes pretty hefty) package for a single function, and 2) that function often only works in a specific context, which means it\u0026rsquo;s not very flexible. In practice, I usually end up defining functions for this as-needed, in a sort of ad-hoc dirtbag fashion. This post will outline the basics of doing just that.\nFirst, let\u0026rsquo;s take a look at our images:\nThis is a lovely set of 40 images of jewel beetles, taken by my collaborator Nathan P. Lord. Not only are they all nicely uniform and centered, but the backgrounds are transparent \u0026ndash; this almost always makes for nicer plotting, because you don\u0026rsquo;t get big white corners when the images overlap. Although if you\u0026rsquo;re just using these plots as diagnostics instead of figures, it doesn\u0026rsquo;t really matter as long as it helps you understand your data better.\nAs an example of the kind of thing we might want to plot, we\u0026rsquo;ll use colordistance to generate a distance matrix of color similarity for the forty images above:\nlibrary(colordistance) # in my case, I have a folder called 'images' which contains the 40 images, # so 'images' is a vector of 40 paths images \u0026lt;- dir(\u0026quot;images/\u0026quot;, full.names = TRUE) # generate a distance matrix using all the package defaults cdm \u0026lt;- imageClusterPipeline(images, sample.size = FALSE)  The idea of this analysis is to cluster the images with the most similar color palettes together. So, we want to know if the clusters produced by this distance matrix represent images that actually are the most similar-looking.\nBy default, colordistance generates a heatmap representing the pairwise color distances between each image, where darker blue indicates that two images are more similar, and brighter pink indicates that they are less similar. You might notice that this graphic is kind of useless for diagnostics. The image names are just printed as labels, and their names are not indicative of their contents\u0026ndash;so I have no idea if this analysis has lumped together the green-and-shiny beetles separate from the black-and-yellow beetles, or if I need to try different settings, and sitting here looking up image names is not a quick way to check.\nA better solution is to plot the images at the tips of the hierarchical clustering tree shown on the top and left of the heatmap:\nTo make this plot, I used the ape package to plot a neighbor-joining tree of the distance matrix, then defined a function for plotting images at the tips. First, let\u0026rsquo;s make the neighbor-joining tree:\nlibrary(ape) tree \u0026lt;- nj(as.dist(cdm)) plot(tree, direction = \u0026quot;upwards\u0026quot;, cex = 0.5)  Next, we can define a function, add_image, which adds an image to a plot at a given set of XY coordinates. It\u0026rsquo;s sort of analogous to the points function, which lets you add points to an existing base R plot.\nThis is more complicated than just plotting an image as an image, because we have to play nice with existing plotting parameters (aspect ratio, range of the X- and Y-axes, etc).\n# define add_image function: add_image \u0026lt;- function(obj, # an object interpretable by rasterImage x = NULL, # x \u0026amp; y coordinates for the center of the image y = NULL, width = NULL, # width of the image interpolate = TRUE, # method for resizing angle = 0) { # get current plotting window parameters: usr \u0026lt;- graphics::par()$usr # extremes of user coordinates in the plotting region pin \u0026lt;- graphics::par()$pin # plot dimensions (in inches) # image dimensions and scaling factor: imdim \u0026lt;- dim(obj) sf \u0026lt;- imdim[1] / imdim[2] # set the width of the image (relative to x-axis) w \u0026lt;- width / (usr[2] - usr[1]) * pin[1] h \u0026lt;- w * sf # height is proportional to width hu \u0026lt;- h / pin[2] * (usr[4] - usr[3]) # scale height to y-axis range # plot the image graphics::rasterImage(image = obj, xleft = x - (width / 2), xright = x + (width / 2), ybottom = y - (hu / 2), ytop = y + (hu/2), interpolate = interpolate, angle = angle) }  Note that if you just want to plot an image as an image, you can use rasterImage from the graphics package, and almost any other image analysis package will come with a plotting method (for example, in recolorize you can use plotImageArray).\nWe can use this function to plot the images on a regular XY plot:\nX \u0026lt;- runif(40) Y \u0026lt;- runif(40) plot(X, Y) for (i in 1:length(images)) { # read the image into R: img \u0026lt;- png::readPNG(images[i]) # add the image: add_image(img, x = X[i], y = Y[i], width = 0.05) }  If we wanted to actually plot the distance matrix as a bivariate plot, we could use non-metric multidimensional scaling (NMDS), as described in this post, to represent the distance matrix with a set of 2D coordinates:\n# explaining NMDS is beyond the scope of this post (and is probably best left to the ecologists) # see this link for more: # https://cougrstats.wordpress.com/2019/12/11/non-metric-multidimensional-scaling-nmds-in-r/ # for now, we'll just do it in two lines! library(vegan) nmds_scores \u0026lt;- scores(metaMDS(comm = as.dist(cdm)))  ## Run 0 stress 0.1158626 ## Run 1 stress 0.1139447 ## ... New best solution ## ... Procrustes: rmse 0.01705616 max resid 0.09840254 ## Run 2 stress 0.1140722 ## ... Procrustes: rmse 0.007575743 max resid 0.03447679 ## Run 3 stress 0.1159785 ## Run 4 stress 0.1158626 ## Run 5 stress 0.1139448 ## ... Procrustes: rmse 0.0001779728 max resid 0.0006533518 ## ... Similar to previous best ## Run 6 stress 0.1158627 ## Run 7 stress 0.115814 ## Run 8 stress 0.1158626 ## Run 9 stress 0.1139447 ## ... New best solution ## ... Procrustes: rmse 6.912601e-05 max resid 0.0002514859 ## ... Similar to previous best ## Run 10 stress 0.1139447 ## ... Procrustes: rmse 5.642976e-05 max resid 0.0001733153 ## ... Similar to previous best ## Run 11 stress 0.1140719 ## ... Procrustes: rmse 0.007496018 max resid 0.03406593 ## Run 12 stress 0.1139448 ## ... Procrustes: rmse 9.99557e-05 max resid 0.000494275 ## ... Similar to previous best ## Run 13 stress 0.1139447 ## ... Procrustes: rmse 9.345179e-05 max resid 0.0004509194 ## ... Similar to previous best ## Run 14 stress 0.1139447 ## ... Procrustes: rmse 6.492366e-05 max resid 0.0003038437 ## ... Similar to previous best ## Run 15 stress 0.1140722 ## ... Procrustes: rmse 0.007604775 max resid 0.03440036 ## Run 16 stress 0.1158141 ## Run 17 stress 0.1158141 ## Run 18 stress 0.1158142 ## Run 19 stress 0.1139447 ## ... Procrustes: rmse 5.046786e-05 max resid 0.0002452231 ## ... Similar to previous best ## Run 20 stress 0.1158141 ## *** Solution reached  plot(nmds_scores) for (i in 1:length(images)) { # read the image into R: img \u0026lt;- png::readPNG(images[i]) # add the image: add_image(img, x = nmds_scores[i, 1], y = nmds_scores[i, 2], width = 0.05) }  If this were a presentation-quality figure, I would probably bump out the X- and Y-axis ranges a bit so none of the images got cut off, but this looks pretty good!\nHowever, I did promise plotting images at the tips of trees, and that turns out to be pretty easy once you\u0026rsquo;ve got a plotted tree and a set of tips. This mostly comes from this post on Liam Revell\u0026rsquo;s Phytools blog, which shows how to extract the XY coordinates of the tree tips from a plotted phylogeny; once we have those XY coordinates, we can plot images at those coordinates just as we would for a regular bivariate plot:\n# plot the tree plot(tree, show.tip.label = FALSE, direction = \u0026quot;upward\u0026quot;) # get the parameters from the plotting environment lastPP \u0026lt;- get(\u0026quot;last_plot.phylo\u0026quot;, envir = .PlotPhyloEnv) # get the xy coordinates of the tips ntip \u0026lt;- lastPP$Ntip # first n values are the tips, remaining values are the coordinates of the # nodes: xy \u0026lt;- data.frame(x = lastPP$xx[1:ntip], y = lastPP$yy[1:ntip]) # we can add points to the tree pretty easily using generic functions: points(xy[ , 1], xy[ , 2], col = viridisLite::viridis(40), pch = 19, cex = 2)  Putting it all together, all we have to do is plot the tree, then plot the images. One crucial thing here\u0026ndash;and familiar to anyone working with trees\u0026ndash;make sure your images are in the same order as your tips. This will save you a lot of head-scratching later.\n# get image names imnames \u0026lt;- tools::file_path_sans_ext(basename(images)) # get tip labels tipnames \u0026lt;- tree$tip.label # in my case, the tip labels are identical to the image names, so I can # use these to check that my images are in the right order: image_order \u0026lt;- match(tipnames, imnames) images \u0026lt;- images[image_order] # and plot! par(mar = rep(0, 4)) plot(tree, show.tip.label = FALSE, direction = \u0026quot;upward\u0026quot;) # get the parameters from the plotting environment lastPP \u0026lt;- get(\u0026quot;last_plot.phylo\u0026quot;, envir = .PlotPhyloEnv) # get the xy coordinates of the tips ntip \u0026lt;- lastPP$Ntip xy \u0026lt;- data.frame(x = lastPP$xx[1:ntip], y = lastPP$yy[1:ntip]) for (i in 1:length(images)) { add_image(png::readPNG(images[i]), x = xy[i, 1], y = xy[i, 2], width = 3) }  At this point, I usually write a wrapper function to do all of this for me, so I can plot an image tree from a tree and a list of image paths. I also add a bit of trickery to fix the x-axis scaling; phylogenies typically have weird axis scaling.\nimage_tree \u0026lt;- function(tree, # a phylo object image_paths, # a vector of image paths (in order) image_width = 0.1, # image width (as a proportion of the x-axis) tip_label = FALSE, # whether to draw the tip labels ...) { require(ape) # plot the tree plot.phylo(tree, show.tip.label = tip_label, ...) # this is the weird part: we get the phylo plot parameters from the active # graphics device lastPP \u0026lt;- get(\u0026quot;last_plot.phylo\u0026quot;, envir = .PlotPhyloEnv) # get the xy coordinates of the tips ntip \u0026lt;- lastPP$Ntip xy \u0026lt;- data.frame(x = lastPP$xx[1:ntip], y = lastPP$yy[1:ntip]) # scale image width according to plot width image_width \u0026lt;- diff(range(lastPP$x.lim)) * image_width # add the images using the add_image function for (i in 1:length(image_paths)) { img \u0026lt;- recolorize::readImage(image_paths[i]) add_image(img, xy[i, 1], xy[i, 2], width = image_width) } }  I\u0026rsquo;ll save that function and the add_image function in a script file (typically called image_tree.R or similar) and source that for the relevant project, so in practice my actual workflow looks like this:\nlibrary(ape) library(colordistance) # get images images \u0026lt;- dir(\u0026quot;images/\u0026quot;, full.names = TRUE) # get distance matrix cdm \u0026lt;- imageClusterPipeline(images, plot.heatmap = FALSE, sample.size = NULL) # make neighbor-joining tree tree \u0026lt;- nj(as.dist(cdm)) # plot image tree par(mar = rep(0, 4)) image_tree(tree, images, direction = \u0026quot;upward\u0026quot;, y.lim = c(0, 0.7))  So it\u0026rsquo;s actually pretty straightforward!\nOne question I would have after reading this post is: \u0026ldquo;Why don\u0026rsquo;t you just add this function to your R packages, instead of the crummy-looking default?\u0026rdquo; This is a good question, and there are two answers.\nFirst, I didn\u0026rsquo;t know how to do this when I first wrote the colordistance package, but I did know how to make heatmaps. I assumed that anyone using the package would take a quick look at the heatmap and then export the distance matrix for further analysis and plotting to emphasize whatever was most important about their results, and it didn\u0026rsquo;t occur to me that people are pretty likely to use the default visualization because they assume that\u0026rsquo;s what the package author intended. If I ever get the time and incentive to update the package, I hope to do so.\nSecond, I find myself redefining or tweaking this function so much for specific use cases (for instance, in this case we just use the readPNG function because all the images are PNGs, but you would need to change this if that\u0026rsquo;s not true of your images) that I didn\u0026rsquo;t see the point of including a static version in any one package. There\u0026rsquo;s so much variability in how plots are displayed and in how images are stored and displayed that a post explaining the details of the function was more helpful than creating a static version that would be out of date or too specific in scope. For instance, the image_tree function above doesn\u0026rsquo;t try to match your image list to your tree tips, because that would require assuming your images are named the same way as your tree tips, which probably won\u0026rsquo;t usually be the case.\nAnyways, there is probably a happier middle ground than what I\u0026rsquo;ve included here. Hopefully this post provides enough detail for other people to modify it for their needs!\n","date":1620345600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1620396986,"objectID":"a7ffb1135a8879d74b6cf032f2c23379","permalink":"/post/image-trees/","publishdate":"2021-05-07T00:00:00Z","relpermalink":"/post/image-trees/","section":"post","summary":"One of the most direct ways to tell whether or not your image analysis is working is to plot your images themselves as points in your result plots, which is usually easier said than done.","tags":["color","r"],"title":"Image trees","type":"post"},{"authors":["Hannah Weller"],"categories":[],"content":"A quick reference gallery for what the most broadly useful functions do.\nLoading and pre-processing images  readImage: Reads in a PNG or JPEG image, optionally resizing and/or rotating it.  img \u0026lt;- system.file(\u0026quot;extdata/corbetti.png\u0026quot;, package = \u0026quot;recolorize\u0026quot;) loaded_image \u0026lt;- readImage(img_path = img, resize = NULL, rotate = NULL)   blurImage: Applies one of several blurring filters from the imager package to a loaded image. Helpful for dealing with variation from textures (e.g. scales, reflections, hairs, etc).  blurred_image \u0026lt;- blurImage(loaded_image, blur_function = \u0026quot;medianblur\u0026quot;, n = 3, threshold = 5)  Initial segmentation  recolorize: The major function of the package. Segments colors using color binning (method = \u0026quot;hist\u0026quot;) or k-means clustering (method = \u0026quot;k\u0026quot;), in several color spaces.  rc_hist \u0026lt;- recolorize(img, method = \u0026quot;hist\u0026quot;, bins = 2, color_space = \u0026quot;sRGB\u0026quot;) #\u0026gt; #\u0026gt; Using 2^3 = 8 total bins  rc_k \u0026lt;- recolorize(img, method = \u0026quot;k\u0026quot;, n = 8, color_space = \u0026quot;sRGB\u0026quot;)   recolorize2: Runs recolorize and recluster (see next section) in sequence. I have found this to be an effective, fast combination for very many kinds of images, so if you\u0026rsquo;re going to pick one function to start with, pick this one!  rc \u0026lt;- recolorize2(img, cutoff = 45) #\u0026gt; #\u0026gt; Using 2^3 = 8 total bins   imposeColors: Imposes colors from one image onto another image (useful for batch processing).  colors \u0026lt;- c(\u0026quot;tomato\u0026quot;, \u0026quot;limegreen\u0026quot;, \u0026quot;dodgerblue\u0026quot;, \u0026quot;cornsilk\u0026quot;, \u0026quot;black\u0026quot;) colors \u0026lt;- t(col2rgb(colors)) / 255 imposed \u0026lt;- imposeColors(img, centers = colors)  Refining initial results  recluster: Combines existing clusters based on either a cutoff for color similarity or a target number of colors.  recluster_fit \u0026lt;- recluster(rc_hist, similarity_cutoff = 45)   thresholdRecolor: Drops the smallest clusters from a recolorize fit and refits the original image.  rc_thresh \u0026lt;- thresholdRecolor(rc_hist, pct = 0.01)   wernerColor: Remaps a recolorize object to the colors in Werner\u0026rsquo;s Nomenclature of Colors by Patrick Syme (1821), one of the first attempts at an objective color reference in western science, notably used by Charles Darwin. This one is mostly just for fun.  rc_werner \u0026lt;- wernerColor(recluster_fit)  Minor edits  absorbLayer: \u0026ldquo;Absorbs\u0026rdquo; all or part of a layer into the surrounding colors, optionally according to a size or location condition.  absorb_red \u0026lt;- absorbLayer(recluster_fit, layer_idx = 3, size_condition = function(s) s \u0026lt;= 100, highlight_color = \u0026quot;cyan\u0026quot;)   editLayer/editLayers: Applies one of several morphological operations from imager to a layer (or layers) of a recolorize object. This can be used to despeckle, fill in holes, or uniformly grow or shrink a color patch.  rc_edit \u0026lt;- editLayer(absorb_red, layer_idx = 3, operation = \u0026quot;fill\u0026quot;, px_size = 2)   mergeLayers: Merges specified layers together, with options for setting the new color.  merged_rc \u0026lt;- mergeLayers(rc_hist, merge_list = list(c(4, 7), c(3, 5), c(6, 8)))  Visualization  plotImageArray: Plots a 1D or 3D array as an RGB image.  layout(matrix(1:4, nrow = 1)) plotImageArray(loaded_image, main = \u0026quot;original\u0026quot;) plotImageArray(loaded_image[ , , 1], main = \u0026quot;red\u0026quot;) plotImageArray(loaded_image[ , , 2], main = \u0026quot;green\u0026quot;) plotImageArray(loaded_image[ , , 3], main = \u0026quot;blue\u0026quot;)   imDist | imHeatmap: Compares two versions of the same image by calculating the color distance between the colors of each pair of pixels (imDist), and gives you a few more options for plotting the results (imHeatmap).  layout(matrix(1:2, nrow = 1)) par(mar = rep(0, 4)) im_dist \u0026lt;- imDist(im1 = raster_to_array(recluster_fit$original_img), im2 = recoloredImage(recluster_fit), color_space = \u0026quot;Lab\u0026quot;) imHeatmap(im_dist, palette = viridisLite::viridis(100), legend = FALSE)   plotColorClusters: Plots color clusters in a 3D color space.  par(mar = rep(1, 4)) plotColorClusters(recluster_fit$centers, recluster_fit$sizes, color_space = \u0026quot;sRGB\u0026quot;, xlab = \u0026quot;red\u0026quot;, ylab = \u0026quot;green\u0026quot;, zlab = \u0026quot;blue\u0026quot;)   plotColorPalette: Alternatively, just plot as a color palette.  par(mar = rep(0, 4)) plotColorPalette(recluster_fit$centers, recluster_fit$sizes)  Exporting to other packages or files  splitByColor: Separates color clusters into individual layers (binary masks).  layout(matrix(1:6, nrow = 1)) plotImageArray(rc_edit$original_img) corbetti_layers \u0026lt;- splitByColor(rc_edit, plot_method = \u0026quot;over\u0026quot;)    classify_recolorize: Converts a recolorize object to a classify object in the pavo package for linking with spectral data.\n  recolorize_adjacency: Converts to a classify object using the above function, then runs the adjacency and boundary strength analysis function using values for human perceptual similarity.\n  recolorizeVector: Converts a bitmap (i.e. pixel) image to a vector image.\n  rc_vector \u0026lt;- recolorizeVector(recluster_fit, size_filter = 0.15, smoothness = 5)  ","date":1618272000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1618349229,"objectID":"6e384bde14a838a46df235e779e0354c","permalink":"/post/function-gallery-for-recolorize/","publishdate":"2021-04-13T00:00:00Z","relpermalink":"/post/function-gallery-for-recolorize/","section":"post","summary":"A copy of the recolorize function gallery vignette.","tags":["recolorize","color","r packages"],"title":"Function gallery for recolorize","type":"post"},{"authors":["Hannah Weller"],"categories":[],"content":"color-based image segmentation (for people with other things to do)  You can also tour the functions in the function gallery.\n The recolorize package is a toolbox for making color maps, essentially color-based image segmentation, using a combination of automatic, semi-automatic, and manual procedures. It has four major goals:\n  Provide a middle ground between automatic segmentation methods (which are hard to modify when they don\u0026rsquo;t work well) and manual methods (which can be slow and subjective).\n  Be deterministic whenever possible, so that you always get the same results from the same code.\n  Be modular and modifiable, so that you can tailor it for your purposes.\n  Play nice with other color analysis tools.\n  The color map above, for example, was generated using a single function which runs in a few seconds (and is deterministic):\nlibrary(recolorize) # get the path to the image (comes with the package, so we use system.file): img \u0026lt;- system.file(\u0026quot;extdata/corbetti.png\u0026quot;, package = \u0026quot;recolorize\u0026quot;) # fit a color map (only provided parameter is a color similarity cutoff) recolorize_obj \u0026lt;- recolorize2(img, cutoff = 45)  Notice what we didn‚Äôt have to input: we didn‚Äôt have to declare how many colors we expected (5), what we expect those colors to be (red, green, blue, black, and white), which pixels to include in each color patch, or where the boundaries of those patches are.\nThis introduction is intended to get you up and running with the recolorize package. Ideally, after reading it, you will have enough information to start to play around with the set of tools that it provides in a way that suits what you need it to do.\nI have tried not to assume too much about the reader\u0026rsquo;s background knowledge and needs, except that you are willing to use R and you have a color segmentation problem you have to solve before you can do something interesting with images. I primarily work with images of animals (beetles, fish, lizards, butterflies, snakes, birds, etc), and that will probably come through in the documentation. But it should work just as well for other kinds of images. Maybe better!\nI hope that this package will be helpful to you, and that if it is, you will share it with others who might find it helpful too. I had a lot of fun discussions with a lot of interesting people while I was making it, for which I\u0026rsquo;m very grateful.\nIf something is unclear or you find a bug, please get in touch or file an issue on the GitHub page. Suggestions for improvements are always welcome!\nQuick start  The bare minimum to start toying around with the package.\n The basic recolorize workflow is initial clustering step \\(\\rightarrow\\) refinement step \\(\\rightarrow\\) manual tweaks.\n  Images should first be color-corrected and have any background masked out, ideally with transparency, as in the image above, for example (Chrysochroa corbetti, taken by Nathan P. Lord, used with permission and egregiously downsampled to ~250x150 pixels by me).\n  In the initial clustering step, we bin all of the pixels into (in this case) 8 total clusters:\n  init_fit \u0026lt;- recolorize(img, method = \u0026quot;hist\u0026quot;, bins = 2, color_space = \u0026quot;sRGB\u0026quot;) #\u0026gt; #\u0026gt; Using 2^3 = 8 total bins  Followed by a refinement step where we combine clusters by their similarity:  refined_fit \u0026lt;- recluster(init_fit, similarity_cutoff = 45)  # pretty big improvement!  The recolorize2 function above calls these functions in sequence, since they tend to be pretty effective in combination.\nFinally, we can do manual refinements to clean up the different color layers, for example absorbing the red speckles into the surrounding color patches:  absorb_red \u0026lt;- absorbLayer(refined_fit, layer_idx = 3, size_condition = function(s) s \u0026lt;= 15, highlight_color = \u0026quot;cyan\u0026quot;)  Or performing simple morphological operations on individual layers:\nfinal_fit \u0026lt;- editLayer(absorb_red, 3, operation = \u0026quot;fill\u0026quot;, px_size = 4)  You can also batch process images using the same parameters, although recolorize functions only deal with one image at a time, so you will have to use a for loop or define a new function to call the appropriate functions in the right order:\n# get all 5 beetle images: images \u0026lt;- dir(system.file(\u0026quot;extdata\u0026quot;, package = \u0026quot;recolorize\u0026quot;), \u0026quot;png\u0026quot;, full.names = TRUE) # make an empty list to store the results: rc_list \u0026lt;- vector(\u0026quot;list\u0026quot;, length = length(images)) # run `recolorize2` on each image # you would probably want to add more sophisticated steps in here as well, but you get the idea for (i in 1:length(images)) { rc_list[[i]] \u0026lt;- suppressMessages(recolorize2(images[i], bins = 2, cutoff = 30, plotting = FALSE)) } # plot for comparison: layout(matrix(1:10, nrow = 2)) for (i in rc_list) { plotImageArray(i$original_img) plotImageArray(recoloredImage(i)) }  # given the variety of colors in the dataset, not too bad, # although you might go in and refine these individually  Once you have a color map you\u0026rsquo;re happy with, you can export to a variety of formats. For instance, if I wanted to run Endler\u0026rsquo;s adjacency and boundary strength analysis in the pavo package, using human perception:\nadj \u0026lt;- recolorize_adjacency(rc_list[[1]], coldist = \u0026quot;default\u0026quot;, hsl = \u0026quot;default\u0026quot;) #\u0026gt; Using single set of coldists for all images. #\u0026gt; Using single set of hsl values for all images. print(adj[ , c(57:62)]) # just print the chromatic and achromatic boundary strength values #\u0026gt; m_dS s_dS cv_dS m_dL s_dL cv_dL #\u0026gt; 36.33178 11.90417 0.3276517 24.88669 17.80173 0.7153115  If you\u0026rsquo;d like a deeper explanation of each of these steps, as well as how to modify them to suit your needs, along with what else the package can do: read on!\nBefore you start Color segmentation can be a real rabbit hole‚Äîthat is, it can be pretty easy to become fixated on getting perfect results, or on trying to define some objective standard for what correct segmentation looks like. The problem with this mindset is that there‚Äôs no set of universal parameters that will give you perfect segmentation results for every image, because images alone don‚Äôt always contain all the relevant information: color variation due to poor lighting in one image could be just as distinct as color variation due to pattern striations in another.\nThe correct output for color segmentation depends on your goal: are you concerned with identifying regions of structural vs. pigmented color? Does the intensity of the stain on your slide matter, or just presence/absence? If you have a few dozen stray pixels of the wrong color in an image with hundreds of thousands of correctly categorized pixels, will that meaningfully affect your calculations?\nLet\u0026rsquo;s take the jewel beetle (family Buprestidae) images that come with the package as an example. If I want to segment the lefthand image (Chrysochroa fulgidissima), the solution depends on my question. If my question is \u0026ldquo;How does the placement and size of these red bands compare to that of closely related beetles?\u0026rdquo; then I really just want to separate the red bands from the rest of the body, so I would want the color map in the middle. If my question is \u0026ldquo;How much do these red bands stand out from the iridescent green base of the beetle?\u0026rdquo; then I care about the brighter orange borders of the bands, because these increase the boundary strength and overall contrast in the beetle\u0026rsquo;s visual appearance‚Äîso I would go with map 2 on the right. So before you start, I highly recommend writing down precisely what you want to measure at the end of your analysis, to avoid becoming weighed down by details that may not matter. It will save you a lot of time.\nStep 0: Image acquisition \u0026amp; preparation  What to do before you use recolorize.\n Before we attempt image segmentation, we need segmentable images. recolorize doesn‚Äôt process your images for you beyond a few basic things like resizing, rotating, and blurring (which can help with segmentation). You should do all image processing steps which are usually necessary for getting quantitative color data, like white balance correction, gradient correction, or background removal, before inputting them to recolorize.\nThere are lots of software tools available for making these kinds of corrections: GIMP, FIJI/ImageJ, and even the imager package will provide options for some or all of these. If you really want to get pipeline-y, Python has a much more robust set of image processing libraries that will help with automatic color correction and background masking, which is well beyond the scope of this intro.\nIf you are at all concerned with sensory biology and animal vision, I highly recommend micaToolbox, which is a well-documented and comprehensive toolkit for creating images as animals see them (rather than as cameras and computers see them); see especially the instructions for creating false color cone-mapped images.\nThe corrections you have to make really depend on what you‚Äôre trying to do. If you just care about the regions but don‚Äôt really care about the final colors they end up being assigned, you probably don‚Äôt need to worry too much about color correction; if you‚Äôre working with histology slides, you probably don‚Äôt need to mask the background; if you have a really even and diffuse lighting setup, you probably won‚Äôt have to deal with shadows or gradients.\nBackground masking with transparencies If you‚Äôre masking the background, use transparencies. This is pretty easy to do in GIMP, Photoshop, or ImageJ. The transparency layer (or alpha channel) is the fourth channel of an image (the other three being the R, G, and B channels), and recolorize treats it like a binary mask: any pixel with an alpha value of 1 is retained, and any pixel with an alpha value of \u0026lt; 1 is ignored. This means you don‚Äôt have to worry about finding a uniform background color that is sufficiently different from your foreground object in every image, which can otherwise be a real pain.\nUsing transparency is unambiguous, and has the bonus benefit of making for nicer plots, too, since you don‚Äôt have to worry about the corners of your images overlapping and blocking each other. All the images in this demo have transparent backgrounds. However, you can use the lower and upper arguments to set boundaries for excluding pixels as background based on their color (see documentation). Just know that these will be set to transparent internally.\nStep 1: Loading \u0026amp; processing images  How to get images into R.\n We can read in an image by passing the filepath to the readImage function. This is a pretty generic function (almost every image processing package in R has something similar); the recolorize version doesn\u0026rsquo;t even assign the output to a special class (so don\u0026rsquo;t try to print it).\n# define image path - we're using an image that comes with the package img_path \u0026lt;- system.file(\u0026quot;extdata/corbetti.png\u0026quot;, package = \u0026quot;recolorize\u0026quot;) # load image img \u0026lt;- readImage(img_path, resize = NULL, rotate = NULL) # it's just an array with 4 channels: dim(img) #\u0026gt; [1] 243 116 4  An image is a numeric array with either 3 or 4 channels (R, G, B, and optionally alpha for transparency). JPG images will only have 3 channels; PNG images will have 4. This is quite a small image (243x116 pixels) with 4 channels.\nWe can plot the whole array as an image, or plot one channel at a time. Notice that the red patches are bright in the R channel, same for blue-B channel, green-G channel, etc‚Äîand that the off-white patch is bright for all channels, while the black patches are dark in all channels. The alpha channel is essentially just a mask that tells us which parts of the image to ignore when processing it further.\nlayout(matrix(1:5, nrow = 1)) plotImageArray(img, main = \u0026quot;RGB image\u0026quot;) plotImageArray(img[ , , 1], main = \u0026quot;R channel\u0026quot;) plotImageArray(img[ , , 2], main = \u0026quot;G channel\u0026quot;) plotImageArray(img[ , , 3], main = \u0026quot;B channel\u0026quot;) plotImageArray(img[ , , 4], main = \u0026quot;Alpha channel\u0026quot;)  Optionally, when you load the image, you can resize it (highly recommended for large images) and rotate it. Image processing is computationally intensive, and R is not especially good at it, so downsampling it usually a good idea. A good rule of thumb for downsampling is that you want the smallest details you care about in the image (say, spots on a ladybug) to be about 5 pixels in diameter (so if your spots have a 20 pixel diameter, you can set resize = 0.25).\nThe only other thing you might do to your images before sending them to the main recolorize functions is blurImage. This is really useful for minimizing color variation due to texture (e.g. scales on a lizard, feathers on a bird, sensory hairs on an insect), and you can apply one of several smoothing algorithms from the imager package, including edge-preserving blurs:\nblurred_img \u0026lt;- blurImage(img, blur_function = \u0026quot;blur_anisotropic\u0026quot;, amplitude = 10, sharpness = 0.2)  This step is optional: most of the recolorize functions will accept a path to an image as well as an image array. But once you\u0026rsquo;re happy here, we can start defining color regions!\nStep 2: Initial clustering  Go from thousands of colors to a manageable number for further refinement.\n The color clustering in recolorize usually starts with an initial clustering step which produces more color clusters than the final color map will have, which are then edited and combined to form the final color map. We start with an over-clustering step because it is a quick way to go from an overwhelming number of colors (256^3 unique RGB colors) to a manageable number that can be manually inspected or automatically re-clustered. You‚Äôll usually do this using the recolorize function, which is the core of the package (go figure!):\ncorbetti \u0026lt;- system.file(\u0026quot;extdata/corbetti.png\u0026quot;, package = \u0026quot;recolorize\u0026quot;) recolorize_defaults \u0026lt;- recolorize(img = corbetti) #\u0026gt; #\u0026gt; Using 2^3 = 8 total bins  This function does a lot under the hood: we read in the image as an array, binned every pixel in the image into one of eight bins in RGB color space, calculated the average color of all the pixels assigned to a given bin, recolored the image to show which pixel was assigned to which color center, and returned all of that information in the recolorize_defaults object. Pretty much everything beyond this step will be a modification of one of those elements, so we\u0026rsquo;ll take a second to examine the contents of that output.\nThe recolorize class Objects of S3 class recolorize are lists with several elements:\nattributes(recolorize_defaults) #\u0026gt; $names #\u0026gt; [1] \u0026quot;original_img\u0026quot; \u0026quot;centers\u0026quot; \u0026quot;sizes\u0026quot; #\u0026gt; [4] \u0026quot;pixel_assignments\u0026quot; #\u0026gt; #\u0026gt; $class #\u0026gt; [1] \u0026quot;recolorize\u0026quot;    original_img is a a raster matrix, essentially a matrix of hex color codes. This is a more lightweight version of the 3D/4D color image array we loaded earlier, and can be plotted easily by running plot(recolorize_defaults$original_img).\n  centers is a matrix of RGB centers (0-1 range) for each of the color patches. Their order matches the index values in the pixel_assignments matrix.\n  sizes is a vector of patch sizes, whose order matches the row order of centers.\n  pixel_assignments is a paint-by-numbers matrix, where each pixel is coded as the color center to which it was assigned. For example, cells with a 1 have been assigned to the color represented by row 1 of centers. Background pixels are marked as 0.\n  If you plot the whole recolorize object, you\u0026rsquo;ll get back the plot you see above: the original image, the color map (where each pixel has been recolored), and the color palette. You can also plot each of these individually:\nlayout(matrix(1:3, nrow = 1), widths = c(0.45, 0.45, 0.1)) par(mar = rep(0, 4)) plot(recolorize_defaults$original_img) plotImageArray(recolorize_defaults$pixel_assignments / 8) plotColorPalette(recolorize_defaults$centers, recolorize_defaults$sizes, horiz = FALSE)  You\u0026rsquo;ll notice this doesn\u0026rsquo;t look exactly like the function output above. Aside from some wonky scaling issues, the pixel assignment matrix plotted as a grayscale image (and we had to divide it by the number of colors in the image so it was in a 0-1 range). That\u0026rsquo;s because we didn\u0026rsquo;t tell R which colors to make each of those values, so layer 1 is the darkest color and layer 8 is the brightest color in the image.\nYou can get the recolored image by calling recoloredImage:\n# type = raster gets you a raster (like original_img); type = array gets you an # image array recolored_img \u0026lt;- recoloredImage(recolorize_defaults, type = \u0026quot;array\u0026quot;) plotImageArray(recolored_img)  recoloredImage is just a shortcut function for constructImage, which lets you decide which colors to assign to each category in case you want to swap out the palette:\ncolors \u0026lt;- c(\u0026quot;navy\u0026quot;, \u0026quot;lightblue\u0026quot;, \u0026quot;blueviolet\u0026quot;, \u0026quot;turquoise\u0026quot;, \u0026quot;slateblue\u0026quot;, \u0026quot;royalblue\u0026quot;, \u0026quot;aquamarine\u0026quot;, \u0026quot;dodgerblue\u0026quot;) blue_beetle \u0026lt;- constructImage(recolorize_defaults$pixel_assignments, centers = t(col2rgb(colors) / 255)) # a very blue beetle indeed: plotImageArray(blue_beetle)  Now that you have a better understanding of what these objects contain and what to do with them, we can start to unpack exactly what this function is doing.\nThe recolorize function The main recolorize function has a simple goal: to take your image from a huge number of colors to a manageable number of color clusters. This falls under a category of methods for color quantization, although we have a slightly different goal here. The typical reason for doing color quantization is to simplify an image while making it look as visually similar as possible to the original; our goal is not to represent the original image, but to create a set of building blocks to combine and clean up so we can refer to whole color patches easily.\nIf you look at the documentation for the recolorize function, you‚Äôll see a lot of user-specifiable parameters. There are only really 3 major ones:\n the color space in which the clustering is done (color_space) the clustering method (the method argument) the number of color clusters (bins for method = hist and n for method = kmeans)  You can also map an image to an externally imposed set of colors using another function, imposeColors, which can be useful for batch processing images.\nWe\u0026rsquo;ll go over each of these parameters and what they do. I\u0026rsquo;ll give mild advice about how to navigate these options, but there\u0026rsquo;s a reason I\u0026rsquo;ve included all of theme here, which is that I think any combination of these parameters can be useful depending on the context.\nColor spaces Color spaces are ways to represent colors as points in multi-dimensional spaces, where each axis corresponds to some aspect of the color. You\u0026rsquo;re probably familiar with RGB (red-green-blue) color space and HSV (hue-saturation-value) color space. In RGB space, colors vary by the amount of red, green, and blue they have, where a coordinate of [0, 0, 1] would be pure blue (no red or green), [1, 1, 1] would be white, [0, 1, 1] would be cyan, etc. This is how most images are stored and displayed on computers, although it\u0026rsquo;s not always very intuitive.\nThe recolorize package gives you a variety of options for color spaces, but by far the two most commonly used are RGB (color_space = sRGB) and CIE Lab (color_space = Lab). CIE Lab is popular because it approximates perceptual uniformity, which means that the distances between colors in CIE Lab space are proportional to how different they actually seem to human beings. The axes represent luminance (L, 0 = black and 100 = white), red-green (a, negative values = more green and positive values = more red), and blue-yellow (b, negative values = more blue and positive values = more yellow). The idea is that something can be greenish-blue, or reddish-yellow, but not reddish-green, etc. This can be a little confusing, but the results it provides are really intuitive. For example, in RGB space, red is as similar to yellow as it is to black. In CIE Lab, red and yellow are close together, and are about equally far from black.\nI\u0026rsquo;ve written in more detail about color spaces for another package here, which I would recommend reading for a more detailed overview, but let\u0026rsquo;s see what happens if we plot all of the non-background pixels from our C. corbetti example in RGB compared to CIE Lab color space (forgive the crummy plotting):\nWe can identify green, red, blue, black, and white pixels in both sets of plots, but their distributions are very different.\nIn practice, I find myself toggling between these two color spaces depending on the color distributions in my images. For example, when dealing with C. corbetti, I would use RGB, because the beetle is literally red, green, and blue. When dealing with the red and green C. fulgidissima above, I found that CIE Lab produced better results, because it separates red and green pixels by much more distance. But in general, especially as you increase the number of initial clusters, this matters less at this stage than at the refinement stage (where you can switch between color spaces again). Because CIE Lab is not evenly distributed on all axes (i.e. is not a cube), you may need to use more bins in CIE Lab space than in RGB. (Try fitting the C. corbetti image with CIE Lab space and see what happens for an idea of how much the choice of color space can matter.)\nClustering methods The two clustering methods in recolorize are color histogram binning (fast, consistent, and deterministic) and k-means clustering (comparatively slower and heuristic, but more intuitive). The bins argument is accessed by the histogram method, and n goes with the kmeans method. I highly recommend the histogram binning unless you have a good reason not to use it, but there are good reasons to use k-means clustering sometimes.\nThe histogram binning method is essentially just a 3-dimensional color histogram: we divide up each channel of a color space into a predetermined number of bins, then count the number of pixels that fall into that bin and calculate their average color. So, when we divide each of 3 color channels into 2 bins, we end up with \\(2^3 = 8\\) total bins (which is why setting bins = 2 will produce 8 colors as above).\nk-means clustering, on the other hand, is a well-known method for partitioning data into n clusters. You just provide the number of clusters you want, and it will try to find the best locations for them, where ‚Äòbest‚Äô means minimizing the squared Euclidean distances between pixels and color centers within each cluster.\nTo appreciate these differences, we can fit the same number of colors (64) using the histogram method and the k-means method on the same image, then view the resulting color distributions:\n# fit 64 colors, both ways r_hist \u0026lt;- recolorize(img_path, method = \u0026quot;hist\u0026quot;, bins = 4, plotting = FALSE) #\u0026gt; #\u0026gt; Using 4^3 = 64 total bins r_k \u0026lt;- recolorize(img_path, method = \u0026quot;k\u0026quot;, n = 64, plotting = FALSE) plotColorClusters(r_hist$centers, r_hist$sizes, plus = .5, xlab = \u0026quot;red\u0026quot;, ylab = \u0026quot;green\u0026quot;, zlab = \u0026quot;blue\u0026quot;, mar = c(3, 3, 2, 2), main = \u0026quot;Histogram method\u0026quot;)  plotColorClusters(r_k$centers, r_k$sizes, plus = .5, xlab = \u0026quot;red\u0026quot;, ylab = \u0026quot;green\u0026quot;, zlab = \u0026quot;blue\u0026quot;, mar = c(3, 3, 2, 2), main = \u0026quot;k-means clustering\u0026quot;)  The histogram method produced a lot of tiny, nearly-empty clusters that are evenly distributed in the color space, with only a few large clusters (like the black and white ones). The k-means clustering method, on the other hand, produced a lot more medium-sized clusters, as well as splitting the black and white patches across multiple clusters.\nA lot of color segmentation tools will only use k-means clustering (or a similar method), because it‚Äôs relatively easy to implement and does produce good results if your images have clear color boundaries and very different colors (i.e. the pixels are far apart in color space). If you were going to stop at the initial clustering step, this would probably be a better option than the histogram binning for that reason. The main reason I recommend against it is that it is not deterministic: you will get different colors, and in a different order, every time you run it. For example, if we fit 10 colors three separate times, we get the following color palettes:\nk_list \u0026lt;- lapply(1:3, function(i) recolorize(img_path, \u0026quot;k\u0026quot;, n = 10, plotting = F)) layout(1:3) par(mar = rep(1, 4)) lapply(k_list, function(i) plotColorPalette(i$centers, i$sizes))  #\u0026gt; [[1]] #\u0026gt; NULL #\u0026gt; #\u0026gt; [[2]] #\u0026gt; NULL #\u0026gt; #\u0026gt; [[3]] #\u0026gt; NULL  The colors are similar, but not identical, and they are returned in an arbitrary order. If you run this code one day and pull out all the red clusters by their index, or merge the multiple green clusters, those values will change the next time you run the code. That and the need to specify cluster numbers for each image are more or less why I recommend not using this method unless you have a reason.\nBinning the colors (histograms) is usually more viable as a first step. It‚Äôs quite fast, since we‚Äôre not really doing any clustering; the bins we assign the pixels to will be the same for every image, and we‚Äôre not calculating the distances between the pixels and their assigned color. It‚Äôs also deterministic, which means you get the same result every single time you run it. The downside is that makes this approach almost guaranteed to over-split colors, since your color regions will rarely fall cleanly within the boundaries of these bins, and many of the bins you end up with will be empty or have very few pixels.\nNumber of clusters Unlike the color space and binning method, this parameter is pretty intuitive: the more clusters you fit, the more the colors in your image will be split up. It‚Äôs convenient to use the same scheme for every image in your dataset, so you might end up using whatever values are needed for your most complex image and over-splitting most of your other images. That‚Äôs usually fine, because the next set of steps will try to lump colors together or remove minor details. You want to be just granular enough to capture the details you care about, and it‚Äôs okay if some colors are split up.\nOne thing to note is that the bins argument allows for a different number of bins for each channel. Setting bins = 2 will divide each channel into 2 bins, but you can also set bins = c(5, 2, 2) to divide up the red channel into 5 bins and the blue and green channels into 2 bins (if in RGB space). This can be convenient if you have a lot of color diversity on only one axis, e.g. you have photographs of mammals which are shades of reddish-brown, and don\u0026rsquo;t need to waste computational time dividing up the blue channel.\n# we can go from an unacceptable to an acceptable color map in # CIE Lab space by adding a single additional bin in the luminance channel: r_hist_2 \u0026lt;- recolorize(img_path, method = \u0026quot;hist\u0026quot;, color_space = \u0026quot;Lab\u0026quot;, bins = 2) #\u0026gt; #\u0026gt; Using 2^3 = 8 total bins  r_hist_322 \u0026lt;- recolorize(img_path, method = \u0026quot;hist\u0026quot;, bins = c(3, 2, 2)) #\u0026gt; #\u0026gt; Using 3*2*2 = 12 bins  imposeColors() Another option is to impose colors on an image, rather than using intrinsic image colors. Every pixel is assigned to the color it is closest to in some specified color space. Usually, this is useful for batch processing: you get colors from one image, then map them onto another image, so that the color centers correspond across all your images.\nim1 \u0026lt;- system.file(\u0026quot;extdata/ocellata.png\u0026quot;, package = \u0026quot;recolorize\u0026quot;) im2 \u0026lt;- system.file(\u0026quot;extdata/ephippigera.png\u0026quot;, package = \u0026quot;recolorize\u0026quot;) # fit the first image fit1 \u0026lt;- recolorize(im1) #\u0026gt; #\u0026gt; Using 2^3 = 8 total bins  # fit the second image using colors from the first # adjust_centers = TRUE would find the average color of all the pixels assigned to # the imposed colors to better match the raw image fit2 \u0026lt;- imposeColors(im2, fit1$centers, adjust_centers = FALSE)  Step 3: Refinement  Using simple rules to improve the initial results.\n Once we‚Äôve reduced an image down to a tractable number of colors, we can define simple procedures for how to combine them based on similarity. recolorize (currently) comes with two of these: recluster, which merges colors by perceived similarity, and thresholdRecolor, which drops minor colors. Both are simple, but surprisingly effective. They‚Äôre also built on top of some really simple functions we‚Äôll see in a bit, so if you need to, you can build out a similar procedure tailored to your dataset‚Äîfor example, combining layers based only on their brightness values, or only combining green layers.\nrecluster() and recolorize2() This is the one I use the most often, and its implementation is really simple. This function calculates the Euclidean distances between all the color centers in a recolorize object, clusters them hierarchically using hclust, then uses a user-specified cutoff to combine the most similar colors. As with recolorize, you can choose your color space, and that will make a big difference. Let‚Äôs see this in action:\nrecluster_results \u0026lt;- recluster(recolorize_defaults, similarity_cutoff = 45)  Notice the color dendrogram: it lumped together clusters 4 \u0026amp; 7, clusters 3 \u0026amp; 5, and clusters 6 \u0026amp; 8, because their distance was less than 45. This is in CIE Lab space; if we use RGB space, the range of distances is 0-1:\nrecluster_rgb \u0026lt;- recluster(recolorize_defaults, color_space = \u0026quot;sRGB\u0026quot;, similarity_cutoff = 0.5)  In this case, we get the same results, but this is always worth playing around with. Despite its simplicity, this function is highly effective at producing intuitive results. This is partly because, in only using color similarity to combine clusters, it does not penalize smaller color clusters that can still retain important details. I find myself using it so often that I included a wrapper function, recolorize2, to run recolorize and recluster sequentially in a single step:\n# let's use a different image: img \u0026lt;- system.file(\u0026quot;extdata/chongi.png\u0026quot;, package = \u0026quot;recolorize\u0026quot;) # this is identical to running: # fit1 \u0026lt;- recolorize(img, bins = 3) # fit2 \u0026lt;- recluster(fit1, similarity_cutoff = 50) chongi_fit \u0026lt;- recolorize2(img, bins = 3, cutoff = 45) #\u0026gt; #\u0026gt; Using 3^3 = 27 total bins  There‚Äôs also a lot of room for modification here: this is a pretty unsophisticated rule for combining color clusters (ignoring, for example, cluster size, proximity, geometry, and boundary strength), but it‚Äôs pretty simple to write better rules if you can think of them, because the functions that are called to implement this are also exported by the package.\nthresholdRecolor() An even simpler rule: drop the smallest color clusters whose cumulative sum (as a proportion of total pixels assigned) is lower than some threshold, like 5% of the image. I thought this would be too simple to be useful, but every once in a while it‚Äôs just the thing, especially if you always end up with weird spurious details.\nchongi_threshold \u0026lt;- thresholdRecolor(chongi_fit, pct = 0.1)  Step 4: Minor edits  Cleaning up the details.\n These are functions that can be called individually to address problem areas in specific images, or strung together as building blocks to do more complicated operations.\nabsorbLayer \u0026ldquo;Absorbs\u0026rdquo; all or part of a layer into the surrounding colors, optionally according to a size or location condition.\nimg \u0026lt;- system.file(\u0026quot;extdata/fulgidissima.png\u0026quot;, package = \u0026quot;recolorize\u0026quot;) ful_init \u0026lt;- recolorize2(img, bins = 3, cutoff = 60, plotting = F) #\u0026gt; #\u0026gt; Using 3^3 = 27 total bins ful_absorb \u0026lt;- absorbLayer(ful_init, layer_idx = 3, function(s) s \u0026lt;= 250, y_range = c(0, 0.8), highlight_color = \u0026quot;cyan\u0026quot;)  This function is really useful, but fair warning: it can be quite slow. It works by finding the color patch with which each highlighted component shares the longest border and switching the highlighted component to that color, which is more sophisticated than simply switching the patch color, but requires many more calculations. If you find yourself using this a lot, it\u0026rsquo;s a good idea to make sure you\u0026rsquo;ve downsampled your images using the resize argument.\neditLayer/editLayers Applies one of several morphological operations from imager to a layer (or layers) of a recolorize object. This can be used to despeckle, fill in holes, or uniformly grow or shrink a color patch. In practice, this is mostly only useful for fixing small imperfections; anything too drastic tends to alter the overall shape of the patch.\n# cleans up some of the speckles in the above output ful_clean \u0026lt;- editLayers(ful_absorb, layer_idx = c(2, 5), operations = \u0026quot;fill\u0026quot;, px_sizes = 3, plotting = T)  This function is also easy to modify. Internally, it splits the color map into individual masks using splitByColor() (another recolorize function), then converts those to pixsets for use in imager before slotting them back in with the unchanged layers.\nmergeLayers Sometimes, you don‚Äôt want to define fancy rules for deciding which layers to combine; you just want to combine layers. That‚Äôs what this function is for. It takes in a list of numeric vectors for layers to combine (layers in the same vector are combined; those in different list elements are kept separate).\nmerge_fit \u0026lt;- mergeLayers(recolorize_defaults, merge_list = list(1, 2, c(3, 5), c(4, 7), c(6, 8)))  You might notice this is a bit different than our recluster results above. That‚Äôs because internally, recluster actually uses imposeColors to refit the color map, rather than just merging layers; I have found this often produces slightly nicer results, because pixels that were on the border of one cutoff or another don‚Äôt get stranded in the wrong layer. On the other hand, mergeLayers is considerably faster.\nStep 4.5: Visualizations Making color maps is an obviously visual process, so it‚Äôs good to use visual feedback as much as possible. We‚Äôve already seen a few of these functions in action, specifically plotColorPalette and plotImageArray, which are used in almost every function that produces a recolorize object. I‚Äôll point out three others that I think are quite useful: imDist, plotColorClusters, and splitByColor (which also doubles as an export function).\nimDist Compares two versions of the same image by calculating the color distance between the colors of each pair of pixels (imDist), and gives you a few more options for plotting the results (imHeatmap). You can use it to get the distances between the original image and the color map:\nlayout(matrix(1:2, nrow = 1)) # calculates the distance matrix and plots the results dist_original \u0026lt;- imDist(readImage(img), recoloredImage(ful_clean), color_space = \u0026quot;sRGB\u0026quot;) # more plotting options - setting the range is important for comparing # across images (max is sqrt(3) in sRGB space, ~120 in Lab) imHeatmap(dist_original, viridisLite::inferno(100), range = c(0, sqrt(3)))  The resulting object is a simple matrix of distances between each pair of pixels in the given color space. These are essentially residuals:\nhist(dist_original, main = \u0026quot;sRGB distances\u0026quot;, xlab = \u0026quot;Distance\u0026quot;)  A word of warning here: it is easy to look at this and decide to come up with a procedure for automatically fitting color maps using a kind of AIC metric, trying to get the lowest SSE with the minimum set of color centers. You‚Äôre welcome to try that, but given that this is discarding spatial information, it is probably not a general solution (I haven‚Äôt had much luck with it). But there is probably some room to play here.\nsplitByColor This is a dual-use function: by splitting up the color map into individual layers, you not only can examine the individual layers and decide whether they need any editing or merging, but you also get out a binary mask representing each layer, so you can export individual patches.\nlayout(matrix(1:10, nrow = 2, byrow = TRUE)) # 'overlay' is not always the clearest option, but it is usually the prettiest: layers \u0026lt;- splitByColor(recluster_results, plot_method = \u0026quot;overlay\u0026quot;) # layers is a list of matrices, which we can just plot: lapply(layers, plotImageArray)  #\u0026gt; [[1]] #\u0026gt; [[1]]$mar #\u0026gt; [1] 0 0 2 0 #\u0026gt; #\u0026gt; #\u0026gt; [[2]] #\u0026gt; [[2]]$mar #\u0026gt; [1] 0 0 2 0 #\u0026gt; #\u0026gt; #\u0026gt; [[3]] #\u0026gt; [[3]]$mar #\u0026gt; [1] 0 0 2 0 #\u0026gt; #\u0026gt; #\u0026gt; [[4]] #\u0026gt; [[4]]$mar #\u0026gt; [1] 0 0 2 0 #\u0026gt; #\u0026gt; #\u0026gt; [[5]] #\u0026gt; [[5]]$mar #\u0026gt; [1] 0 0 2 0  Step 5: Exporting  The whole point of this package is to make it easier to use other methods!\n Exporting to aimges The most direct thing you can do is simply export your recolored images as images, then pass those to whatever other tool you‚Äôd like to use, although obviously this doesn‚Äôt take full advantage of the format:\n# export color map png::writePNG(recoloredImage(recluster_results), target = \u0026quot;recolored_corbetti.png\u0026quot;) # export individual layers from splitByColor for (i in 1:length(layers)) { png::writePNG(layers[[i]], target = paste0(\u0026quot;layer_\u0026quot;, i, \u0026quot;.png\u0026quot;)) }  pavo package You can also convert a recolorize object to a classify object in the wonderful pavo package and then run an adjacency analysis. Bonus points if you have reflectance spectra for each of your color patches: by combining the spatial information in the color map with the coldist object generated by spectral measurements, you can run adjacency analysis for the visual system(s) of your choice right out of the box!\n# convert to a classify object as_classify \u0026lt;- classify_recolorize(recluster_results, imgname = \u0026quot;corbetti\u0026quot;) adj_analysis \u0026lt;- pavo::adjacent(as_classify, xscale = 10) # run adjacent directly using human perceptual color distances (i.e. no spectral data - proceed with caution) adj_human \u0026lt;- recolorize_adjacency(recluster_results)  You can also run an adjacency analysis with recolorize_adjacency, but only as long as you keep your skeptic hat on. This function works by calculating a coldist object right from the CIE Lab colors in the color maps, which are themselves probably derived from your RGB image, which is at best a very loose representation of how these colors appear to human eyes. The only reason this is at all reasonable is that it‚Äôs producing these values for human vision, so you will be able to see if it‚Äôs completely unreasonable. This is fine for getting some preliminary results or if you‚Äôre working with aggregate data from many sources and you‚Äôre content with specifically human (not just non-UV, but only human) vision. Otherwise, it‚Äôs probably a last resort.\npatternize Coming soon (pending a patternize update), and with many thanks to Steven van Belleghem for his help in making recolorize and patternize get along!\nSome advice This is a lot of options. How do I choose a procedure? Most things will more or less work; if it looks reasonable, it is. Keep in mind that there is a big difference between getting slightly different color maps and getting qualitatively different results. Keep your final goal in mind. You can also try lots of different things and see if it makes a real difference.\nI wish I could write a single function that would do all of these steps in the correct sequence and produce perfect results; the reason that function does not exist is because I find I have to do experiment a fair amount with every image set, and I often end up with a different order of operations depending on the problem.\nStart with recolorize2 and identify the common problems you\u0026rsquo;re encountering. Does it make sense to batch process all of your images, then refine them individually? Is it better to choose a different cutoff for each image? Luckily, these functions are relatively fast, so you can test out different options.\nYou can also get way fancier with cutoffs than I have here. This package is built on some pretty simple scaffolding: you get a starting set of clusters, then you modify them. If you have a better/more refined way of deciding which colors to cluster, then go for it. I will soon be adding some example workflows from collaborators which should be helpful.\nThere is another very tempting option: make a small training set of nice color maps manually with recolorize, then use those to either fit a statistical model for other fits or use machine learning to do the rest. I think this is a really compelling idea; I just haven\u0026rsquo;t tested it yet. Maybe you want to try it out?\nCan you define an optimality condition to do all the segmentation automatically? As far as I can tell, no. This is because of the problem I pointed out at the beginning: the \u0026lsquo;correct\u0026rsquo; segmentation depends on your particular question more than anything else.\nHow should you store the code used to generate a color map? I like to use rlang::enexpr to capture the code I run to generate a color map, and store it as another aspect of the recolorize object, like so:\nlibrary(rlang) # run this code, then capture it in the brackets: steps \u0026lt;- { fit \u0026lt;- recolorize2(img,bins = 3, cutoff = 50) fit2 \u0026lt;- editLayers(fit, c(2, 5), operations = \u0026quot;fill\u0026quot;, px_sizes = 3) } %\u0026gt;% enexprs() fit2$steps \u0026lt;- steps  What about batch processing? Every function in this package operates on a single image at a time. This is because I\u0026rsquo;ve found that there is so much variation in how people go about batch processing anything: if I tried to impose what I considered to be a useful batch processing structure, within a few months I would find that it was too inflexible for some new project structure I needed to use it for. So, instead, the idea is that you can write your own batch processing functions or for loops as needed to suit your data structure. Or maybe you come up with something better than I can think of, in which case, please let me add it to the package!\nWhat about machine learning approaches? Using machine learning could work, but only if you already have segmented images for use in training (which presumably you had to do by hand), and making that training set could be extremely time consuming; and the amount of modification required to get a generic algorithm to work might be unjustifiable given the size of (or variance in) your image set. This problem gets a lot worse the more images we have and the more different they are, especially if you have a lot of variance in a small dataset (pretty typical in comparative biology).\nThat said, I don\u0026rsquo;t have much background in ML of any stripe. If you have a handy idea in this area, I would love to know about it.\nJust for fun There are two fun functions in here: wernerColor and recolorizeVector.\nwernerColor remaps a recolorize object to the colors in Werner\u0026rsquo;s Nomenclature of Colors by Patrick Syme (1821), one of the first attempts at an objective color reference in western science, notably used by Charles Darwin. This is always fun to try out, especially given how many things get tagged as \u0026ldquo;veinous blood red\u0026rdquo; (delightful!):\nrc_werner \u0026lt;- wernerColor(recluster_results)  Finally, recolorizeVector converts a bitmap (i.e. pixel) image to a vector image.\nrc_vector \u0026lt;- recolorizeVector(recluster_fit, size_filter = 0.15, smoothness = 5, plotting = TRUE) # to save as an SVG: svg(filename = \u0026quot;corbett_vector.svg\u0026quot;, width = 2, height = 4) plot(rc_vector) dev.off()  This function is VERY experimental. If it gives you errors or looks too funky, try decreasing the size filter (which absorbs all components below some size to simplify the image) and the smoothness. Then again, sometimes you want things to look funky. If this is the case, recolorizeVector will happily enable you.\n","date":1618272000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1618349069,"objectID":"e99cbd225ad595d6a216ac0cb259f778","permalink":"/post/introduction-to-recolorize/","publishdate":"2021-04-13T00:00:00Z","relpermalink":"/post/introduction-to-recolorize/","section":"post","summary":"A copy of the recolorize introductory vignette.","tags":["recolorize","color","r packages"],"title":"Introduction to recolorize","type":"post"},{"authors":["Hannah Weller"],"categories":[],"content":"            The plotPixels function in colordistance is pretty inflexible. It was originally meant as a diagnostic tool, and the plots it produces are not exactly beautiful:\nlibrary(colordistance) # image from the 'recolorize' package (github.com/hiweller/recolorize) img \u0026lt;- system.file(\u0026quot;extdata/fulgidissima.png\u0026quot;, package = \u0026quot;recolorize\u0026quot;) # load the image: loaded_img \u0026lt;- loadImage(img) # set the plot layout for opposing pixel plots layout(matrix(1:3, nrow = 1), widths = c(0.46, 0.08, 0.46)) # plot the pixels in RGB color space from two angles: plotPixels(loaded_img) # plot the original image par(mar = rep(0, 4)) # no margin plotImage(loaded_img) # and pixels from the opposite angle: plotPixels(loaded_img, angle = -45)  These plots are certainly fine if you want to scope out the color distribution in the image, but I wouldn‚Äôt want to display them for communication: the axis text is too large and some of the tick marks overlap; the axis labels are oddly spaced; and depending on the intention of the graphic, I might not want the grid or the plot frame. The axis label thing in particular has always bothered me.\nSome of those changes are possible to make by passing additional parameters to the plotPixels function itself, but in practice, I often want more flexibility than this provides. Luckily, the function itself has such simple building blocks that it‚Äôs pretty easy to unpack them to get more customized plots.\nThis is how plotPixels works:\n It takes a dataframe of RGB colors, where pixels are rows and color channels are columns. It creates a vector of hex codes from the RGB colors to tell R which color to make each point. It uses scatterplot3d to plot in the 3D color space indicated with the color.space argument.  I chose the scatterplot3d package because, of all the 3D plotting packages, it‚Äôs the most lightweight, and more or less just extends the base plotting syntax. It was also written in 2003, so there are a lot of newer packages that provide prettier output and more options, like plot3D by Karline Soetaert, or the plotly library.\n# load the plot3D library library(plot3D) # get the RGB pixel matrix pixels \u0026lt;- loaded_img$filtered.rgb.2d # make the hex color vector using the rgb() function color_vector \u0026lt;- rgb(pixels); head(color_vector) # just a bunch of hex codes!  ## [1] \u0026quot;#247872\u0026quot; \u0026quot;#006862\u0026quot; \u0026quot;#006B62\u0026quot; \u0026quot;#00776A\u0026quot; \u0026quot;#00645C\u0026quot; \u0026quot;#007B71\u0026quot;  # use the scatter3D function scatter3D(x = pixels[ , 1], y = pixels[ , 2], z = pixels[ , 3], colvar = 1:nrow(pixels), # \u0026lt;- note we have to make a fake 'variable' to assign each pixel a different color col = color_vector, colkey = FALSE, # gets rid of the (in this case meaningless) legend xlab = \u0026quot;Red\u0026quot;, ylab = \u0026quot;Green\u0026quot;, zlab = \u0026quot;Blue\u0026quot;)  Even the default scatter3D plot looks a lot better to me: the axis labels hug the axes, and the angle is nicer. We can get fancier with a lot of the options, too:\nscatter3D(x = pixels[ , 1], y = pixels[ , 2], z = pixels[ , 3], colvar = 1:nrow(pixels), col = color_vector, colkey = F, xlab = \u0026quot;Red\u0026quot;, ylab = \u0026quot;Green\u0026quot;, zlab = \u0026quot;Blue\u0026quot;, xlim = 0:1, ylim = 0:1, zlim = 0:1, # RGB max and min pch = 19, # filled circles alpha = 0.5, # partially transparent theta = 115, phi = 25, # change viewing angle bty = \u0026quot;bl2\u0026quot;) # black grid background looks sort of cool  What if you want to plot in another color space besides RGB? The only difference is that you have to first convert your pixel matrix to a given color space, for which you have several options.\n# convert pixels to CIE Lab coordinates pixels_lab \u0026lt;- convertColor(pixels, from = \u0026quot;sRGB\u0026quot;, to = \u0026quot;Lab\u0026quot;) # color vector remains the same! color_vector \u0026lt;- rgb(pixels) scatter3D(x = pixels_lab[ , 1], y = pixels_lab[ , 2], z = pixels_lab[ , 3], colvar = 1:nrow(pixels_lab), col = color_vector, colkey = F, xlab = \u0026quot;Luminance\u0026quot;, ylab = \u0026quot;a (red-green)\u0026quot;, zlab = \u0026quot;b (yellow-blue)\u0026quot;, theta = 120, phi = -5, xlim = c(0, 100), pch = 19, # filled circles alpha = 0.5, # partially transparent bty = \u0026quot;b2\u0026quot;)  As an aside, it‚Äôs good practice to set the axis limits thoughtfully. This is easy with RGB: all three channels have a 0-1 range. With CIE Lab, this depends on your reference white. The L channel will always be 0-100, and the outer limits for the a and b channels are -127 to 128 each, but for a given reference white converting from sRGB it will be a subset within that range. The axis limits will be set to the range of the data by default, which could be misleading if you‚Äôre comparing plots of multiple images.\nIf you‚Äôd rather have an interactive plot (especially helpful for data exploration), you can use the plotly package. I find I have to implement more workarounds to get these plots to behave how I‚Äôd expect, but once you get out an interactive plot, it‚Äôs pretty slick:\nlibrary(plotly, quietly = TRUE) # let's subsample down to 100 pixels just for this example pixel_sub \u0026lt;- as.data.frame(pixels[sample(1:nrow(pixels), 100), ]) plotly_colors \u0026lt;- rgb(pixel_sub) # and plot! plot_ly(data = pixel_sub, x = ~r, y = ~g, z = ~b, type = \u0026quot;scatter3d\u0026quot;, mode = \u0026quot;markers\u0026quot;, color = I(plotly_colors), # this is a bit of a hack and you'll get a warning... colors = plotly_colors)   {\"x\":{\"visdat\":{\"948a74a44f66\":[\"function () \",\"plotlyVisDat\"]},\"cur_data\":\"948a74a44f66\",\"attrs\":{\"948a74a44f66\":{\"x\":{},\"y\":{},\"z\":{},\"mode\":\"markers\",\"color\":[\"#4D272F\",\"#008173\",\"#220C10\",\"#DB4042\",\"#402E2E\",\"#6A2023\",\"#342232\",\"#578900\",\"#31811C\",\"#6B813F\",\"#0092BA\",\"#4B3842\",\"#9F343D\",\"#00AD7B\",\"#52AA23\",\"#51333D\",\"#0B803A\",\"#405D44\",\"#549BC2\",\"#709200\",\"#559864\",\"#67202C\",\"#6E7600\",\"#007E3A\",\"#38A337\",\"#676A20\",\"#55423B\",\"#85C400\",\"#425F46\",\"#009D42\",\"#37833C\",\"#31AC00\",\"#539000\",\"#C43628\",\"#858F00\",\"#9B1F2C\",\"#009C3D\",\"#B9202A\",\"#50512C\",\"#8A8884\",\"#822122\",\"#329D86\",\"#6F661D\",\"#008D3B\",\"#006E45\",\"#006E7A\",\"#861A31\",\"#339000\",\"#427A0F\",\"#BBC4E1\",\"#29A234\",\"#52781E\",\"#049509\",\"#877D00\",\"#78131D\",\"#509B35\",\"#0084D4\",\"#00B27B\",\"#00697F\",\"#7D272B\",\"#44A212\",\"#BA9E00\",\"#6E303B\",\"#325E3D\",\"#288929\",\"#781E21\",\"#638B00\",\"#1A1B20\",\"#E47C00\",\"#832330\",\"#837F41\",\"#546E0D\",\"#611F22\",\"#30822C\",\"#66272E\",\"#7A770C\",\"#6E7600\",\"#3B6E3D\",\"#972931\",\"#009658\",\"#573B39\",\"#1D963C\",\"#00DCBE\",\"#4A5786\",\"#8B8E00\",\"#006A85\",\"#00852E\",\"#4E803E\",\"#006C82\",\"#00972E\",\"#009037\",\"#4C711E\",\"#531C29\",\"#0072A8\",\"#3B262B\",\"#4C9628\",\"#698A35\",\"#658300\",\"#B96612\",\"#9F5918\"],\"colors\":[\"#4D272F\",\"#008173\",\"#220C10\",\"#DB4042\",\"#402E2E\",\"#6A2023\",\"#342232\",\"#578900\",\"#31811C\",\"#6B813F\",\"#0092BA\",\"#4B3842\",\"#9F343D\",\"#00AD7B\",\"#52AA23\",\"#51333D\",\"#0B803A\",\"#405D44\",\"#549BC2\",\"#709200\",\"#559864\",\"#67202C\",\"#6E7600\",\"#007E3A\",\"#38A337\",\"#676A20\",\"#55423B\",\"#85C400\",\"#425F46\",\"#009D42\",\"#37833C\",\"#31AC00\",\"#539000\",\"#C43628\",\"#858F00\",\"#9B1F2C\",\"#009C3D\",\"#B9202A\",\"#50512C\",\"#8A8884\",\"#822122\",\"#329D86\",\"#6F661D\",\"#008D3B\",\"#006E45\",\"#006E7A\",\"#861A31\",\"#339000\",\"#427A0F\",\"#BBC4E1\",\"#29A234\",\"#52781E\",\"#049509\",\"#877D00\",\"#78131D\",\"#509B35\",\"#0084D4\",\"#00B27B\",\"#00697F\",\"#7D272B\",\"#44A212\",\"#BA9E00\",\"#6E303B\",\"#325E3D\",\"#288929\",\"#781E21\",\"#638B00\",\"#1A1B20\",\"#E47C00\",\"#832330\",\"#837F41\",\"#546E0D\",\"#611F22\",\"#30822C\",\"#66272E\",\"#7A770C\",\"#6E7600\",\"#3B6E3D\",\"#972931\",\"#009658\",\"#573B39\",\"#1D963C\",\"#00DCBE\",\"#4A5786\",\"#8B8E00\",\"#006A85\",\"#00852E\",\"#4E803E\",\"#006C82\",\"#00972E\",\"#009037\",\"#4C711E\",\"#531C29\",\"#0072A8\",\"#3B262B\",\"#4C9628\",\"#698A35\",\"#658300\",\"#B96612\",\"#9F5918\"],\"alpha_stroke\":1,\"sizes\":[10,100],\"spans\":[1,20],\"type\":\"scatter3d\"}},\"layout\":{\"margin\":{\"b\":40,\"l\":60,\"t\":25,\"r\":10},\"scene\":{\"xaxis\":{\"title\":\"r\"},\"yaxis\":{\"title\":\"g\"},\"zaxis\":{\"title\":\"b\"}},\"hovermode\":\"closest\",\"showlegend\":false},\"source\":\"A\",\"config\":{\"showSendToCloud\":false},\"data\":[{\"x\":[0.301960784313725,0,0.133333333333333,0.858823529411765,0.250980392156863,0.415686274509804,0.203921568627451,0.341176470588235,0.192156862745098,0.419607843137255,0,0.294117647058824,0.623529411764706,0,0.32156862745098,0.317647058823529,0.0431372549019608,0.250980392156863,0.329411764705882,0.43921568627451,0.333333333333333,0.403921568627451,0.431372549019608,0,0.219607843137255,0.403921568627451,0.333333333333333,0.52156862745098,0.258823529411765,0,0.215686274509804,0.192156862745098,0.325490196078431,0.768627450980392,0.52156862745098,0.607843137254902,0,0.725490196078431,0.313725490196078,0.541176470588235,0.509803921568627,0.196078431372549,0.435294117647059,0,0,0,0.525490196078431,0.2,0.258823529411765,0.733333333333333,0.16078431372549,0.32156862745098,0.0156862745098039,0.529411764705882,0.470588235294118,0.313725490196078,0,0,0,0.490196078431373,0.266666666666667,0.729411764705882,0.431372549019608,0.196078431372549,0.156862745098039,0.470588235294118,0.388235294117647,0.101960784313725,0.894117647058824,0.513725490196078,0.513725490196078,0.329411764705882,0.380392156862745,0.188235294117647,0.4,0.47843137254902,0.431372549019608,0.231372549019608,0.592156862745098,0,0.341176470588235,0.113725490196078,0,0.290196078431373,0.545098039215686,0,0,0.305882352941176,0,0,0,0.298039215686275,0.325490196078431,0,0.231372549019608,0.298039215686275,0.411764705882353,0.396078431372549,0.725490196078431,0.623529411764706],\"y\":[0.152941176470588,0.505882352941176,0.0470588235294118,0.250980392156863,0.180392156862745,0.125490196078431,0.133333333333333,0.537254901960784,0.505882352941176,0.505882352941176,0.572549019607843,0.219607843137255,0.203921568627451,0.67843137254902,0.666666666666667,0.2,0.501960784313725,0.364705882352941,0.607843137254902,0.572549019607843,0.596078431372549,0.125490196078431,0.462745098039216,0.494117647058824,0.63921568627451,0.415686274509804,0.258823529411765,0.768627450980392,0.372549019607843,0.615686274509804,0.513725490196078,0.674509803921569,0.564705882352941,0.211764705882353,0.56078431372549,0.12156862745098,0.611764705882353,0.125490196078431,0.317647058823529,0.533333333333333,0.129411764705882,0.615686274509804,0.4,0.552941176470588,0.431372549019608,0.431372549019608,0.101960784313725,0.564705882352941,0.47843137254902,0.768627450980392,0.635294117647059,0.470588235294118,0.584313725490196,0.490196078431373,0.0745098039215686,0.607843137254902,0.517647058823529,0.698039215686274,0.411764705882353,0.152941176470588,0.635294117647059,0.619607843137255,0.188235294117647,0.368627450980392,0.537254901960784,0.117647058823529,0.545098039215686,0.105882352941176,0.486274509803922,0.137254901960784,0.498039215686275,0.431372549019608,0.12156862745098,0.509803921568627,0.152941176470588,0.466666666666667,0.462745098039216,0.431372549019608,0.16078431372549,0.588235294117647,0.231372549019608,0.588235294117647,0.862745098039216,0.341176470588235,0.556862745098039,0.415686274509804,0.52156862745098,0.501960784313725,0.423529411764706,0.592156862745098,0.564705882352941,0.443137254901961,0.109803921568627,0.447058823529412,0.149019607843137,0.588235294117647,0.541176470588235,0.513725490196078,0.4,0.349019607843137],\"z\":[0.184313725490196,0.450980392156863,0.0627450980392157,0.258823529411765,0.180392156862745,0.137254901960784,0.196078431372549,0,0.109803921568627,0.247058823529412,0.729411764705882,0.258823529411765,0.23921568627451,0.482352941176471,0.137254901960784,0.23921568627451,0.227450980392157,0.266666666666667,0.76078431372549,0,0.392156862745098,0.172549019607843,0,0.227450980392157,0.215686274509804,0.125490196078431,0.231372549019608,0,0.274509803921569,0.258823529411765,0.235294117647059,0,0,0.156862745098039,0,0.172549019607843,0.23921568627451,0.164705882352941,0.172549019607843,0.517647058823529,0.133333333333333,0.525490196078431,0.113725490196078,0.231372549019608,0.270588235294118,0.47843137254902,0.192156862745098,0,0.0588235294117647,0.882352941176471,0.203921568627451,0.117647058823529,0.0352941176470588,0,0.113725490196078,0.207843137254902,0.831372549019608,0.482352941176471,0.498039215686275,0.168627450980392,0.0705882352941176,0,0.231372549019608,0.23921568627451,0.16078431372549,0.129411764705882,0,0.125490196078431,0,0.188235294117647,0.254901960784314,0.0509803921568627,0.133333333333333,0.172549019607843,0.180392156862745,0.0470588235294118,0,0.23921568627451,0.192156862745098,0.345098039215686,0.223529411764706,0.235294117647059,0.745098039215686,0.525490196078431,0,0.52156862745098,0.180392156862745,0.243137254901961,0.509803921568627,0.180392156862745,0.215686274509804,0.117647058823529,0.16078431372549,0.658823529411765,0.168627450980392,0.156862745098039,0.207843137254902,0,0.0705882352941176,0.0941176470588235],\"mode\":\"markers\",\"type\":\"scatter3d\",\"marker\":{\"color\":[\"rgba(77,39,47,1)\",\"rgba(0,129,115,1)\",\"rgba(34,12,16,1)\",\"rgba(219,64,66,1)\",\"rgba(64,46,46,1)\",\"rgba(106,32,35,1)\",\"rgba(52,34,50,1)\",\"rgba(87,137,0,1)\",\"rgba(49,129,28,1)\",\"rgba(107,129,63,1)\",\"rgba(0,146,186,1)\",\"rgba(75,56,66,1)\",\"rgba(159,52,61,1)\",\"rgba(0,173,123,1)\",\"rgba(82,170,35,1)\",\"rgba(81,51,61,1)\",\"rgba(11,128,58,1)\",\"rgba(64,93,68,1)\",\"rgba(84,155,194,1)\",\"rgba(112,146,0,1)\",\"rgba(85,152,100,1)\",\"rgba(103,32,44,1)\",\"rgba(110,118,0,1)\",\"rgba(0,126,58,1)\",\"rgba(56,163,55,1)\",\"rgba(103,106,32,1)\",\"rgba(85,66,59,1)\",\"rgba(133,196,0,1)\",\"rgba(66,95,70,1)\",\"rgba(0,157,66,1)\",\"rgba(55,131,60,1)\",\"rgba(49,172,0,1)\",\"rgba(83,144,0,1)\",\"rgba(196,54,40,1)\",\"rgba(133,143,0,1)\",\"rgba(155,31,44,1)\",\"rgba(0,156,61,1)\",\"rgba(185,32,42,1)\",\"rgba(80,81,44,1)\",\"rgba(138,136,132,1)\",\"rgba(130,33,34,1)\",\"rgba(50,157,134,1)\",\"rgba(111,102,29,1)\",\"rgba(0,141,59,1)\",\"rgba(0,110,69,1)\",\"rgba(0,110,122,1)\",\"rgba(134,26,49,1)\",\"rgba(51,144,0,1)\",\"rgba(66,122,15,1)\",\"rgba(187,196,225,1)\",\"rgba(41,162,52,1)\",\"rgba(82,120,30,1)\",\"rgba(4,149,9,1)\",\"rgba(135,125,0,1)\",\"rgba(120,19,29,1)\",\"rgba(80,155,53,1)\",\"rgba(0,132,212,1)\",\"rgba(0,178,123,1)\",\"rgba(0,105,127,1)\",\"rgba(125,39,43,1)\",\"rgba(68,162,18,1)\",\"rgba(186,158,0,1)\",\"rgba(110,48,59,1)\",\"rgba(50,94,61,1)\",\"rgba(40,137,41,1)\",\"rgba(120,30,33,1)\",\"rgba(99,139,0,1)\",\"rgba(26,27,32,1)\",\"rgba(228,124,0,1)\",\"rgba(131,35,48,1)\",\"rgba(131,127,65,1)\",\"rgba(84,110,13,1)\",\"rgba(97,31,34,1)\",\"rgba(48,130,44,1)\",\"rgba(102,39,46,1)\",\"rgba(122,119,12,1)\",\"rgba(110,118,0,1)\",\"rgba(59,110,61,1)\",\"rgba(151,41,49,1)\",\"rgba(0,150,88,1)\",\"rgba(87,59,57,1)\",\"rgba(29,150,60,1)\",\"rgba(0,220,190,1)\",\"rgba(74,87,134,1)\",\"rgba(139,142,0,1)\",\"rgba(0,106,133,1)\",\"rgba(0,133,46,1)\",\"rgba(78,128,62,1)\",\"rgba(0,108,130,1)\",\"rgba(0,151,46,1)\",\"rgba(0,144,55,1)\",\"rgba(76,113,30,1)\",\"rgba(83,28,41,1)\",\"rgba(0,114,168,1)\",\"rgba(59,38,43,1)\",\"rgba(76,150,40,1)\",\"rgba(105,138,53,1)\",\"rgba(101,131,0,1)\",\"rgba(185,102,18,1)\",\"rgba(159,89,24,1)\"],\"line\":{\"color\":[\"rgba(77,39,47,1)\",\"rgba(0,129,115,1)\",\"rgba(34,12,16,1)\",\"rgba(219,64,66,1)\",\"rgba(64,46,46,1)\",\"rgba(106,32,35,1)\",\"rgba(52,34,50,1)\",\"rgba(87,137,0,1)\",\"rgba(49,129,28,1)\",\"rgba(107,129,63,1)\",\"rgba(0,146,186,1)\",\"rgba(75,56,66,1)\",\"rgba(159,52,61,1)\",\"rgba(0,173,123,1)\",\"rgba(82,170,35,1)\",\"rgba(81,51,61,1)\",\"rgba(11,128,58,1)\",\"rgba(64,93,68,1)\",\"rgba(84,155,194,1)\",\"rgba(112,146,0,1)\",\"rgba(85,152,100,1)\",\"rgba(103,32,44,1)\",\"rgba(110,118,0,1)\",\"rgba(0,126,58,1)\",\"rgba(56,163,55,1)\",\"rgba(103,106,32,1)\",\"rgba(85,66,59,1)\",\"rgba(133,196,0,1)\",\"rgba(66,95,70,1)\",\"rgba(0,157,66,1)\",\"rgba(55,131,60,1)\",\"rgba(49,172,0,1)\",\"rgba(83,144,0,1)\",\"rgba(196,54,40,1)\",\"rgba(133,143,0,1)\",\"rgba(155,31,44,1)\",\"rgba(0,156,61,1)\",\"rgba(185,32,42,1)\",\"rgba(80,81,44,1)\",\"rgba(138,136,132,1)\",\"rgba(130,33,34,1)\",\"rgba(50,157,134,1)\",\"rgba(111,102,29,1)\",\"rgba(0,141,59,1)\",\"rgba(0,110,69,1)\",\"rgba(0,110,122,1)\",\"rgba(134,26,49,1)\",\"rgba(51,144,0,1)\",\"rgba(66,122,15,1)\",\"rgba(187,196,225,1)\",\"rgba(41,162,52,1)\",\"rgba(82,120,30,1)\",\"rgba(4,149,9,1)\",\"rgba(135,125,0,1)\",\"rgba(120,19,29,1)\",\"rgba(80,155,53,1)\",\"rgba(0,132,212,1)\",\"rgba(0,178,123,1)\",\"rgba(0,105,127,1)\",\"rgba(125,39,43,1)\",\"rgba(68,162,18,1)\",\"rgba(186,158,0,1)\",\"rgba(110,48,59,1)\",\"rgba(50,94,61,1)\",\"rgba(40,137,41,1)\",\"rgba(120,30,33,1)\",\"rgba(99,139,0,1)\",\"rgba(26,27,32,1)\",\"rgba(228,124,0,1)\",\"rgba(131,35,48,1)\",\"rgba(131,127,65,1)\",\"rgba(84,110,13,1)\",\"rgba(97,31,34,1)\",\"rgba(48,130,44,1)\",\"rgba(102,39,46,1)\",\"rgba(122,119,12,1)\",\"rgba(110,118,0,1)\",\"rgba(59,110,61,1)\",\"rgba(151,41,49,1)\",\"rgba(0,150,88,1)\",\"rgba(87,59,57,1)\",\"rgba(29,150,60,1)\",\"rgba(0,220,190,1)\",\"rgba(74,87,134,1)\",\"rgba(139,142,0,1)\",\"rgba(0,106,133,1)\",\"rgba(0,133,46,1)\",\"rgba(78,128,62,1)\",\"rgba(0,108,130,1)\",\"rgba(0,151,46,1)\",\"rgba(0,144,55,1)\",\"rgba(76,113,30,1)\",\"rgba(83,28,41,1)\",\"rgba(0,114,168,1)\",\"rgba(59,38,43,1)\",\"rgba(76,150,40,1)\",\"rgba(105,138,53,1)\",\"rgba(101,131,0,1)\",\"rgba(185,102,18,1)\",\"rgba(159,89,24,1)\"]}},\"textfont\":{\"color\":[\"rgba(77,39,47,1)\",\"rgba(0,129,115,1)\",\"rgba(34,12,16,1)\",\"rgba(219,64,66,1)\",\"rgba(64,46,46,1)\",\"rgba(106,32,35,1)\",\"rgba(52,34,50,1)\",\"rgba(87,137,0,1)\",\"rgba(49,129,28,1)\",\"rgba(107,129,63,1)\",\"rgba(0,146,186,1)\",\"rgba(75,56,66,1)\",\"rgba(159,52,61,1)\",\"rgba(0,173,123,1)\",\"rgba(82,170,35,1)\",\"rgba(81,51,61,1)\",\"rgba(11,128,58,1)\",\"rgba(64,93,68,1)\",\"rgba(84,155,194,1)\",\"rgba(112,146,0,1)\",\"rgba(85,152,100,1)\",\"rgba(103,32,44,1)\",\"rgba(110,118,0,1)\",\"rgba(0,126,58,1)\",\"rgba(56,163,55,1)\",\"rgba(103,106,32,1)\",\"rgba(85,66,59,1)\",\"rgba(133,196,0,1)\",\"rgba(66,95,70,1)\",\"rgba(0,157,66,1)\",\"rgba(55,131,60,1)\",\"rgba(49,172,0,1)\",\"rgba(83,144,0,1)\",\"rgba(196,54,40,1)\",\"rgba(133,143,0,1)\",\"rgba(155,31,44,1)\",\"rgba(0,156,61,1)\",\"rgba(185,32,42,1)\",\"rgba(80,81,44,1)\",\"rgba(138,136,132,1)\",\"rgba(130,33,34,1)\",\"rgba(50,157,134,1)\",\"rgba(111,102,29,1)\",\"rgba(0,141,59,1)\",\"rgba(0,110,69,1)\",\"rgba(0,110,122,1)\",\"rgba(134,26,49,1)\",\"rgba(51,144,0,1)\",\"rgba(66,122,15,1)\",\"rgba(187,196,225,1)\",\"rgba(41,162,52,1)\",\"rgba(82,120,30,1)\",\"rgba(4,149,9,1)\",\"rgba(135,125,0,1)\",\"rgba(120,19,29,1)\",\"rgba(80,155,53,1)\",\"rgba(0,132,212,1)\",\"rgba(0,178,123,1)\",\"rgba(0,105,127,1)\",\"rgba(125,39,43,1)\",\"rgba(68,162,18,1)\",\"rgba(186,158,0,1)\",\"rgba(110,48,59,1)\",\"rgba(50,94,61,1)\",\"rgba(40,137,41,1)\",\"rgba(120,30,33,1)\",\"rgba(99,139,0,1)\",\"rgba(26,27,32,1)\",\"rgba(228,124,0,1)\",\"rgba(131,35,48,1)\",\"rgba(131,127,65,1)\",\"rgba(84,110,13,1)\",\"rgba(97,31,34,1)\",\"rgba(48,130,44,1)\",\"rgba(102,39,46,1)\",\"rgba(122,119,12,1)\",\"rgba(110,118,0,1)\",\"rgba(59,110,61,1)\",\"rgba(151,41,49,1)\",\"rgba(0,150,88,1)\",\"rgba(87,59,57,1)\",\"rgba(29,150,60,1)\",\"rgba(0,220,190,1)\",\"rgba(74,87,134,1)\",\"rgba(139,142,0,1)\",\"rgba(0,106,133,1)\",\"rgba(0,133,46,1)\",\"rgba(78,128,62,1)\",\"rgba(0,108,130,1)\",\"rgba(0,151,46,1)\",\"rgba(0,144,55,1)\",\"rgba(76,113,30,1)\",\"rgba(83,28,41,1)\",\"rgba(0,114,168,1)\",\"rgba(59,38,43,1)\",\"rgba(76,150,40,1)\",\"rgba(105,138,53,1)\",\"rgba(101,131,0,1)\",\"rgba(185,102,18,1)\",\"rgba(159,89,24,1)\"]},\"line\":{\"color\":[\"rgba(77,39,47,1)\",\"rgba(0,129,115,1)\",\"rgba(34,12,16,1)\",\"rgba(219,64,66,1)\",\"rgba(64,46,46,1)\",\"rgba(106,32,35,1)\",\"rgba(52,34,50,1)\",\"rgba(87,137,0,1)\",\"rgba(49,129,28,1)\",\"rgba(107,129,63,1)\",\"rgba(0,146,186,1)\",\"rgba(75,56,66,1)\",\"rgba(159,52,61,1)\",\"rgba(0,173,123,1)\",\"rgba(82,170,35,1)\",\"rgba(81,51,61,1)\",\"rgba(11,128,58,1)\",\"rgba(64,93,68,1)\",\"rgba(84,155,194,1)\",\"rgba(112,146,0,1)\",\"rgba(85,152,100,1)\",\"rgba(103,32,44,1)\",\"rgba(110,118,0,1)\",\"rgba(0,126,58,1)\",\"rgba(56,163,55,1)\",\"rgba(103,106,32,1)\",\"rgba(85,66,59,1)\",\"rgba(133,196,0,1)\",\"rgba(66,95,70,1)\",\"rgba(0,157,66,1)\",\"rgba(55,131,60,1)\",\"rgba(49,172,0,1)\",\"rgba(83,144,0,1)\",\"rgba(196,54,40,1)\",\"rgba(133,143,0,1)\",\"rgba(155,31,44,1)\",\"rgba(0,156,61,1)\",\"rgba(185,32,42,1)\",\"rgba(80,81,44,1)\",\"rgba(138,136,132,1)\",\"rgba(130,33,34,1)\",\"rgba(50,157,134,1)\",\"rgba(111,102,29,1)\",\"rgba(0,141,59,1)\",\"rgba(0,110,69,1)\",\"rgba(0,110,122,1)\",\"rgba(134,26,49,1)\",\"rgba(51,144,0,1)\",\"rgba(66,122,15,1)\",\"rgba(187,196,225,1)\",\"rgba(41,162,52,1)\",\"rgba(82,120,30,1)\",\"rgba(4,149,9,1)\",\"rgba(135,125,0,1)\",\"rgba(120,19,29,1)\",\"rgba(80,155,53,1)\",\"rgba(0,132,212,1)\",\"rgba(0,178,123,1)\",\"rgba(0,105,127,1)\",\"rgba(125,39,43,1)\",\"rgba(68,162,18,1)\",\"rgba(186,158,0,1)\",\"rgba(110,48,59,1)\",\"rgba(50,94,61,1)\",\"rgba(40,137,41,1)\",\"rgba(120,30,33,1)\",\"rgba(99,139,0,1)\",\"rgba(26,27,32,1)\",\"rgba(228,124,0,1)\",\"rgba(131,35,48,1)\",\"rgba(131,127,65,1)\",\"rgba(84,110,13,1)\",\"rgba(97,31,34,1)\",\"rgba(48,130,44,1)\",\"rgba(102,39,46,1)\",\"rgba(122,119,12,1)\",\"rgba(110,118,0,1)\",\"rgba(59,110,61,1)\",\"rgba(151,41,49,1)\",\"rgba(0,150,88,1)\",\"rgba(87,59,57,1)\",\"rgba(29,150,60,1)\",\"rgba(0,220,190,1)\",\"rgba(74,87,134,1)\",\"rgba(139,142,0,1)\",\"rgba(0,106,133,1)\",\"rgba(0,133,46,1)\",\"rgba(78,128,62,1)\",\"rgba(0,108,130,1)\",\"rgba(0,151,46,1)\",\"rgba(0,144,55,1)\",\"rgba(76,113,30,1)\",\"rgba(83,28,41,1)\",\"rgba(0,114,168,1)\",\"rgba(59,38,43,1)\",\"rgba(76,150,40,1)\",\"rgba(105,138,53,1)\",\"rgba(101,131,0,1)\",\"rgba(185,102,18,1)\",\"rgba(159,89,24,1)\"]},\"frame\":null}],\"highlight\":{\"on\":\"plotly_click\",\"persistent\":false,\"dynamic\":false,\"selectize\":false,\"opacityDim\":0.2,\"selected\":{\"opacity\":1},\"debounce\":0},\"shinyEvents\":[\"plotly_hover\",\"plotly_click\",\"plotly_selected\",\"plotly_relayout\",\"plotly_brushed\",\"plotly_brushing\",\"plotly_clickannotation\",\"plotly_doubleclick\",\"plotly_deselect\",\"plotly_afterplot\",\"plotly_sunburstclick\"],\"base_url\":\"https://plot.ly\"},\"evals\":[],\"jsHooks\":[]} If you play around with this enough, you‚Äôll realize that plotting all of your 3D data on a plot as individual points is kind of cumbersome when you have thousands of points; you can‚Äôt really tell which regions of your color space are more or less dense. It may better suit your purposes to cluster the data a bit first, and then plot the clusters:\nclusters \u0026lt;- extractClusters(getKMeanColors(img, color.space = \u0026quot;Lab\u0026quot;, ref.white = \u0026quot;D65\u0026quot;, n = 50, plotting = F)) colnames(clusters) \u0026lt;- c(\u0026quot;L\u0026quot;, \u0026quot;a\u0026quot;, \u0026quot;b\u0026quot;, \u0026quot;Pct\u0026quot;) # We can do this with a colordistance function... scatter3dclusters(clusters, color.space = \u0026quot;lab\u0026quot;, scaling = 100)  # we can also use scatter3D, with a bit of a hack to get different point sizes col_vector \u0026lt;- rgb(convertColor(clusters[ , 1:3], from = \u0026quot;Lab\u0026quot;, to = \u0026quot;sRGB\u0026quot;)) # make blank plot scatter3D(clusters$L, clusters$a, clusters$b, cex = 0, colkey = F, phi = 35, theta = 60, xlab = \u0026quot;L\u0026quot;, ylab = \u0026quot;a\u0026quot;, zlab = \u0026quot;b\u0026quot;) # set scale multiplier for point sizes scale \u0026lt;- 80 # add one point at a time, setting size with the cex argument for (i in 1:nrow(clusters)) { scatter3D(x = clusters$L[i], y = clusters$a[i], z = clusters$b[i], cex = clusters$Pct[i] * scale, pch = 19, alpha = 0.5, col = col_vector[i], add = TRUE) }  # or, we can just use plotly again plot_ly(data = clusters, x = ~L, y = ~a, z = ~b, type = \u0026quot;scatter3d\u0026quot;, mode = \u0026quot;markers\u0026quot;, color = I(col_vector), # this is a bit of a hack and you'll get a warning... colors = col_vector, size = ~Pct)   {\"x\":{\"visdat\":{\"948a2bab3c7f\":[\"function () \",\"plotlyVisDat\"]},\"cur_data\":\"948a2bab3c7f\",\"attrs\":{\"948a2bab3c7f\":{\"x\":{},\"y\":{},\"z\":{},\"mode\":\"markers\",\"color\":[\"#647933\",\"#732429\",\"#986516\",\"#832128\",\"#232023\",\"#138233\",\"#943E1F\",\"#17AEAB\",\"#0C8864\",\"#499535\",\"#384D39\",\"#573F3B\",\"#9F8008\",\"#047579\",\"#077C6D\",\"#108DB0\",\"#BA2E2C\",\"#06667C\",\"#0DD7B4\",\"#86B20D\",\"#8B8784\",\"#5F780B\",\"#766D2A\",\"#C06012\",\"#1860A9\",\"#62512B\",\"#B1A206\",\"#545D2C\",\"#37CA7A\",\"#22587E\",\"#3A9610\",\"#55BC26\",\"#66242C\",\"#473037\",\"#C34622\",\"#038E51\",\"#1EA43A\",\"#C17C08\",\"#4B8407\",\"#10753B\",\"#758F07\",\"#15AA7C\",\"#265F37\",\"#7E1C32\",\"#119846\",\"#9C212B\",\"#898609\",\"#582531\",\"#5E9706\",\"#3F7624\"],\"size\":{},\"colors\":[\"#647933\",\"#732429\",\"#986516\",\"#832128\",\"#232023\",\"#138233\",\"#943E1F\",\"#17AEAB\",\"#0C8864\",\"#499535\",\"#384D39\",\"#573F3B\",\"#9F8008\",\"#047579\",\"#077C6D\",\"#108DB0\",\"#BA2E2C\",\"#06667C\",\"#0DD7B4\",\"#86B20D\",\"#8B8784\",\"#5F780B\",\"#766D2A\",\"#C06012\",\"#1860A9\",\"#62512B\",\"#B1A206\",\"#545D2C\",\"#37CA7A\",\"#22587E\",\"#3A9610\",\"#55BC26\",\"#66242C\",\"#473037\",\"#C34622\",\"#038E51\",\"#1EA43A\",\"#C17C08\",\"#4B8407\",\"#10753B\",\"#758F07\",\"#15AA7C\",\"#265F37\",\"#7E1C32\",\"#119846\",\"#9C212B\",\"#898609\",\"#582531\",\"#5E9706\",\"#3F7624\"],\"alpha_stroke\":1,\"sizes\":[10,100],\"spans\":[1,20],\"type\":\"scatter3d\"}},\"layout\":{\"margin\":{\"b\":40,\"l\":60,\"t\":25,\"r\":10},\"scene\":{\"xaxis\":{\"title\":\"L\"},\"yaxis\":{\"title\":\"a\"},\"zaxis\":{\"title\":\"b\"}},\"hovermode\":\"closest\",\"showlegend\":false},\"source\":\"A\",\"config\":{\"showSendToCloud\":false},\"data\":[{\"x\":[47.6470370380395,26.9343668030093,47.1828825841296,29.6773078450699,12.7045820506114,47.424072153912,37.7095435680787,64.3966358719047,50.2170710701797,55.2008575013163,30.5639393862333,29.2352057210328,54.8739847444633,44.21651687503,46.4348311991777,54.3461739170174,42.2864967235475,39.4002815788575,77.246141494701,67.3417368584501,56.4821125357197,46.7293021620499,45.4986153446182,51.5290761538363,40.1338361320406,35.3775725076101,65.7951807953912,37.6049924843794,72.5005589738049,35.5332956130365,54.909869921715,68.0826258130497,24.7407170414244,22.5787090735333,47.2231676738661,51.6978881209399,58.9315019517576,57.9255190148564,49.6384118056201,42.7699221303017,55.5967406937468,61.8862757522115,35.7827664244431,28.3085165518734,55.1722752062099,34.8245048991748,54.4721552035387,22.5158972154844,56.5773369142028,44.269889046976],\"y\":[-19.1560055874448,35.1558839259183,14.2720406280373,41.9759780318741,2.27151473468494,-46.9672270333645,34.3138626384862,-36.1794957895637,-40.2015036246707,-42.360763899665,-13.1006131883921,9.63830700679229,2.01930745644207,-25.3929355968667,-32.9400403387384,-18.7144718129058,55.1505786880039,-16.2512192026847,-52.1150378079515,-35.0058101398964,1.02962325971967,-23.1977287309345,-5.20162909915775,34.3707882925485,6.26556465863796,1.71232485126426,-8.39391101221517,-11.537025012239,-56.2902446947673,-4.37777177199427,-49.6422187683071,-55.1756294819106,30.2241914435395,11.7955865367038,48.1326705218909,-47.3050354036041,-56.1539990811367,19.1817983458343,-37.485801469206,-41.0176083687218,-25.7079300352572,-47.2926881095034,-28.8228745220873,42.6000075381491,-52.123231334113,50.0622436786647,-11.744290691161,24.877420489759,-38.5316644028869,-33.7940092938693],\"z\":[35.0345104570875,15.8418904092597,48.9346846157969,20.9249840908074,-1.78570914036779,33.9657948063168,36.0165415746418,-9.12817187084792,10.7727716382048,42.0852089437363,9.12095630563548,6.53578709802711,58.7122851209248,-10.4896224844523,0.31534768082993,-27.1244338341843,35.7715089466191,-19.564988924393,5.09492563363152,66.8339063421687,1.77215759159408,49.418203672323,37.6199540518151,55.9747127260182,-45.4109435932669,24.7964325796627,67.9103022988535,26.7723537600718,28.8802186644586,-26.5501909597621,54.4569417630617,61.1975442753346,10.9134354860685,-0.242929238594548,46.7112900443923,23.6139098568036,43.9314187088593,62.7749914938315,51.9201408757226,24.0782770255091,57.8980119441862,13.286134167394,17.2635276892329,12.5309090043427,33.8323009989432,26.1273787908098,57.374791997851,4.05616250233049,58.1589172091652,37.9475390129431],\"mode\":\"markers\",\"type\":\"scatter3d\",\"marker\":{\"color\":[\"rgba(100,121,51,1)\",\"rgba(115,36,41,1)\",\"rgba(152,101,22,1)\",\"rgba(131,33,40,1)\",\"rgba(35,32,35,1)\",\"rgba(19,130,51,1)\",\"rgba(148,62,31,1)\",\"rgba(23,174,171,1)\",\"rgba(12,136,100,1)\",\"rgba(73,149,53,1)\",\"rgba(56,77,57,1)\",\"rgba(87,63,59,1)\",\"rgba(159,128,8,1)\",\"rgba(4,117,121,1)\",\"rgba(7,124,109,1)\",\"rgba(16,141,176,1)\",\"rgba(186,46,44,1)\",\"rgba(6,102,124,1)\",\"rgba(13,215,180,1)\",\"rgba(134,178,13,1)\",\"rgba(139,135,132,1)\",\"rgba(95,120,11,1)\",\"rgba(118,109,42,1)\",\"rgba(192,96,18,1)\",\"rgba(24,96,169,1)\",\"rgba(98,81,43,1)\",\"rgba(177,162,6,1)\",\"rgba(84,93,44,1)\",\"rgba(55,202,122,1)\",\"rgba(34,88,126,1)\",\"rgba(58,150,16,1)\",\"rgba(85,188,38,1)\",\"rgba(102,36,44,1)\",\"rgba(71,48,55,1)\",\"rgba(195,70,34,1)\",\"rgba(3,142,81,1)\",\"rgba(30,164,58,1)\",\"rgba(193,124,8,1)\",\"rgba(75,132,7,1)\",\"rgba(16,117,59,1)\",\"rgba(117,143,7,1)\",\"rgba(21,170,124,1)\",\"rgba(38,95,55,1)\",\"rgba(126,28,50,1)\",\"rgba(17,152,70,1)\",\"rgba(156,33,43,1)\",\"rgba(137,134,9,1)\",\"rgba(88,37,49,1)\",\"rgba(94,151,6,1)\",\"rgba(63,118,36,1)\"],\"size\":[55.8780487804878,100,19.8780487804878,68.609756097561,46.8780487804878,56.0975609756098,24.2682926829268,18.5609756097561,57.1951219512195,52.1463414634146,31.5121951219512,50.390243902439,39.6341463414634,60.2682926829268,56.7560975609756,10.6585365853659,48.4146341463415,61.3658536585366,13.5121951219512,21.4146341463415,17.2439024390244,54.5609756097561,50.1707317073171,34.8048780487805,10,33.2682926829268,18.1219512195122,53.2439024390244,11.7560975609756,28.4390243902439,74.3170731707317,24.4878048780488,83.3170731707317,67.9512195121951,42.4878048780488,70.3658536585366,56.5365853658537,26.9024390243902,74.9756097560976,40.9512195121951,59.609756097561,19.4390243902439,29.0975609756098,32.390243902439,75.4146341463415,60.7073170731707,51.0487804878049,49.7317073170732,55.219512195122,52.5853658536585],\"sizemode\":\"area\",\"line\":{\"color\":[\"rgba(100,121,51,1)\",\"rgba(115,36,41,1)\",\"rgba(152,101,22,1)\",\"rgba(131,33,40,1)\",\"rgba(35,32,35,1)\",\"rgba(19,130,51,1)\",\"rgba(148,62,31,1)\",\"rgba(23,174,171,1)\",\"rgba(12,136,100,1)\",\"rgba(73,149,53,1)\",\"rgba(56,77,57,1)\",\"rgba(87,63,59,1)\",\"rgba(159,128,8,1)\",\"rgba(4,117,121,1)\",\"rgba(7,124,109,1)\",\"rgba(16,141,176,1)\",\"rgba(186,46,44,1)\",\"rgba(6,102,124,1)\",\"rgba(13,215,180,1)\",\"rgba(134,178,13,1)\",\"rgba(139,135,132,1)\",\"rgba(95,120,11,1)\",\"rgba(118,109,42,1)\",\"rgba(192,96,18,1)\",\"rgba(24,96,169,1)\",\"rgba(98,81,43,1)\",\"rgba(177,162,6,1)\",\"rgba(84,93,44,1)\",\"rgba(55,202,122,1)\",\"rgba(34,88,126,1)\",\"rgba(58,150,16,1)\",\"rgba(85,188,38,1)\",\"rgba(102,36,44,1)\",\"rgba(71,48,55,1)\",\"rgba(195,70,34,1)\",\"rgba(3,142,81,1)\",\"rgba(30,164,58,1)\",\"rgba(193,124,8,1)\",\"rgba(75,132,7,1)\",\"rgba(16,117,59,1)\",\"rgba(117,143,7,1)\",\"rgba(21,170,124,1)\",\"rgba(38,95,55,1)\",\"rgba(126,28,50,1)\",\"rgba(17,152,70,1)\",\"rgba(156,33,43,1)\",\"rgba(137,134,9,1)\",\"rgba(88,37,49,1)\",\"rgba(94,151,6,1)\",\"rgba(63,118,36,1)\"]}},\"textfont\":{\"color\":[\"rgba(100,121,51,1)\",\"rgba(115,36,41,1)\",\"rgba(152,101,22,1)\",\"rgba(131,33,40,1)\",\"rgba(35,32,35,1)\",\"rgba(19,130,51,1)\",\"rgba(148,62,31,1)\",\"rgba(23,174,171,1)\",\"rgba(12,136,100,1)\",\"rgba(73,149,53,1)\",\"rgba(56,77,57,1)\",\"rgba(87,63,59,1)\",\"rgba(159,128,8,1)\",\"rgba(4,117,121,1)\",\"rgba(7,124,109,1)\",\"rgba(16,141,176,1)\",\"rgba(186,46,44,1)\",\"rgba(6,102,124,1)\",\"rgba(13,215,180,1)\",\"rgba(134,178,13,1)\",\"rgba(139,135,132,1)\",\"rgba(95,120,11,1)\",\"rgba(118,109,42,1)\",\"rgba(192,96,18,1)\",\"rgba(24,96,169,1)\",\"rgba(98,81,43,1)\",\"rgba(177,162,6,1)\",\"rgba(84,93,44,1)\",\"rgba(55,202,122,1)\",\"rgba(34,88,126,1)\",\"rgba(58,150,16,1)\",\"rgba(85,188,38,1)\",\"rgba(102,36,44,1)\",\"rgba(71,48,55,1)\",\"rgba(195,70,34,1)\",\"rgba(3,142,81,1)\",\"rgba(30,164,58,1)\",\"rgba(193,124,8,1)\",\"rgba(75,132,7,1)\",\"rgba(16,117,59,1)\",\"rgba(117,143,7,1)\",\"rgba(21,170,124,1)\",\"rgba(38,95,55,1)\",\"rgba(126,28,50,1)\",\"rgba(17,152,70,1)\",\"rgba(156,33,43,1)\",\"rgba(137,134,9,1)\",\"rgba(88,37,49,1)\",\"rgba(94,151,6,1)\",\"rgba(63,118,36,1)\"],\"size\":[55.8780487804878,100,19.8780487804878,68.609756097561,46.8780487804878,56.0975609756098,24.2682926829268,18.5609756097561,57.1951219512195,52.1463414634146,31.5121951219512,50.390243902439,39.6341463414634,60.2682926829268,56.7560975609756,10.6585365853659,48.4146341463415,61.3658536585366,13.5121951219512,21.4146341463415,17.2439024390244,54.5609756097561,50.1707317073171,34.8048780487805,10,33.2682926829268,18.1219512195122,53.2439024390244,11.7560975609756,28.4390243902439,74.3170731707317,24.4878048780488,83.3170731707317,67.9512195121951,42.4878048780488,70.3658536585366,56.5365853658537,26.9024390243902,74.9756097560976,40.9512195121951,59.609756097561,19.4390243902439,29.0975609756098,32.390243902439,75.4146341463415,60.7073170731707,51.0487804878049,49.7317073170732,55.219512195122,52.5853658536585]},\"error_y\":{\"width\":[]},\"error_x\":{\"width\":[]},\"line\":{\"color\":[\"rgba(100,121,51,1)\",\"rgba(115,36,41,1)\",\"rgba(152,101,22,1)\",\"rgba(131,33,40,1)\",\"rgba(35,32,35,1)\",\"rgba(19,130,51,1)\",\"rgba(148,62,31,1)\",\"rgba(23,174,171,1)\",\"rgba(12,136,100,1)\",\"rgba(73,149,53,1)\",\"rgba(56,77,57,1)\",\"rgba(87,63,59,1)\",\"rgba(159,128,8,1)\",\"rgba(4,117,121,1)\",\"rgba(7,124,109,1)\",\"rgba(16,141,176,1)\",\"rgba(186,46,44,1)\",\"rgba(6,102,124,1)\",\"rgba(13,215,180,1)\",\"rgba(134,178,13,1)\",\"rgba(139,135,132,1)\",\"rgba(95,120,11,1)\",\"rgba(118,109,42,1)\",\"rgba(192,96,18,1)\",\"rgba(24,96,169,1)\",\"rgba(98,81,43,1)\",\"rgba(177,162,6,1)\",\"rgba(84,93,44,1)\",\"rgba(55,202,122,1)\",\"rgba(34,88,126,1)\",\"rgba(58,150,16,1)\",\"rgba(85,188,38,1)\",\"rgba(102,36,44,1)\",\"rgba(71,48,55,1)\",\"rgba(195,70,34,1)\",\"rgba(3,142,81,1)\",\"rgba(30,164,58,1)\",\"rgba(193,124,8,1)\",\"rgba(75,132,7,1)\",\"rgba(16,117,59,1)\",\"rgba(117,143,7,1)\",\"rgba(21,170,124,1)\",\"rgba(38,95,55,1)\",\"rgba(126,28,50,1)\",\"rgba(17,152,70,1)\",\"rgba(156,33,43,1)\",\"rgba(137,134,9,1)\",\"rgba(88,37,49,1)\",\"rgba(94,151,6,1)\",\"rgba(63,118,36,1)\"]},\"frame\":null}],\"highlight\":{\"on\":\"plotly_click\",\"persistent\":false,\"dynamic\":false,\"selectize\":false,\"opacityDim\":0.2,\"selected\":{\"opacity\":1},\"debounce\":0},\"shinyEvents\":[\"plotly_hover\",\"plotly_click\",\"plotly_selected\",\"plotly_relayout\",\"plotly_brushed\",\"plotly_brushing\",\"plotly_clickannotation\",\"plotly_doubleclick\",\"plotly_deselect\",\"plotly_afterplot\",\"plotly_sunburstclick\"],\"base_url\":\"https://plot.ly\"},\"evals\":[],\"jsHooks\":[]} ","date":1618272000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1618329969,"objectID":"5354f59d6f9387f598b2da8cfa7e773f","permalink":"/post/modifying-pixel-plots/","publishdate":"2021-04-13T00:00:00Z","relpermalink":"/post/modifying-pixel-plots/","section":"post","summary":"How to make prettier, more flexible plots of pixels in color space.","tags":["colordistance","color","r packages","plotting"],"title":"Modifying pixel plots","type":"post"},{"authors":null,"categories":null,"content":"What I\u0026rsquo;m working on these days Pivoting to a postdoc! I defended my PhD dissertation in April 2023, and am taking a slow summer to move, reset, and embark on my postdoctoral research. In September 2023 I am joining the Integrative Evolutionary Biology group led by Claudius Kratochwil at the University of Helsinki, Finland. Here I will study the evolution and development of color patterns in fishes, and focus especially on the interplay robustness (the persistence of a phenotype under perturbation) with evolutionary changes. So here\u0026rsquo;s an important Finnish sentence for you: t√§m√§ kala on jotenkin outo!\nLab webpage here.\nThe evolution of mouthbrooding in fishes Right now I\u0026rsquo;m studying the evolution of mouthbrooding (a kind of parental care where parents incubate offspring in their mouths), and how much feeding phenotypes influence that evolution. I think that mouthbrooding is a beautiful example of an extraordinary behavior (using a mouth as a nursery!) that arises from co-opting existing traits. Specifically, it seems like mouthbrooding is more likely to evolve in species whose feeding adaptations already make them good at mouthbrooding. And the further I dive into the whys and hows, the more I appreciate how much this behavior seems to be the result of the interplay of morphology, behavior, and environment. I have a somewhat out-there idea for why only some fishes in a given environment will evolve mouthbrooding, and it\u0026rsquo;s what I\u0026rsquo;m testing now.\nMore here.\nColor analysis software I also work a lot on color pattern evolution, and have written a few R packages to make that easier for myself and others. I think that accessible, objective tools for color pattern research are an important part of making sure we\u0026rsquo;re being careful with the conclusions we draw about how much color matters to other living things. Current projects include:\n  recolorize, an R package for color segmentation, which integrates with the pavo and patternize packages as well as providing a variety of other output formats (vectorized images, individual layers, masks). More here.\n  colordistance, an R package for comparing images by quantitative color similarity:\n  More here.\nCollaborations I\u0026rsquo;m lucky to have really delightful spread of collaborations with researchers at other universities, including:\n UV in snakes (University of Michigan, Davis-Rabosky lab) The origin and diversity of color jewel beetles (Louisiana State University, Lord lab) Population structure and color variation in brook trout (Pennsylvania State University) The effect of mouthbrooding on the rate of craniofacial evolution (Clemson, Price lab)  ","date":1616889600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1616889600,"objectID":"9ecfcf41a6071b7ae32bf44d8216e251","permalink":"/currently/","publishdate":"2021-03-28T00:00:00Z","relpermalink":"/currently/","section":"","summary":"What I'm working on now.","tags":null,"title":"","type":"page"},{"authors":["Karly E. Cohen","Hannah I. Weller","Adam P. Summers"],"categories":null,"content":"","date":1604188800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1604188800,"objectID":"5b8fea067671c026fc83adab980fa0ba","permalink":"/publication/cohen-not-2020/","publishdate":"2025-03-04T15:40:54.122034Z","relpermalink":"/publication/cohen-not-2020/","section":"publication","summary":"Teeth tell the tale of interactions between predator and prey. If a dental battery is made up of teeth that look similar, they are morphologically homodont, but if there is an unspecified amount of regional specialization in size or shape, they are morphologically heterodont. These are vague terms with no useful functional implication because morphological homodonty does not necessarily equal functional homodonty. Teeth that look the same may not function the same. Conical teeth are prevalent in fishes, superficially tasked with the simple job of puncture. There is a great deal of variation in the shape and placement of conical teeth. Anterior teeth may be larger than posterior ones, larger teeth may be surrounded by small ones, and patches of teeth may all have the same size and shape. Such variations suggest that conical dentitions might represent a single morphological solution for different functional problems. We are interested in the concept of homodonty and using the conical tooth as a model to differentiate between tooth shape and performance. We consider the stress that a tooth can exert on prey as stress is what causes damage. To create a statistical measure of functional homodonty, stress was calculated from measurements of surface area, position, and applied force. Functional homodonty is then defined as the degree to which teeth along the jaw all bear/exert similar stresses despite changes in shape. We find that morphologically heterodont teeth are often functionally homodont and that position is a better predictor of performance than shape. Furthermore, the arrangement of teeth affects their function, such that there is a functional advantage to having several smaller teeth surrounding a singular large tooth. We demonstrate that this arrangement of teeth is useful to grab, rather than tear, prey upon puncture, with the smaller teeth dissipating large stress forces around the larger tooth. We show that measurements of how shape affects stress distribution in response to loading give us a clearer picture of the evolution of conically shaped teeth.","tags":null,"title":"Not your father‚Äôs homodonty‚Äîstress, tooth shape, and the functional homodont","type":"publication"},{"authors":["Hannah Weller","Sarah E. Hooper","Sybill K. Amelon"],"categories":null,"content":"","date":1601510400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601510400,"objectID":"ca4c2bac1aff8c698a79ae6dba6a8453","permalink":"/publication/weller-countcolors-2020/","publishdate":"2025-03-04T15:40:54.14154Z","relpermalink":"/publication/weller-countcolors-2020/","section":"publication","summary":"Pseudogymnoascus destructans colonizes the wing membrane of hibernating bats with the potential to form dense fungal hyphae aggregates within cupping erosions. These fungal cupping erosions emit a characteristic fluorescent orange-yellow color when the wing membrane is transilluminated with 385 nm ultraviolet (UV) light. The purpose of this study was to create and validate the R package, countcolors, for quantifying the distinct orange‚Äìyellow UV fluorescence in batwing membrane lesions caused by P. destructans. Validation of countcolors was completed by first quantifying the percent area of 20, 2.5 cm2 images. These generated images were of two known pixel colors ranging from 0% to 100% of the pixels. The countcolors package accurately measured the known proportion of a given color in each image. Next, 40, 2.5 cm2 sections of UV transilluminated photographs of little brown bat (Myotis lucifugus) wings were given to a single evaluator. The area of fluorescence was both manually measured and calculated using image analysis software and quantified with countcolors. There was good agreement between the two methods (Pearson‚Äôs correlation¬º0.915); however, the manual use of imaging software showed a consistent negative bias. Reproducibility of the analysis methods was tested by providing the same images to naive evaluators who previously never used the software; no significant difference (P¬º0.099) was found among evaluators. Using the R package countcolors takes less time than does manually measuring the fluorescence in image analysis software, and our results showed that countcolors can improve the accuracy when quantifying the area of P. destructans infection in bat wing-membranes.","tags":null,"title":"Countcolors, an R package for quantification of the fluorescence emitted by Pseudogymnoascus destructans lesions on the wing membranes of hibernating bats","type":"publication"},{"authors":null,"categories":null,"content":"A package for implementing distance metrics to quantify color diversity across images. This is done by binning pixels by color using either data-dependent or automatically generated color bins, quantitatively measuring color similarity among images using one of several distance metrics for comparing pixel color clusters, and clustering images by object color similarity. Uses CIE Lab, RGB, or HSV color spaces. Originally written for use with organism coloration (reef fish color diversity, butterfly mimicry, etc), but easily applicable for any image set.\n","date":1587945600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1587945600,"objectID":"e915fe00a55f8980c91cd319f0a77a5e","permalink":"/project/colordistance/","publishdate":"2020-04-27T00:00:00Z","relpermalink":"/project/colordistance/","section":"project","summary":"An R package for quantitative color comparisons.","tags":["color","r packages","colordistance"],"title":"colordistance","type":"project"},{"authors":null,"categories":null,"content":"This is a package for making color maps, which are needed (or at least useful) for a wide range of color analysis techniques. It was born out of conversations with many biologists who found, to their surprise and mine, that generating color maps was the bottleneck step in their analyses. Fully automated methods rarely work all of the time, and are difficult to modify, while fully manual methods are subjective and time-consuming. This package tries to split the difference by giving you a mix of tools that will do a pretty good job with no user input, and then allow minor manual changes like merging and filtering layers or splitting components, before exporting them to the next step of your analysis. It\u0026rsquo;s also, for the most part, totally deterministic ‚Äì no arbitrary seed-setting for repeatability.\n","date":1587945600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1587945600,"objectID":"9cbda19e8c9aaff8e9b1f81e512f9ab0","permalink":"/project/recolorize/","publishdate":"2020-04-27T00:00:00Z","relpermalink":"/project/recolorize/","section":"project","summary":"An R package for automatic, semi-automatic, and manual color segmentation.","tags":["color","r packages","recolorize"],"title":"recolorize","type":"project"},{"authors":["Hannah I. Weller","Aaron M. Olsen","Ariel L. Camp","Armita R. Manafzadeh","L. Patricia Hernandez","Elizabeth L. Brainerd"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"4784f375aef74219b34f46cae33c2006","permalink":"/publication/weller-xromm-2020/","publishdate":"2025-03-04T15:40:54.096526Z","relpermalink":"/publication/weller-xromm-2020/","section":"publication","summary":"Synopsis Most predatory ray-finned fishes swallow their food whole, which can pose a significant challenge, given that prey items can be half as large as the predators themselves. How do fish transport captured food from the mouth to the stomach? Prior work indicates that, in general, fish use the pharyngeal jaws to manipulate food into the esophagus, where peristalsis is thought to take over. We used X-Ray Reconstruction of Moving Morphology to track prey transport in channel catfish (Ictalurus punctatus). By reconstructing the 3D motions of both the food and the catfish, we were able to track how the catfish move food through the head and into the stomach. Food enters the oral cavity at high velocities as a continuation of suction and stops in the approximate location of the branchial basket before moving in a much slower, more complex path toward the esophagus. This slow phase coincides with little motion in the head and no substantial mouth opening or hyoid depression. Once the prey is in the esophagus, however, its transport is surprisingly tightly correlated with gulping motions (hyoid depression, girdle retraction, hypaxial shortening, and mouth opening) of the head. Although the transport mechanism itself remains unknown, to our knowledge, this is the first description of synchrony between cranial expansion and esophageal transport in a fish. Our results provide direct evidence of prey transport within the esophagus and suggest that peristalsis may not be the sole mechanism of esophageal transport in catfish.","tags":null,"title":"An XROMM Study of Food Transport and Swallowing in Channel Catfish","type":"publication"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Wowchemy Wowchemy | Documentation\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)   Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \n A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/media/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}   Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }   Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Wowchemy's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":["Hannah I. Weller","Mark W. Westneat"],"categories":null,"content":"","date":1548979200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1548979200,"objectID":"fe8c6d0dc4cc8c2ced4a13e1376a5305","permalink":"/publication/weller-quantitative-2019/","publishdate":"2025-03-04T15:40:54.151321Z","relpermalink":"/publication/weller-quantitative-2019/","section":"publication","summary":"Biological color may be adaptive or incidental, seasonal or permanent, species- or population-speciÔ¨Åc, or modiÔ¨Åed for breeding, defense or camouÔ¨Çage. Although color is a hugely informative aspect of biology, quantitative color comparisons are notoriously difÔ¨Åcult. Color comparison is limited by categorization methods, with available tools requiring either subjective classiÔ¨Åcations, or expensive equipment, software, and expertise. We present an R package for processing images of organisms (or other objects) in order to quantify color proÔ¨Åles, gather color trait data, and compare color palettes on the basis of color similarity and amount. The package treats image pixels as 3D coordinates in a ‚Äúcolor space,‚Äù producing a multidimensional color histogram for each image. Pairwise distances between histograms are computed using earth mover‚Äôs distance, a technique borrowed from computer vision, that compares histograms using transportation costs. Users choose a color space, parameters for generating color histograms, and a pairwise comparison method to produce a color distance matrix for a set of images. The package is intended as a more rigorous alternative to subjective, manual digital image analyses, not as a replacement for more advanced techniques that rely on detailed spectrophotometry methods unavailable to many users. Here, we outline the basic functions of colordistance, provide guidelines for the available color spaces and quantiÔ¨Åcation methods, and compare this toolkit with other available methods.","tags":null,"title":"Quantitative color profiling of digital images with earth mover‚Äôs distance using the R package colordistance","type":"publication"},{"authors":["Noraly M. M. E. Van Meer","Hannah I. Weller","Armita R. Manafzadeh","Elska B. Kaczmarek","Bradley Scott","Sander W. S. Gussekloo","Cheryl D. Wilga","Elizabeth L. Brainerd","Ariel L. Camp"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"ca81d085ebd6629d7dc24819f23a4b6f","permalink":"/publication/van-meer-intra-oropharyngeal-2019/","publishdate":"2025-03-04T15:40:54.130711Z","relpermalink":"/publication/van-meer-intra-oropharyngeal-2019/","section":"publication","summary":"Despite the importance of intraoral food transport and swallowing, relatively few studies have examined the biomechanics of these behaviors in non-tetrapods, which lack a muscular tongue. Studies show that elasmobranch and teleost fishes generate water currents as a ‚Äòhydrodynamic tongue‚Äô that presumably transports food towards and into the esophagus. However, it remains largely unknown how specific musculoskeletal motions during transport correspond to food motion. Previous studies of white-spotted bamboo sharks (Chiloscyllium plagiosum) hypothesized that motions of the hyoid, branchial arches, and pectoral girdle, generate caudal motion of the food through the long oropharynx of modern sharks. To test these hypotheses, we measured food and cartilage motion with XROMM during intra-oropharyngeal transport and swallowing (n=3 individuals, 2-3 trials per individual). After entering the mouth, food does not move smoothly toward the esophagus, but rather moves in distinct steps with relatively little retrograde motion. Caudal food motion coincides with hyoid elevation and a closed mouth, supporting earlier studies showing that hyoid motion contributes to intra-oropharyngeal food transport by creating caudally-directed water currents. Little correspondence between pectoral girdle and food motion was found, indicating minimal contribution of pectoral girdle motion. Transport speed was fast as food entered the mouth, slower and step-wise through the pharyngeal region and then fast again as it entered the esophagus. The food's static periods in the step-wise motion and its high velocity during swallowing could not be explained by hyoid or girdle motion, suggesting these sharks may also use the branchial arches for intra-oropharyngeal transport and swallowing.","tags":null,"title":"Intra-oropharyngeal food transport and swallowing in white-spotted bamboo sharks","type":"publication"},{"authors":null,"categories":null,"content":"  Click here to download.\nHannah Weller  Research interests: Evolution, development, and ecology of color patterns; broadly, how traits interact to shape evolutionary trajectories; method and software development for accessible phenotyping approaches.\n  Present Position  2023‚ÄîPresent Postdoctoral researcher, Integrative Evolutionary Biology Group\nUniversity of Helsinki (Helsinki, Finland)  Education  2019‚Äî2023 PhD, Ecology and Evolutionary Biology\nBrown University (Providence, RI) Thesis: The opportunities of evolutionary constraint, with a focus on the evolution of mouthbrooding in cichlids 2017‚Äî2019 Transitional M.Sc., Ecology and Evolutionary Biology\nBrown University (Providence, RI) Thesis: How do feeding adaptations influence the convergent evolution of mouthbrooding? 2012‚Äî2016 Honors B.Sc., Biology\nUniversity of Chicago (Chicago, IL) Thesis: Winnowing in the eartheater cichlids  Awards and Fellowships  January 2022 Doctoral Dissertation Enhancement Grant\n$10,000, Bushnell Fund at Brown University May 2021 Dean\u0026rsquo;s Excellence in Teaching Award Brown University, Alpert Medical School April 2019 Graduate Research Fellowship\n$138,000, National Science Foundation December 2018 Field Museum Visiting Scientist Scholarship\n$1,500, Field Museum of Natural History May 2017 Presidential Fellowship\n$108,000, Brown University June 2015 Jeff Metcalf Undergraduate Research Fellowship\n$5,000, Marine Biological Laboratory March 2015 Elected to Phi Beta Kappa Society September 2014 Best Presentation, Undergraduate Research Symposium\n$150, University of Chicago June 2014 Elliott and Eileen Hinkes Research Fellowship\n$4,000, University of Chicago  Peer-reviewed publications Crowell, H.L.*, Curlis, J.D.*, Weller, H.I.*, Davis Rabosky, A.R.* (2024). Predators drive the evolution of ultraviolet colouration in snakes. Nature Communications. DOI: https://doi.org/10.1038/.\n*These authors contributed equally to this work.\nWeller, H.I., Hiller, A.E., Van Belleghem, S.M., and Lord, N.P. (2024). Recolorize: flexible color segmentation of biological images. Ecology Letters. DOI: https://doi.org/10.1111/ele.14378.\nThis article was featured on the cover of the journal: Paquette, S.E., Martin, N., Rodd, A., Manz, K.E., Camarillo, M., Allen, E., Weller, H.I, Pennell, K., and Plavicki, J (2023). Neural dysregulation and aberrant microglial responses to brain injury following perfluorooctane sulfonate exposure in larval zebrafish. Environmental Health Perspectives. DOI: https://doi.org/10.1289/EHP1286.\nTumulty, J.P., Miller, S.E., Van Belleghem, S.M., Weller, H.I., Jernigan, C.M., Vincent, S., Staudenraus, R.J., Legan, A.W., Polnaszek, T.J., Uy, F.M.K, Walton, A., and Sheehan, M.J. (2023). Evidence for a selective link between cooperation and individual recognition. Current Biology. DOI: https://doi.org/10.1016/j.cub.2023.11.032.\nWeller, H.I., L√≥pez-Fern√°ndez, H., McMahan, C.D., and Brainerd, E.L. (2022). Relaxed feeding constraints facilitate the evolution of mouthbrooding in Neotropical cichlids. The American Naturalist. DOI: https://doi.org/10.1086/719235.\nCapano, J.G., Boback, S.M., Weller, H.I., Cieri, R.L., Zemer, C.F., and Brainerd, E.L. (2022). Modular lung ventilation in Boa constrictor. Journal of Experimental Biology. DOI: https://doi.org/10.1242/jeb.243119.\nWeller, H.I., Olsen, A., Camp, A.L., Hernandez, L.P., Manafzadeh, A.R., and Brainerd, E.L. (2020). An XROMM study of intra-oral transport and swallowing in catfish. Integrative Organismal Biology. DOI: https://doi.org/10.1093/iob/obaa018.\nCohen, K.E., Weller, H.I., Westneat, M.W., and Summers, A.P (2020). The Evolutionary Continuum of Functional Homodonty to Heterodonty in the Dentition of Halichoeres Wrasses. Integrative and Comparative Biology. DOI: https://doi.org/10.1093/icb/icaa137.\nWeller, H.I.*, Hooper, S.E.*, and Amelon, S.K* (2020). Countcolors, an R package for quantification of the fluorescence emitted by Pseudogymnoascus destructans lesions on the wing membranes of hibernating bats. Journal of Wildlife Diseases. DOI: https://doi.org/10.7589/2019-09-231\n*These authors contributed equally to this work.\nCohen, K.E., Weller, H.I., and Summers, A.P. (2020). Not your father‚Äôs homodonty‚Äîstress, tooth shape, and the functional homodont. Journal of Anatomy. DOI: https://doi.org/10.1111/joa.13248\nvan Meer, N.M., Weller, H.I., Manafzadeh, A.R., Kaczmarek, E.B., Scott, B., Gussekloo, S.W.S, Wilga, C.D., Camp, A.L., and Brainerd, E.L. (2019). Intra-oropharyngeal food transport and swallowing in white-spotted bamboo sharks. Journal of Experimental Biology. DOI: 10.1242/jeb.201426\nWeller, H.I., and Westneat, M.W. (2019). Quantitative color profiling of digital images with earth mover‚Äôs distance using the R package colordistance. PeerJ. DOI: 10.7717/peerj.6398\nWeller, H.I., McMahan, C.D., and Westneat, M.W. (2017). Dirt-sifting Devilfish: Winnowing in the geophagine cichlid Satanoperca daemon and evolutionary implications. Zoomorphology. DOI: 10.1007/s00435-016-0335-6\nSoftware Weller, H.I. (2021). recolorize: Simplify and Remap Image Colors for Biological Analysis (ver. 0.9.000). CRAN Repository. https://CRAN.R-project.org/package=recolorize\nO\u0026rsquo;Sullivan, D., Weller, H.I., and Lord, N.P. Insect Color Database (ICDB). In development. https://insectcolor.com/\nWeller, H.I. (2019). colordistance: Distance Metrics for Image Color Similarity (ver. 1.1.0). CRAN repository. https://CRAN.R-project.org/package=colordistance\nWeller, H.I. (2018). countcolors: Locates and Counts Pixels Within Color Range(s) in Images (ver. 0.9.1). CRAN Repository. https://CRAN.R-project.org/package=countcolors\nPresentations Weller, H.I., Van Belleghem, S. (April 2024). Talk: recolorize: An R package for flexible colour segmentation of biological images. Finnish Molecular Ecology Symposium, Joensuu, Finland.\nWeller, H.I., Van Belleghem, S. (July 2023). Invited symposium talk: One way to measure color pattern variation in coral reef fishes. International Conference for Vertebrate Morphology, Cairns, Australia.\nWeller, H.I., and Van Belleghem, S. (January 2023). Poster: Flexible color segmentation of biological images with the R package recolorize. Society for Integrative and Comparative Biology, Austin, TX, USA.\nWeller, H.I., Weissman, M., and L√≥pez-Fern√°ndez, H. (January 2023). Talk: Bet-hedging theory helps explain life history differences among mouthbrooding cichlids. Society for Integrative and Comparative Biology, Austin, TX, USA.\nWeller, H.I. and L√≥pez-Fern√°ndez, H. (September 2022). Invited symposium talk: How (and how much) does feeding influence the evolution of mouthbrooding in Neotropical cichlids? Encontra Brasileiro de Ictiologia, Gramado, Brazil.\nWeller, H.I., Brainerd, E.L, and L√≥pez-Fern√°ndez, H. (January 2022). Talk: Does feeding mediate life history tradeoffs in mouthbrooding cichlids? Society for Integrative and Comparative Biology, virtual conference.\nWeller, H.I., Wham, D., Ezray-Wham, B., and Lord, N.P. (August 2021). Talk: Greater than the sum of their parts? Unpacking the ‚Äúblack box‚Äù of perceptual similarity using classical color pattern metrics. Living Light Early Career Researchers, virtual conference.\nWeller, H.I., Schwartz, S.T., Karan, E., and Lord, N.P. (Jan. 2021). Talk: Recolorize: a flexible R package for color classification. Society for Integrative and Comparative Biology, virtual conference.\nWeller, H.I., L√≥pez-Fern√°ndez, H., McMahan, C.D., and Brainerd, E.L. (Jan. 2020). Talk: The spandrels of Satan\u0026rsquo;s perches: evidence for the co-optation of feeding traits in the convergent evolution of mouthbrooding in Neotropical cichlids. Society for Integrative and Comparative Biology, Austin, TX.\nWeller, H.I., L√≥pez-Fern√°ndez, H., McMahan, C.D., and Brainerd, E.L. (Oct. 2019). Talk: Does mouthbrooding constrain or complement feeding morphology? Regional Division of Vertebrate Morphology (Northeast), Newton, MA.\nWeller, H.I., Olsen, A., Camp, A.L., Hernandez, L.P., Manafzadeh, A.R., and Brainerd, E.L. (Jan. 2019). Talk: 3D-Intra-oral Prey Trajectories Indicate Distinct Phases in how Channel Catfish (Ictalurus punctatus, Siluriformes: Ictaluridae) Swallow Food. International Congress of Vertebrate Morphology, Prague, CZ.\nWeller, H.I., Cohen, K.E., Gibb, A., and Brainerd, E.L. (Jan. 2019). Poster: Using tethers to measure food transport in a flatfish. Society for Integrative and Comparative Biology, Tampa, FL.\nWeller, H.I., Olsen, A., Camp, A.L., Hernandez, L.P., Manafzadeh, A.R., and Brainerd, E.L.(Jan. 2019). Talk: An XROMM study of intra-oral transport and swallowing in catfish. Society for Integrative and Comparative Biology, Tampa, FL.\nWeller, H.I. and Brainerd, E.L. (Oct. 2017). Talk: How do fish swallow food? Regional Division of Vertebrate Morphology (Northeast), Lowell, MA.\nWeller, H.I., McMahan, C.D., and Westneat, M.W. (July 2016). Poster: Dirt-sifting devilfish: winnowing in eartheater cichlids. American Society of Ichthyologists and Herpetologists, New Orleans, LA.\nInvited talks, lectures, \u0026amp; workshops  June 2022 Workshop: Statistics for Biologists\nUniversity of Washington, Friday Harbor Laboratories (Friday Harbor, WA) R workshop focusing on practical statistical approaches to messy biological data. Instructors: Matthew Kolmann and Cassandra Donatelli. May 2022 Podcast: Naturalist Selections\nAmerican Society of Naturalists Podcast interview about 2022 American Naturalist paper on the co-evolution of feeding and mouthbrooding in cichlids. July 2020 Workshop: Phylogenetic Comparative Methods in R\nUniversity of Washington, Friday Harbor Laboratories (Friday Harbor, WA) R workshop focusing on phylogenetic and comparative methods. Instructors: Matthew Kolmann and Cassandra Donatelli. July 2020 A field guide to statistics in organismal biology\nUniversity of Washington, Friday Harbor Laboratories (Friday Harbor, WA)\nGuest lecture. Instructors: Matthew Kolmann and Cassandra Donatelli. July 2020 Mouthbrooding morphologies in Neotropical cichlids\nUniversity of California Davis, Dept. of Ecology and Evolutionary Biology (Davis, CA)\nVirtual seminar. Host: Peter Wainwright. April 2020 Special Topics: Light, Color, and Vision in Biology (BIOL 7901/ENTM 7008)\nLouisiana State University, Dept. of Entomology and Dept. of Biology (Baton Rouge, LA)\nGuest lecturer (3 classes). Instructors: Nathan Lord (ENTM) \u0026amp; Brant Faircloth (BIOL). December 2019 Workshop: R for Biologists\nLouisiana State University, Dept. of Entomology (Baton Rouge, LA)\nOrganizer. Day-long workshop on data analysis and visualization in R.  Research experience  2023‚ÄîPresent Postdoctoral researcher; advisor: Claudius Kratochwil\nUniversity of Helsinki, HiLIFE Institute\nEvolution and development of color pattern robustness in fishes. 2017‚Äî2023 PhD candidate, Brainerd Lab; advisor: Elizabeth Brainerd\nBrown University, Dept. of Ecology \u0026amp; Evolutionary Biology\nComparative morphology, kinematics, and biomechanics of mouthbrooding fishes; XROMM fish feeding and transport. September 2013‚ÄîJuly 2017 Research assistant; advisor: Mark Westneat\nUniversity of Chicago, Dept. of Organismal Biology \u0026amp; Anatomy\nQuantitative color analysis; geometric morphometrics; high-speed video kinematics. June 2015‚ÄîSeptember 2015 Jeff Metcalf Summer Research Fellow; advisor: Roger Hanlon\nBrown University, Dept. of Ecology \u0026amp; Evolutionary Biology\nHyperspectral imaging; image analysis pipelines; camouflage analyses. June 2014‚ÄîSeptember 2014 Summer Research Fellow, Westneat Lab; advisor: Mark Westneat\nUniversity of Chicago, Dept. of Organismal Biology \u0026amp; Anatomy\nOntogenetic scaling; biomechanical modeling; geometric morphometrics.  Teaching, service, and outreach  September 2023‚ÄîPresent Postdoctoral representative, University of Helsinki, Wellbeing Committee (Helsinki, Finland)\nOrganizing events and awareness campaigns around wellbeing resources (e.g. mental health, mentorship, exercise) for Helsinki Institute of Life Sciences research community. September 2023‚ÄîJune 2024 Local organizer, Meeting of the European Society of Evolutionary and Developmental Biology, 2024 (Helsinki, Finland)\nOrganized outreach events and coordinated conference activities. June 2021 \u0026amp; July 2022 Instructor, Brown University, Summer@Brown Program (Providence, RI)\nAnatomy, Behavior, and Evolution: Fishy Solutions to Life Underwater\nIntensive high school course including labs, assignments, and mentoring of final project. Responsible for proposal, design, and implementation of entire course. August 2020‚ÄîApril 2021 Teaching assistant, Brown University, Alpert Medical School (Providence, RI)\nCOVID-modified Human Anatomy (lecture and lab)\nRestructuring the traditional gross anatomy curriculum, including remote/small group work and prosection-based staggered labs. September 2019‚Äî2023 R User Group, Brown University, Dept. of Ecology and Evolutionary Biology (Providence, RI)\nOrganizing and running monthly R workshops for graduate and undergraduate students, focusing on techniques for biological analysis (e.g., data organization, statistics, and visualization). August 2019‚ÄîApril 2020 Teaching assistant, Brown University, Alpert Medical School (Providence, RI)\nHuman Anatomy (lecture and lab)\nGuiding medical students through cadaver-based human anatomy labs. September 2018‚ÄîJune 2022 Marine Science Club, Paul Cuffee High School (Providence, RI)\nCollaborating with high school teachers for weekly science activities with high school students. September 2017‚ÄîDec. 2017 Teaching assistant, Brown University, Dept. of Ecology \u0026amp; Evolutionary Biology (Providence, RI)\nDiversity of Life (lecture) January 2015‚ÄîApril 2017 Teaching assistant, University of Chicago, Dept. of Biological Sciences (Chicago, IL)\nPresenting and supervising lab experiments; writing and grading assignments; lecturing; leading paper discussions and review sessions; guiding dissection-based anatomy labs. Genetic and Developmental Biology (lab \u0026amp; lecture)\nMultiscale Modeling of Biological Systems (lecture)\nMolecular Biology of the Cell (lab)\nComparative Vertebrate Anatomy (lab \u0026amp; lecture)\n June 2013‚ÄîSeptember 2013 Animal care intern, New England Aquarium (Boston, MA)\nDaily animal care and maintenance; visitor outreach; collection trips.  Skills  Coding R, Python (OpenCV, Scrapy, \u0026amp; BioPython libraries), MATLAB, UNIX, MEL Software Latex, Maya, FIJI/ImageJ, Horos, 3DSlicer, XMALab, Mesquite, Pandoc, Microsoft Office Languages English (native), French (intermediate), Finnish (basic)    Lab: Viikinkaari 5, 00790 Helsinki, Finland\nEmail: hannahiweller@gmail.com\nWebsite: hiweller.rbind.io\n ","date":1530144000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530144000,"objectID":"fd36605688ef45e10dc233c860158012","permalink":"/cv/","publishdate":"2018-06-28T00:00:00Z","relpermalink":"/cv/","section":"","summary":"My current CV.","tags":null,"title":"","type":"page"},{"authors":["Hannah I. Weller","Caleb D. McMahan","Mark W. Westneat"],"categories":null,"content":"","date":1488326400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1488326400,"objectID":"aebf7686c830e2912367d4a4c8459638","permalink":"/publication/weller-dirt-sifting-2017/","publishdate":"2025-03-04T15:40:54.126895Z","relpermalink":"/publication/weller-dirt-sifting-2017/","section":"publication","summary":"","tags":null,"title":"Dirt-sifting devilfish: winnowing in the geophagine cichlid Satanoperca daemon and evolutionary implications","type":"publication"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"10208a5f41e42dc21587b71536d3b832","permalink":"/project/icdb/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/icdb/","section":"project","summary":"Check out the `Insect Color Database`, a resource for storing image, spectral, and EM data relating to insect color.","tags":["color"],"title":"Insect Color Database","type":"project"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f26b5133c34eec1aa0a09390a36c2ade","permalink":"/admin/config.yml","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/admin/config.yml","section":"","summary":"","tags":null,"title":"","type":"wowchemycms"}]